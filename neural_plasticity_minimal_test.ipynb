{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4391633",
   "metadata": {},
   "source": [
    "# Neural Plasticity Minimal Test (v0.0.53)\n",
    "\n",
    "This notebook tests the core tensor operations and visualizations of the neural plasticity module without loading datasets or full models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set environment variables for safer execution\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = '1'\n",
    "\n",
    "# Add project root to path\n",
    "if not os.getcwd() in sys.path:\n",
    "    sys.path.append(os.getcwd())\n",
    "\n",
    "# Import neural plasticity modules\n",
    "from utils.neural_plasticity.core import calculate_head_entropy\n",
    "from utils.neural_plasticity.visualization import (\n",
    "    visualize_head_entropy,\n",
    "    visualize_head_gradients,\n",
    "    visualize_pruning_decisions,\n",
    "    visualize_attention_patterns\n",
    ")\n",
    "from utils.colab.helpers import safe_tensor_imshow\n",
    "\n",
    "print(\"Neural plasticity imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf70dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test attention tensors\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "num_layers = 6\n",
    "seq_len = 32\n",
    "\n",
    "# Create random attention-like matrices\n",
    "attention_maps = torch.rand(batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "# Ensure they sum to 1 along the last dimension (proper attention distributions)\n",
    "attention_maps = attention_maps / attention_maps.sum(dim=-1, keepdim=True)\n",
    "\n",
    "print(f\"Created attention tensor of shape {attention_maps.shape}\")\n",
    "print(f\"Attention min/max/mean: {attention_maps.min().item():.4f}/{attention_maps.max().item():.4f}/{attention_maps.mean().item():.4f}\")\n",
    "\n",
    "# Check that rows sum to 1\n",
    "row_sums = attention_maps.sum(dim=-1)\n",
    "print(f\"Row sums close to 1.0: {torch.allclose(row_sums, torch.ones_like(row_sums))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ed885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test entropy calculation\n",
    "entropy = calculate_head_entropy(attention_maps)\n",
    "print(f\"Entropy shape: {entropy.shape}\")\n",
    "print(f\"Entropy min/max/mean: {entropy.min().item():.4f}/{entropy.max().item():.4f}/{entropy.mean().item():.4f}\")\n",
    "\n",
    "# Create example layer x head entropy tensor\n",
    "layer_entropies = torch.rand(num_layers, num_heads)\n",
    "print(f\"Layer entropies shape: {layer_entropies.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09303e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test entropy visualization\n",
    "fig1 = visualize_head_entropy(\n",
    "    entropy_values=layer_entropies,\n",
    "    title=\"Test Entropy Visualization\",\n",
    "    annotate=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543adc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock gradient values\n",
    "grad_values = torch.rand(num_layers, num_heads)\n",
    "\n",
    "# Create test pruned/revived heads\n",
    "pruned_heads = [(0, 1), (2, 3)]  # Layer 0, Head 1 and Layer 2, Head 3\n",
    "revived_heads = [(1, 2)]         # Layer 1, Head 2\n",
    "\n",
    "# Test gradient visualization\n",
    "fig2 = visualize_head_gradients(\n",
    "    grad_norm_values=grad_values,\n",
    "    pruned_heads=pruned_heads,\n",
    "    revived_heads=revived_heads,\n",
    "    title=\"Test Gradient Visualization\",\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf0a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock pruning mask\n",
    "pruning_mask = torch.zeros(num_layers, num_heads, dtype=torch.bool)\n",
    "for layer, head in pruned_heads:\n",
    "    pruning_mask[layer, head] = True\n",
    "\n",
    "# Test pruning decision visualization\n",
    "fig3 = visualize_pruning_decisions(\n",
    "    grad_norm_values=grad_values,\n",
    "    pruning_mask=pruning_mask,\n",
    "    title=\"Test Pruning Decisions\",\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention pattern visualization\n",
    "# For single head\n",
    "fig4 = visualize_attention_patterns(\n",
    "    attention_maps=attention_maps,\n",
    "    layer_idx=0,\n",
    "    head_idx=0,\n",
    "    title=\"Single Head Attention Pattern\",\n",
    "    figsize=(8, 6)\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For multiple heads\n",
    "fig5 = visualize_attention_patterns(\n",
    "    attention_maps=attention_maps,\n",
    "    layer_idx=0,\n",
    "    head_idx=None,  # Show multiple heads\n",
    "    title=\"Multiple Heads Attention Patterns\",\n",
    "    figsize=(14, 6),\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27770212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test safe_tensor_imshow with both CPU and simulated GPU tensors\n",
    "print(\"Testing safe_tensor_imshow function...\")\n",
    "\n",
    "# Create a test tensor\n",
    "test_tensor = torch.rand(10, 10)\n",
    "\n",
    "# Test with additional metadata that simulates a GPU tensor with gradients\n",
    "test_tensor.requires_grad = True\n",
    "\n",
    "# Use safe_tensor_imshow\n",
    "plt.figure(figsize=(8, 6))\n",
    "img = safe_tensor_imshow(\n",
    "    test_tensor, \n",
    "    title=\"Safe Tensor Visualization Test\",\n",
    "    cmap=\"inferno\"\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Safe tensor visualization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ac21d5",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "\n",
    "If you've reached this point without errors, the neural plasticity tensor operations and visualizations are working correctly. This indicates that the fixes for BLAS/libtorch issues have been successful.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try running the full notebook in Colab with GPU acceleration\n",
    "2. If issues persist, examine specific components as needed"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
