{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Transformer Inference Demo\n",
    "\n",
    "This notebook demonstrates how to use the Adaptive Transformer with Sentinel Gates for text generation. We'll load a pre-trained model, generate text, and visualize the attention patterns and gate values during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the parent directory to the path to import modules\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models\n",
    "\n",
    "We'll load both the baseline model (GPT-2) and our adaptive model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from models.loaders.loader import load_baseline_model, load_adaptive_model\n",
    "from utils.checkpoint import load_checkpoint\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizer and baseline model\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "baseline_model = load_baseline_model(model_name, device)\n",
    "\n",
    "# Load adaptive model\n",
    "adaptive_model = load_adaptive_model(model_name, baseline_model, device)\n",
    "\n",
    "# Path to checkpoint (update this with your checkpoint path)\n",
    "checkpoint_path = \"../checkpoints/adaptive_model.pth\"\n",
    "\n",
    "# Try to load checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    optimizer = torch.optim.AdamW(adaptive_model.parameters())\n",
    "    head_lr_multipliers = {}\n",
    "    adaptive_model, _, _, _, _ = load_checkpoint(\n",
    "        adaptive_model, optimizer, head_lr_multipliers, checkpoint_path, device)\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"No checkpoint found at {checkpoint_path}. Using freshly initialized adaptive model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Generation Wrapper\n",
    "\n",
    "We'll use the GenerationWrapper class to handle text generation with both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from utils.generation_wrapper import GenerationWrapper\n",
    "\n",
    "# Create generation wrappers\n",
    "adaptive_wrapper = GenerationWrapper(model=adaptive_model, tokenizer=tokenizer, device=device)\n",
    "baseline_wrapper = GenerationWrapper(model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Text\n",
    "\n",
    "Let's generate text with both models and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample prompts\n",
    "prompts = [\n",
    "    \"Once upon a time in a land far away\",\n",
    "    \"The scientist made a discovery that would change\",\n",
    "    \"The future of artificial intelligence depends on\"\n",
    "]\n",
    "\n",
    "# Generation parameters\n",
    "generation_params = {\n",
    "    \"max_length\": 100,\n",
    "    \"temperature\": 0.8,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"num_return_sequences\": 1\n",
    "}\n",
    "\n",
    "# Generate text with both models\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\\n\")\n",
    "    \n",
    "    print(\"Adaptive Model:\")\n",
    "    adaptive_outputs = adaptive_wrapper.generate_text(prompt, **generation_params)\n",
    "    for output in adaptive_outputs:\n",
    "        print(f\"{output}\\n\")\n",
    "    \n",
    "    print(\"Baseline Model:\")\n",
    "    baseline_outputs = baseline_wrapper.generate_text(prompt, **generation_params)\n",
    "    for output in baseline_outputs:\n",
    "        print(f\"{output}\\n\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Attention Patterns\n",
    "\n",
    "Now, let's generate text while visualizing the attention patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate with attention visualization\n",
    "prompt = \"The key to understanding artificial intelligence is\"\n",
    "\n",
    "# Generate text with attention visualization\n",
    "adaptive_outputs = adaptive_wrapper.generate_text(\n",
    "    prompt, \n",
    "    **generation_params,\n",
    "    visualize_attention=True,\n",
    "    max_length=50  # Shorter for clearer visualization\n",
    ")\n",
    "\n",
    "print(f\"Generated text:\\n{adaptive_outputs[0]}\")\n",
    "\n",
    "# The visualizations are saved as PNG files in the current directory\n",
    "# Let's display the first few attention maps\n",
    "import glob\n",
    "attention_files = sorted(glob.glob(\"attention_step_*.png\"))\n",
    "\n",
    "# Display the first 3 attention maps or all if fewer than 3\n",
    "for i, file in enumerate(attention_files[:3]):\n",
    "    print(f\"Attention map {i+1}:\")\n",
    "    from IPython.display import Image, display\n",
    "    display(Image(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Gate Values\n",
    "\n",
    "Let's track how gate values change during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate with gate value tracking\n",
    "prompt = \"The future of language models will be determined by\"\n",
    "\n",
    "# Generate text with gate value tracking\n",
    "adaptive_outputs = adaptive_wrapper.generate_text(\n",
    "    prompt, \n",
    "    **generation_params,\n",
    "    track_gate_values=True,\n",
    "    max_length=50  # Shorter for clearer visualization\n",
    ")\n",
    "\n",
    "print(f\"Generated text:\\n{adaptive_outputs[0]}\")\n",
    "\n",
    "# The gate dynamics visualization is saved as gate_dynamics.png\n",
    "from IPython.display import Image, display\n",
    "display(Image(\"gate_dynamics.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Head Utilization\n",
    "\n",
    "Let's analyze which heads are most active in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_head_utilization(model):\n",
    "    \"\"\"Calculate and visualize head utilization.\"\"\"\n",
    "    # Extract gate values\n",
    "    gate_values = {}\n",
    "    for layer_idx, block in enumerate(model.blocks):\n",
    "        attn_module = block[\"attn\"]\n",
    "        gate_values[layer_idx] = attn_module.gate.detach().cpu().numpy()\n",
    "    \n",
    "    # Convert to matrix for visualization\n",
    "    num_layers = len(gate_values)\n",
    "    num_heads = len(gate_values[0])\n",
    "    gate_matrix = np.zeros((num_layers, num_heads))\n",
    "    \n",
    "    for layer_idx, gates in gate_values.items():\n",
    "        gate_matrix[layer_idx] = gates\n",
    "    \n",
    "    # Calculate overall utilization\n",
    "    active_heads = (gate_matrix > 0.1).sum()\n",
    "    total_heads = num_layers * num_heads\n",
    "    utilization = active_heads / total_heads * 100\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(gate_matrix, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
    "    plt.title(f\"Attention Gate Values (Utilization: {utilization:.2f}%)\")\n",
    "    plt.xlabel(\"Head Index\")\n",
    "    plt.ylabel(\"Layer Index\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"active_heads\": int(active_heads),\n",
    "        \"total_heads\": total_heads,\n",
    "        \"utilization_percent\": float(utilization),\n",
    "        \"gate_matrix\": gate_matrix\n",
    "    }\n",
    "\n",
    "# Analyze head utilization\n",
    "utilization_stats = get_head_utilization(adaptive_model)\n",
    "\n",
    "print(f\"Active heads: {utilization_stats['active_heads']} / {utilization_stats['total_heads']} ({utilization_stats['utilization_percent']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Generation Performance\n",
    "\n",
    "Let's benchmark the generation speed of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "def benchmark_generation(wrapper, prompt, max_length, num_runs=5):\n",
    "    \"\"\"Benchmark generation speed.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = wrapper.generate_text(\n",
    "            prompt, \n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    tokens_per_second = max_length / avg_time\n",
    "    \n",
    "    return {\n",
    "        \"avg_time_seconds\": avg_time,\n",
    "        \"tokens_per_second\": tokens_per_second,\n",
    "        \"times\": times\n",
    "    }\n",
    "\n",
    "# Set up benchmark parameters\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "max_length = 200\n",
    "num_runs = 3\n",
    "\n",
    "# Benchmark both models\n",
    "print(\"Benchmarking adaptive model...\")\n",
    "adaptive_benchmark = benchmark_generation(adaptive_wrapper, prompt, max_length, num_runs)\n",
    "\n",
    "print(\"Benchmarking baseline model...\")\n",
    "baseline_benchmark = benchmark_generation(baseline_wrapper, prompt, max_length, num_runs)\n",
    "\n",
    "# Print results\n",
    "print(\"Generation benchmark results:\")\n",
    "print(f\"Adaptive model: {adaptive_benchmark['tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"Baseline model: {baseline_benchmark['tokens_per_second']:.2f} tokens/sec\")\n",
    "\n",
    "# Calculate speedup/slowdown\n",
    "speedup = (adaptive_benchmark['tokens_per_second'] / baseline_benchmark['tokens_per_second'] - 1) * 100\n",
    "print(f\"{('Speedup' if speedup > 0 else 'Slowdown')}: {abs(speedup):.2f}%\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([\"Adaptive Model\", \"Baseline Model\"], \n",
    "        [adaptive_benchmark['tokens_per_second'], baseline_benchmark['tokens_per_second']])\n",
    "plt.ylabel(\"Tokens per Second\")\n",
    "plt.title(\"Generation Speed Comparison\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Parameter Efficiency\n",
    "\n",
    "Finally, let's examine the parameter efficiency of our adaptive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Count parameters in a model, distinguishing between active and inactive.\"\"\"\n",
    "    if not hasattr(model, \"blocks\"):\n",
    "        # Standard HuggingFace model\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            \"total_params\": int(total_params),\n",
    "            \"trainable_params\": int(trainable_params),\n",
    "            \"frozen_params\": int(total_params - trainable_params)\n",
    "        }\n",
    "    else:\n",
    "        # Adaptive model\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Count active heads and parameters\n",
    "        active_heads = 0\n",
    "        active_head_params = 0\n",
    "        inactive_head_params = 0\n",
    "        \n",
    "        for block in model.blocks:\n",
    "            attn_module = block[\"attn\"]\n",
    "            \n",
    "            for head_idx in range(attn_module.num_heads):\n",
    "                # Count params in this head\n",
    "                head_params = 0\n",
    "                for param in list(attn_module.W_q[head_idx].parameters()) + \\\n",
    "                           list(attn_module.W_k[head_idx].parameters()) + \\\n",
    "                           list(attn_module.W_v[head_idx].parameters()) + \\\n",
    "                           list(attn_module.W_o[head_idx].parameters()):\n",
    "                    head_params += param.numel()\n",
    "                \n",
    "                if attn_module.gate[head_idx].item() > 0.1:\n",
    "                    active_heads += 1\n",
    "                    active_head_params += head_params\n",
    "                else:\n",
    "                    inactive_head_params += head_params\n",
    "        \n",
    "        # Count non-head parameters\n",
    "        non_head_params = total_params - (active_head_params + inactive_head_params)\n",
    "        \n",
    "        return {\n",
    "            \"total_params\": int(total_params),\n",
    "            \"trainable_params\": int(trainable_params),\n",
    "            \"frozen_params\": int(total_params - trainable_params),\n",
    "            \"active_heads\": int(active_heads),\n",
    "            \"active_head_params\": int(active_head_params),\n",
    "            \"inactive_head_params\": int(inactive_head_params),\n",
    "            \"non_head_params\": int(non_head_params)\n",
    "        }\n",
    "\n",
    "# Count parameters for both models\n",
    "adaptive_params = count_parameters(adaptive_model)\n",
    "baseline_params = count_parameters(baseline_model)\n",
    "\n",
    "# Print parameter counts\n",
    "print(\"Parameter counts:\")\n",
    "print(f\"Baseline model: {baseline_params['total_params']:,} parameters\")\n",
    "print(f\"Adaptive model: {adaptive_params['total_params']:,} parameters\")\n",
    "print(f\"  Active heads: {adaptive_params.get('active_heads', 'N/A')}\")\n",
    "print(f\"  Active head parameters: {adaptive_params.get('active_head_params', 'N/A'):,}\")\n",
    "print(f\"  Inactive head parameters: {adaptive_params.get('inactive_head_params', 'N/A'):,}\")\n",
    "print(f\"  Non-head parameters: {adaptive_params.get('non_head_params', 'N/A'):,}\")\n",
    "\n",
    "# Calculate efficiency\n",
    "if 'active_head_params' in adaptive_params:\n",
    "    effective_params = adaptive_params['active_head_params'] + adaptive_params['non_head_params']\n",
    "    reduction = (baseline_params['total_params'] - effective_params) / baseline_params['total_params'] * 100\n",
    "    print(f\"\\nEffective parameter reduction: {reduction:.2f}%\")\n",
    "\n",
    "# Visualize parameter distribution\n",
    "if 'active_head_params' in adaptive_params:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    labels = ['Active Head Parameters', 'Inactive Head Parameters', 'Non-Head Parameters']\n",
    "    sizes = [\n",
    "        adaptive_params['active_head_params'],\n",
    "        adaptive_params['inactive_head_params'],\n",
    "        adaptive_params['non_head_params']\n",
    "    ]\n",
    "    colors = ['#2ca02c', '#d62728', '#1f77b4']\n",
    "    \n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    plt.axis('equal')\n",
    "    plt.title('Adaptive Model Parameter Distribution')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the Adaptive Transformer with Sentinel Gates for text generation. We've shown:\n",
    "\n",
    "1. Text generation with both the adaptive and baseline models\n",
    "2. Visualization of attention patterns during generation\n",
    "3. Tracking of gate values during generation\n",
    "4. Analysis of head utilization and parameter efficiency\n",
    "5. Comparison of generation performance\n",
    "\n",
    "The adaptive model uses sentinel gates to dynamically control which attention heads are active during processing, potentially leading to both parameter efficiency and computation efficiency during inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}