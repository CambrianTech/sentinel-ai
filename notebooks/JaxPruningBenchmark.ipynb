{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX Pruning Benchmark\n",
    "\n",
    "This notebook implements a stable pruning benchmark using JAX/Flax instead of PyTorch. This is particularly useful for M1/M2 Macs that may experience BLAS crashes with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Set environment variables for JAX\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "\n",
    "# Import JAX and transformers\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Implementation\n",
    "\n",
    "Next, we'll implement our head pruning functions using JAX/Flax. This approach is more stable on M1/M2 Macs compared to PyTorch-based implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prune_head_in_params(params, layer_idx, head_idx, model_type=\"gpt2\"):\n",
    "    \"\"\"Zero out weights for a specific attention head in Flax params\"\"\"\n",
    "    if model_type == \"gpt2\":\n",
    "        # Access path to transformer layers\n",
    "        transformer_path = \"transformer\"\n",
    "        # In Flax, layer indices are stored as strings\n",
    "        layer_path = f\"h\"\n",
    "        layer_key = str(layer_idx)\n",
    "        attn_path = \"attn\"\n",
    "        \n",
    "        # Get attention block\n",
    "        attn_block = params[transformer_path][layer_path][layer_key][attn_path]\n",
    "        \n",
    "        # Get head dimension and number of heads\n",
    "        num_heads = 12  # Standard for GPT-2\n",
    "        if \"distil\" in model_type:\n",
    "            num_heads = 12  # DistilGPT-2 also has 12 heads\n",
    "        \n",
    "        hidden_size = attn_block[\"c_attn\"][\"kernel\"].shape[1]\n",
    "        head_size = hidden_size // num_heads\n",
    "        \n",
    "        # Calculate start and end indices for this head in query, key, value\n",
    "        q_start = head_idx * head_size\n",
    "        q_end = (head_idx + 1) * head_size\n",
    "        \n",
    "        # Zero out the output projection for this head\n",
    "        output_proj = attn_block[\"c_proj\"][\"kernel\"]\n",
    "        # In Flax, c_proj.kernel has shape [hidden_size, hidden_size]\n",
    "        # We need to zero out the rows corresponding to this head\n",
    "        zeros = jnp.zeros_like(output_proj[q_start:q_end, :])\n",
    "        output_proj = output_proj.at[q_start:q_end, :].set(zeros)\n",
    "        \n",
    "        # Update the parameters\n",
    "        params[transformer_path][layer_path][layer_key][attn_path][\"c_proj\"][\"kernel\"] = output_proj\n",
    "        \n",
    "        print(f\"Successfully pruned layer {layer_idx}, head {head_idx}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "def evaluate_perplexity(model, params, tokenizer, text):\n",
    "    \"\"\"Evaluate model perplexity on text\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(text, return_tensors=\"jax\")\n",
    "    \n",
    "    # Get logits\n",
    "    outputs = model(**inputs, params=params)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Calculate loss\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Shift logits and labels for next token prediction\n",
    "    shift_logits = logits[:, :-1]\n",
    "    shift_labels = input_ids[:, 1:]\n",
    "    \n",
    "    # Calculate cross entropy loss\n",
    "    loss = jnp.mean(\n",
    "        -jnp.sum(\n",
    "            jax.nn.log_softmax(shift_logits) * jax.nn.one_hot(shift_labels, shift_logits.shape[-1]),\n",
    "            axis=-1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return perplexity\n",
    "    return jnp.exp(loss).item()\n",
    "\n",
    "def generate_text(model, params, tokenizer, prompt, max_length=50):\n",
    "    \"\"\"Generate text using the model\"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"jax\")\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        params=params,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    text = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance Calculation Strategies\n",
    "\n",
    "Now we'll implement different strategies for calculating head importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def calculate_random_importance(params, num_layers, num_heads):\n",
    "    \"\"\"Calculate random importance scores for each head\"\"\"\n",
    "    all_head_importance = []\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        # Generate random scores\n",
    "        importance = np.random.rand(num_heads)\n",
    "        \n",
    "        # Normalize scores\n",
    "        importance = importance / np.sum(importance)\n",
    "        \n",
    "        # Add to all heads\n",
    "        for head_idx, score in enumerate(importance):\n",
    "            all_head_importance.append((layer_idx, head_idx, score))\n",
    "    \n",
    "    return all_head_importance\n",
    "\n",
    "def calculate_entropy_importance(params, num_layers, num_heads):\n",
    "    \"\"\"Calculate entropy-based importance scores for each head\"\"\"\n",
    "    all_head_importance = []\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        # Use simplified proxy: norm of output projection weights\n",
    "        importance = np.zeros(num_heads)\n",
    "        layer_params = params[\"transformer\"][\"h\"][str(layer_idx)][\"attn\"]\n",
    "        output_proj = layer_params[\"c_proj\"][\"kernel\"]\n",
    "        \n",
    "        head_size = output_proj.shape[0] // num_heads\n",
    "        for head_idx in range(num_heads):\n",
    "            start_idx = head_idx * head_size\n",
    "            end_idx = (head_idx + 1) * head_size\n",
    "            importance[head_idx] = jnp.linalg.norm(output_proj[start_idx:end_idx, :]).item()\n",
    "        \n",
    "        # Normalize importance scores\n",
    "        if np.sum(importance) > 0:\n",
    "            importance = importance / np.sum(importance)\n",
    "        \n",
    "        # Add to all heads\n",
    "        for head_idx, score in enumerate(importance):\n",
    "            all_head_importance.append((layer_idx, head_idx, score))\n",
    "    \n",
    "    return all_head_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Results\n",
    "\n",
    "Let's implement functions to save and load results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def save_results(strategy_name, pruning_level, results):\n",
    "    \"\"\"Save results to a JSON file\"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = \"pruning_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"{strategy_name}_{pruning_level}_{timestamp}.json\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Save results\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filepath}\")\n",
    "    return filepath\n",
    "\n",
    "def view_results(results_dir=\"pruning_results\"):\n",
    "    \"\"\"Load and display results from previous benchmark runs\"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all result files\n",
    "    result_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(f\"No result files found in {results_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Load results\n",
    "    all_results = []\n",
    "    \n",
    "    for filepath in result_files:\n",
    "        try:\n",
    "            # Extract strategy and pruning level from filename\n",
    "            filename = os.path.basename(filepath)\n",
    "            \n",
    "            # Try to match both formats:\n",
    "            # 1. New format: \"strategy_level_timestamp.json\"\n",
    "            # 2. Old format: \"strategy_pruning_level_results.json\"\n",
    "            match1 = re.match(r\"(\\w+)_(\\d+\\.\\d+)_\\d+\\.json\", filename)\n",
    "            match2 = re.match(r\"(\\w+)_pruning_(\\d+\\.\\d+)_results\\.json\", filename)\n",
    "            \n",
    "            if match1:\n",
    "                # New format\n",
    "                strategy = match1.group(1)\n",
    "                pruning_level = float(match1.group(2))\n",
    "            elif match2:\n",
    "                # Old format\n",
    "                strategy = match2.group(1)\n",
    "                pruning_level = float(match2.group(2))\n",
    "            else:\n",
    "                print(f\"Warning: Couldn't parse filename {filename}, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Load the results file\n",
    "            with open(filepath, \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Add strategy and pruning level if not in results\n",
    "            if \"strategy\" not in results:\n",
    "                results[\"strategy\"] = strategy\n",
    "            if \"pruning_level\" not in results:\n",
    "                results[\"pruning_level\"] = pruning_level\n",
    "                \n",
    "            all_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "    \n",
    "    # Sort results by strategy and pruning level\n",
    "    all_results.sort(key=lambda x: (x[\"strategy\"], x[\"pruning_level\"]))\n",
    "    \n",
    "    # Group results by strategy\n",
    "    strategies = set(r[\"strategy\"] for r in all_results)\n",
    "    grouped_results = {strategy: [] for strategy in strategies}\n",
    "    \n",
    "    for result in all_results:\n",
    "        grouped_results[result[\"strategy\"]].append(result)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Found {len(all_results)} result files:\\n\")\n",
    "    \n",
    "    for strategy, results in grouped_results.items():\n",
    "        print(f\"Strategy: {strategy}\")\n",
    "        for result in results:\n",
    "            perplexity_change = result.get(\"perplexity_change\", \"N/A\")\n",
    "            if isinstance(perplexity_change, (int, float)):\n",
    "                perplexity_change = f\"{perplexity_change:.4f}\"\n",
    "            \n",
    "            print(f\"  Pruning level: {result['pruning_level']}, \" + \n",
    "                  f\"Perplexity change: {perplexity_change}\")\n",
    "        print()\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pruning Benchmark\n",
    "\n",
    "Now let's implement the main function to run the pruning benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_pruning_benchmark(model_name=\"distilgpt2\", strategy_name=\"random\", \n",
    "                         pruning_level=0.3, prompt=\"Artificial intelligence is\"):\n",
    "    \"\"\"Run pruning benchmark with specified parameters\"\"\"\n",
    "    print(f\"Running JAX/Flax pruning benchmark with:\\n\" + \n",
    "          f\"  Model: {model_name}\\n\" +\n",
    "          f\"  Strategy: {strategy_name}\\n\" +\n",
    "          f\"  Pruning level: {pruning_level}\\n\" +\n",
    "          f\"  Prompt: {prompt}\\n\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = FlaxAutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Get model information\n",
    "    if \"gpt2\" in model_name:\n",
    "        model_type = \"gpt2\"\n",
    "        num_layers = len(model.params[\"transformer\"][\"h\"])\n",
    "        num_heads = 12  # Standard for GPT-2\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_name}\")\n",
    "    \n",
    "    print(f\"Model has {num_layers} layers with {num_heads} heads per layer\")\n",
    "    \n",
    "    # Make a copy of the parameters (so we can keep original for comparison)\n",
    "    original_params = model.params\n",
    "    params = jax.tree_util.tree_map(lambda x: x, original_params)  # Deep copy\n",
    "    \n",
    "    # Evaluate model before pruning\n",
    "    print(\"\\nEvaluating model before pruning...\")\n",
    "    perplexity_before = evaluate_perplexity(model, params, tokenizer, prompt)\n",
    "    print(f\"Perplexity before pruning: {perplexity_before:.4f}\")\n",
    "    \n",
    "    generated_before = generate_text(model, params, tokenizer, prompt)\n",
    "    print(f\"Generated (before pruning): {generated_before}\")\n",
    "    \n",
    "    # Calculate head importance based on strategy\n",
    "    print(\"\\nCalculating head importance...\")\n",
    "    if strategy_name.lower() == \"random\":\n",
    "        all_head_importance = calculate_random_importance(params, num_layers, num_heads)\n",
    "    elif strategy_name.lower() == \"entropy\":\n",
    "        all_head_importance = calculate_entropy_importance(params, num_layers, num_heads)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pruning strategy: {strategy_name}\")\n",
    "    \n",
    "    # Sort by importance (ascending)\n",
    "    all_head_importance.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Calculate number of heads to prune\n",
    "    total_heads = num_layers * num_heads\n",
    "    heads_to_prune = int(total_heads * pruning_level)\n",
    "    print(f\"Pruning {heads_to_prune} out of {total_heads} heads\")\n",
    "    \n",
    "    # Get heads to prune (least important first)\n",
    "    pruned_heads = all_head_importance[:heads_to_prune]\n",
    "    \n",
    "    # Prune the heads\n",
    "    print(\"\\nPruning heads...\")\n",
    "    for layer_idx, head_idx, _ in pruned_heads:\n",
    "        params = prune_head_in_params(params, layer_idx, head_idx, model_type)\n",
    "    \n",
    "    # Evaluate model after pruning\n",
    "    print(\"\\nEvaluating model after pruning...\")\n",
    "    perplexity_after = evaluate_perplexity(model, params, tokenizer, prompt)\n",
    "    print(f\"Perplexity after pruning: {perplexity_after:.4f}\")\n",
    "    print(f\"Perplexity change: {perplexity_after - perplexity_before:.4f}\")\n",
    "    \n",
    "    generated_after = generate_text(model, params, tokenizer, prompt)\n",
    "    print(f\"Generated (after pruning): {generated_after}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"strategy\": strategy_name,\n",
    "        \"pruning_level\": pruning_level,\n",
    "        \"pruned_heads\": heads_to_prune,\n",
    "        \"total_heads\": total_heads,\n",
    "        \"prompt\": prompt,\n",
    "        \"perplexity_before\": float(perplexity_before),  # Convert from JAX array\n",
    "        \"perplexity_after\": float(perplexity_after),\n",
    "        \"perplexity_change\": float(perplexity_after - perplexity_before),\n",
    "        \"generated_before\": generated_before,\n",
    "        \"generated_after\": generated_after,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    save_results(strategy_name, pruning_level, results)\n",
    "    \n",
    "    print(\"\\nPruning benchmark completed successfully!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results\n",
    "\n",
    "Let's implement a function to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_results(all_results):\n",
    "    \"\"\"Plot perplexity change vs pruning level for each strategy\"\"\"\n",
    "    if not all_results:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Group results by strategy\n",
    "    strategies = set(r[\"strategy\"] for r in all_results)\n",
    "    grouped_results = {strategy: [] for strategy in strategies}\n",
    "    \n",
    "    for result in all_results:\n",
    "        # Only include results with perplexity change\n",
    "        if \"perplexity_change\" in result and isinstance(result[\"perplexity_change\"], (int, float)):\n",
    "            grouped_results[result[\"strategy\"]].append(result)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\"]\n",
    "    markers = [\"o\", \"s\", \"^\", \"d\", \"x\"]\n",
    "    \n",
    "    for i, (strategy, results) in enumerate(grouped_results.items()):\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        # Sort by pruning level\n",
    "        results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        pruning_levels = [r[\"pruning_level\"] for r in results]\n",
    "        perplexity_changes = [r[\"perplexity_change\"] for r in results]\n",
    "        \n",
    "        # Plot\n",
    "        color = colors[i % len(colors)]\n",
    "        marker = markers[i % len(markers)]\n",
    "        plt.plot(pruning_levels, perplexity_changes, marker=marker, color=color,\n",
    "                linestyle=\"-\", label=strategy.capitalize())\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level\")\n",
    "    plt.ylabel(\"Perplexity Change\")\n",
    "    plt.title(\"Effect of Pruning on Model Perplexity\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add horizontal line at y=0\n",
    "    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Single Benchmark\n",
    "\n",
    "Let's run a single benchmark test with a low pruning level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a single test with minimal pruning\n",
    "results = run_pruning_benchmark(\n",
    "    model_name=\"distilgpt2\",\n",
    "    strategy_name=\"random\",\n",
    "    pruning_level=0.1,\n",
    "    prompt=\"Artificial intelligence is\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and Plot Existing Results\n",
    "\n",
    "Load and visualize existing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and display results\n",
    "all_results = view_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot results if we have any\n",
    "if all_results:\n",
    "    plot_results(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Multiple Benchmarks\n",
    "\n",
    "Run multiple benchmarks with different strategies and pruning levels. This can be run overnight on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_multiple_benchmarks(model_name=\"distilgpt2\", \n",
    "                           strategies=[\"random\", \"entropy\"],\n",
    "                           pruning_levels=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                           prompt=\"Artificial intelligence is\"):\n",
    "    \"\"\"Run multiple benchmarks with different parameters\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for level in pruning_levels:\n",
    "            print(f\"\\n{'='*50}\\nRunning benchmark: {strategy}, level: {level}\\n{'='*50}\\n\")\n",
    "            \n",
    "            try:\n",
    "                result = run_pruning_benchmark(\n",
    "                    model_name=model_name,\n",
    "                    strategy_name=strategy,\n",
    "                    pruning_level=level,\n",
    "                    prompt=prompt\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in benchmark {strategy}, level {level}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nCompleted {len(all_results)} benchmarks out of {len(strategies) * len(pruning_levels)} attempted\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(all_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Uncomment to run multiple benchmarks\n",
    "# Note: This can take a long time, especially for higher pruning levels\n",
    "# all_results = run_multiple_benchmarks(\n",
    "#     model_name=\"distilgpt2\",\n",
    "#     strategies=[\"random\", \"entropy\"],\n",
    "#     pruning_levels=[0.1, 0.3, 0.5]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}