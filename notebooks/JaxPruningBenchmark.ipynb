{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¬ JAX Pruning Benchmark\n",
    "\n",
    "This notebook implements a modular, robust pruning benchmark using JAX/Flax. It runs efficiently on:\n",
    "- Google Colab (using GPUs/TPUs when available)\n",
    "- M1/M2 Macs (avoiding BLAS crashes that occur with PyTorch)\n",
    "- Any standard environment\n",
    "\n",
    "## Features\n",
    "- Auto-installation of dependencies\n",
    "- Multiple pruning strategies\n",
    "- Multiple models (automatically detected based on available memory)\n",
    "- Progressive visualization as results are collected\n",
    "- Robust error handling\n",
    "- Compatible with overnight runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's detect our environment and configure it appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Environment detection\n",
    "class Environment:\n",
    "    \"\"\"Detect and configure the runtime environment\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Check if we're running in Colab\n",
    "        self.in_colab = 'google.colab' in sys.modules\n",
    "        \n",
    "        # Check if we're on a Mac\n",
    "        self.is_mac = platform.system() == \"Darwin\"\n",
    "        self.is_arm_mac = self.is_mac and platform.machine().startswith(\"arm\")\n",
    "        \n",
    "        # Initialize JAX-related properties\n",
    "        self.has_gpu = False\n",
    "        self.has_tpu = False\n",
    "        self.default_device = \"cpu\"\n",
    "        self.memory_limit = 4  # Default memory limit in GB\n",
    "        \n",
    "        # Configure environment\n",
    "        self._configure()\n",
    "        \n",
    "    def _configure(self):\n",
    "        \"\"\"Configure the environment based on detected hardware\"\"\"\n",
    "        if self.is_arm_mac:\n",
    "            # Mac-specific settings to avoid BLAS issues\n",
    "            os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "            os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "            os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "            os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "            self.memory_limit = 8  # M1/M2 Macs can handle more\n",
    "        elif self.in_colab:\n",
    "            # For Colab, try to detect and use TPU if available\n",
    "            try:\n",
    "                import jax\n",
    "                import jax.tools.colab_tpu\n",
    "                jax.tools.colab_tpu.setup_tpu()\n",
    "                self.has_tpu = True\n",
    "                self.default_device = \"tpu\"\n",
    "                self.memory_limit = 24  # TPUs have more memory\n",
    "                print(\"TPU configured for JAX\")\n",
    "            except Exception:\n",
    "                # Check for GPU\n",
    "                try:\n",
    "                    import jax\n",
    "                    jax.config.update('jax_platform_name', 'gpu')\n",
    "                    if len(jax.devices('gpu')) > 0:\n",
    "                        self.has_gpu = True\n",
    "                        self.default_device = \"gpu\"\n",
    "                        self.memory_limit = 12  # GPUs have decent memory\n",
    "                        print(\"GPU configured for JAX\")\n",
    "                except Exception:\n",
    "                    print(\"No TPU or GPU detected, using CPU\")\n",
    "    \n",
    "    def get_suitable_models(self):\n",
    "        \"\"\"Return a list of models suitable for this environment\"\"\"\n",
    "        all_models = {\n",
    "            # Model name: approximate memory needed in GB\n",
    "            \"distilgpt2\": 0.5,\n",
    "            \"gpt2\": 1.5,\n",
    "            \"gpt2-medium\": 3.0,\n",
    "            \"gpt2-large\": 6.0,\n",
    "            \"gpt2-xl\": 12.0,\n",
    "            \"facebook/opt-125m\": 0.5,\n",
    "            \"facebook/opt-350m\": 1.5,\n",
    "            \"facebook/opt-1.3b\": 5.0,\n",
    "            \"EleutherAI/pythia-160m\": 0.7,\n",
    "            \"EleutherAI/pythia-410m\": 1.8,\n",
    "            \"EleutherAI/pythia-1b\": 4.0\n",
    "        }\n",
    "        \n",
    "        # Filter models based on memory limit\n",
    "        suitable_models = {k: v for k, v in all_models.items() if v <= self.memory_limit}\n",
    "        \n",
    "        # Sort by size (smallest first)\n",
    "        return sorted(suitable_models.keys(), key=lambda x: all_models[x])\n",
    "    \n",
    "    def print_info(self):\n",
    "        \"\"\"Print environment information\"\"\"\n",
    "        print(f\"Platform: {platform.platform()}\")\n",
    "        print(f\"Python version: {platform.python_version()}\")\n",
    "        print(f\"Running in Google Colab: {self.in_colab}\")\n",
    "        print(f\"Running on Mac: {self.is_mac}, Apple Silicon: {self.is_arm_mac}\")\n",
    "        print(f\"Default device: {self.default_device}\")\n",
    "        print(f\"Memory limit (GB): {self.memory_limit}\")\n",
    "        print(f\"\\nModels available for this environment:\")\n",
    "        for model in self.get_suitable_models():\n",
    "            print(f\"  - {model}\")\n",
    "\n",
    "# Initialize environment\n",
    "env = Environment()\n",
    "\n",
    "# Import JAX and transformers\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "env.print_info()\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management\n",
    "\n",
    "Next, let's implement classes for managing pruning results and file storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ResultsManager:\n",
    "    \"\"\"Manages pruning benchmark results\"\"\"\n",
    "    \n",
    "    def __init__(self, results_dir=\"pruning_results\"):\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.results_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.all_results = []\n",
    "        self.results_df = None\n",
    "    \n",
    "    def save_result(self, result):\n",
    "        \"\"\"Save a single result to disk and update dataframe\"\"\"\n",
    "        # Generate filename with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        strategy = result[\"strategy\"]\n",
    "        model = result[\"model\"].replace(\"/\", \"_\")\n",
    "        pruning_level = result[\"pruning_level\"]\n",
    "        \n",
    "        filename = f\"{model}_{strategy}_{pruning_level}_{timestamp}.json\"\n",
    "        filepath = self.results_dir / filename\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(filepath, \"w\") as f:\n",
    "            json.dump(result, f, indent=2)\n",
    "            \n",
    "        # Add to results list and update dataframe\n",
    "        self.all_results.append(result)\n",
    "        self._update_dataframe()\n",
    "        \n",
    "        return filepath\n",
    "    \n",
    "    def load_results(self):\n",
    "        \"\"\"Load all results from disk\"\"\"\n",
    "        self.all_results = []\n",
    "        \n",
    "        # Find all result files\n",
    "        result_files = list(self.results_dir.glob(\"*.json\"))\n",
    "        \n",
    "        if not result_files:\n",
    "            print(f\"No result files found in {self.results_dir}\")\n",
    "            return []\n",
    "        \n",
    "        # Load results\n",
    "        for filepath in result_files:\n",
    "            try:\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    result = json.load(f)\n",
    "                self.all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filepath}: {e}\")\n",
    "        \n",
    "        # Update dataframe\n",
    "        self._update_dataframe()\n",
    "        \n",
    "        return self.all_results\n",
    "    \n",
    "    def _update_dataframe(self):\n",
    "        \"\"\"Convert results to a pandas DataFrame for easier analysis\"\"\"\n",
    "        if not self.all_results:\n",
    "            self.results_df = pd.DataFrame()\n",
    "            return\n",
    "        \n",
    "        # Extract the fields we care about for analysis\n",
    "        data = []\n",
    "        for result in self.all_results:\n",
    "            data.append({\n",
    "                \"model\": result.get(\"model\", \"unknown\"),\n",
    "                \"strategy\": result.get(\"strategy\", \"unknown\"),\n",
    "                \"pruning_level\": result.get(\"pruning_level\", 0),\n",
    "                \"perplexity_before\": result.get(\"perplexity_before\", 0),\n",
    "                \"perplexity_after\": result.get(\"perplexity_after\", 0),\n",
    "                \"perplexity_change\": result.get(\"perplexity_change\", 0),\n",
    "                \"timestamp\": result.get(\"timestamp\", \"\")\n",
    "            })\n",
    "        \n",
    "        self.results_df = pd.DataFrame(data)\n",
    "        \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a summary of all results\"\"\"\n",
    "        if self.results_df is None or self.results_df.empty:\n",
    "            print(\"No results available\")\n",
    "            return\n",
    "        \n",
    "        # Group by model and strategy\n",
    "        groups = self.results_df.groupby([\"model\", \"strategy\"])\n",
    "        \n",
    "        print(f\"Found {len(self.all_results)} result files:\\n\")\n",
    "        \n",
    "        for (model, strategy), group in groups:\n",
    "            print(f\"Model: {model}, Strategy: {strategy}\")\n",
    "            # Sort by pruning level\n",
    "            sorted_group = group.sort_values(\"pruning_level\")\n",
    "            for _, row in sorted_group.iterrows():\n",
    "                print(f\"  Pruning level: {row['pruning_level']:.2f}, \" + \n",
    "                      f\"Perplexity change: {row['perplexity_change']:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    def plot_results(self, figsize=(12, 8)):\n",
    "        \"\"\"Plot results as an interactive visualization\"\"\"\n",
    "        if self.results_df is None or self.results_df.empty:\n",
    "            print(\"No results to plot\")\n",
    "            return\n",
    "        \n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, sharey=False)\n",
    "        \n",
    "        # Colors for different strategies\n",
    "        strategies = self.results_df[\"strategy\"].unique()\n",
    "        strategy_colors = dict(zip(strategies, sns.color_palette(\"colorblind\", len(strategies))))\n",
    "        \n",
    "        # Plot 1: Perplexity change vs pruning level, grouped by model and strategy\n",
    "        for model in sorted(self.results_df[\"model\"].unique()):\n",
    "            model_data = self.results_df[self.results_df[\"model\"] == model]\n",
    "            for strategy in strategies:\n",
    "                strategy_data = model_data[model_data[\"strategy\"] == strategy]\n",
    "                if not strategy_data.empty:\n",
    "                    # Sort by pruning level\n",
    "                    strategy_data = strategy_data.sort_values(\"pruning_level\")\n",
    "                    ax1.plot(strategy_data[\"pruning_level\"], strategy_data[\"perplexity_change\"],\n",
    "                            marker=\"o\", linestyle=\"-\", label=f\"{model} - {strategy}\",\n",
    "                            color=strategy_colors[strategy])\n",
    "        \n",
    "        # Add horizontal line at y=0\n",
    "        ax1.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n",
    "        ax1.set_xlabel(\"Pruning Level\")\n",
    "        ax1.set_ylabel(\"Perplexity Change\")\n",
    "        ax1.set_title(\"Effect of Pruning on Model Perplexity\")\n",
    "        ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        \n",
    "        # Plot 2: Before vs After perplexity, grouped by model and strategy\n",
    "        for model in sorted(self.results_df[\"model\"].unique()):\n",
    "            model_data = self.results_df[self.results_df[\"model\"] == model]\n",
    "            for strategy in strategies:\n",
    "                strategy_data = model_data[model_data[\"strategy\"] == strategy]\n",
    "                if not strategy_data.empty:\n",
    "                    # Point size proportional to pruning level\n",
    "                    sizes = 100 * strategy_data[\"pruning_level\"] + 20\n",
    "                    ax2.scatter(strategy_data[\"perplexity_before\"], strategy_data[\"perplexity_after\"],\n",
    "                               s=sizes, alpha=0.7, label=f\"{model} - {strategy}\",\n",
    "                               color=strategy_colors[strategy])\n",
    "        \n",
    "        # Add diagonal line (y=x)\n",
    "        lims = [0, max(self.results_df[\"perplexity_before\"].max(), \n",
    "                      self.results_df[\"perplexity_after\"].max()) * 1.1]\n",
    "        ax2.plot(lims, lims, 'k--', alpha=0.5)\n",
    "        ax2.set_xlabel(\"Perplexity Before Pruning\")\n",
    "        ax2.set_ylabel(\"Perplexity After Pruning\")\n",
    "        ax2.set_title(\"Perplexity Comparison\")\n",
    "        ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        \n",
    "        # Add legend\n",
    "        handles, labels = [], []\n",
    "        for ax in [ax1, ax2]:\n",
    "            h, l = ax.get_legend_handles_labels()\n",
    "            handles.extend(h)\n",
    "            labels.extend(l)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        by_label = dict(zip(labels, handles))\n",
    "        fig.legend(by_label.values(), by_label.keys(), loc='lower center', \n",
    "                   ncol=min(5, len(by_label)), bbox_to_anchor=(0.5, -0.05))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(bottom=0.15)\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Initialize results manager\n",
    "results_manager = ResultsManager()\n",
    "\n",
    "# Load existing results if any\n",
    "results_manager.load_results()\n",
    "results_manager.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Module\n",
    "\n",
    "Now let's implement the core pruning functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PruningModule:\n",
    "    \"\"\"Core pruning implementation using JAX/Flax\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"distilgpt2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.original_params = None\n",
    "        self.model_type = self._get_model_type(model_name)\n",
    "        self.num_layers = 0\n",
    "        self.num_heads = 0\n",
    "        \n",
    "    def _get_model_type(self, model_name):\n",
    "        \"\"\"Determine model type from name\"\"\"\n",
    "        if \"gpt2\" in model_name.lower():\n",
    "            return \"gpt2\"\n",
    "        elif \"opt\" in model_name.lower():\n",
    "            return \"opt\"\n",
    "        elif \"pythia\" in model_name.lower():\n",
    "            return \"pythia\"\n",
    "        else:\n",
    "            # Default to GPT-2 structure\n",
    "            return \"gpt2\"\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model and tokenizer\"\"\"\n",
    "        print(f\"Loading model {self.model_name}...\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = FlaxAutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "            self.original_params = self.model.params\n",
    "            \n",
    "            # Get model details based on model type\n",
    "            if self.model_type == \"gpt2\":\n",
    "                self.num_layers = len(self.original_params[\"transformer\"][\"h\"])\n",
    "                self.num_heads = 12  # Standard for most GPT-2 variants\n",
    "                if \"distil\" in self.model_name.lower():\n",
    "                    self.num_layers = 6  # DistilGPT2 has 6 layers\n",
    "                elif \"medium\" in self.model_name.lower():\n",
    "                    self.num_heads = 16  # GPT2-medium has 16 heads\n",
    "                elif \"large\" in self.model_name.lower():\n",
    "                    self.num_heads = 20  # GPT2-large has 20 heads\n",
    "                elif \"xl\" in self.model_name.lower():\n",
    "                    self.num_heads = 25  # GPT2-xl has 25 heads\n",
    "            elif self.model_type == \"opt\":\n",
    "                self.num_layers = len(self.original_params[\"model\"][\"decoder\"][\"layers\"])\n",
    "                # Extract num_heads from config\n",
    "                self.num_heads = 12  # Default, will be refined below\n",
    "                try:\n",
    "                    if \"125m\" in self.model_name.lower():\n",
    "                        self.num_heads = 12\n",
    "                    elif \"350m\" in self.model_name.lower():\n",
    "                        self.num_heads = 16\n",
    "                    elif \"1.3b\" in self.model_name.lower():\n",
    "                        self.num_heads = 32\n",
    "                except Exception:\n",
    "                    pass  # Stick with default\n",
    "            elif self.model_type == \"pythia\":\n",
    "                self.num_layers = len(self.original_params[\"transformer\"][\"h\"])\n",
    "                # Extract num_heads based on model size\n",
    "                self.num_heads = 12  # Default\n",
    "                try:\n",
    "                    if \"160m\" in self.model_name.lower():\n",
    "                        self.num_heads = 12\n",
    "                    elif \"410m\" in self.model_name.lower():\n",
    "                        self.num_heads = 16\n",
    "                    elif \"1b\" in self.model_name.lower():\n",
    "                        self.num_heads = 16\n",
    "                except Exception:\n",
    "                    pass  # Stick with default\n",
    "            \n",
    "            print(f\"Model loaded successfully. Layers: {self.num_layers}, Heads per layer: {self.num_heads}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def prune_head(self, params, layer_idx, head_idx):\n",
    "        \"\"\"Zero out weights for a specific attention head\"\"\"\n",
    "        if self.model_type == \"gpt2\":\n",
    "            # Access path to transformer layers\n",
    "            transformer_path = \"transformer\"\n",
    "            layer_path = \"h\"\n",
    "            layer_key = str(layer_idx)\n",
    "            attn_path = \"attn\"\n",
    "            \n",
    "            # Get attention block\n",
    "            attn_block = params[transformer_path][layer_path][layer_key][attn_path]\n",
    "            \n",
    "            # Calculate head dimensions\n",
    "            if \"c_attn\" in attn_block:\n",
    "                hidden_size = attn_block[\"c_attn\"][\"kernel\"].shape[1]\n",
    "            else:\n",
    "                # Fallback using output projection\n",
    "                hidden_size = attn_block[\"c_proj\"][\"kernel\"].shape[0]\n",
    "                \n",
    "            head_size = hidden_size // self.num_heads\n",
    "            \n",
    "            # Calculate indices for this head\n",
    "            start_idx = head_idx * head_size\n",
    "            end_idx = (head_idx + 1) * head_size\n",
    "            \n",
    "            # Zero out the output projection for this head\n",
    "            output_proj = attn_block[\"c_proj\"][\"kernel\"]\n",
    "            zeros = jnp.zeros_like(output_proj[start_idx:end_idx, :])\n",
    "            output_proj = output_proj.at[start_idx:end_idx, :].set(zeros)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[transformer_path][layer_path][layer_key][attn_path][\"c_proj\"][\"kernel\"] = output_proj\n",
    "            \n",
    "        elif self.model_type == \"opt\":\n",
    "            # For OPT models\n",
    "            model_path = \"model\"\n",
    "            decoder_path = \"decoder\"\n",
    "            layers_path = \"layers\"\n",
    "            layer_key = str(layer_idx)\n",
    "            attn_path = \"self_attn\"\n",
    "            \n",
    "            # Get attention block\n",
    "            attn_block = params[model_path][decoder_path][layers_path][layer_key][attn_path]\n",
    "            \n",
    "            # Calculate head dimensions\n",
    "            hidden_size = attn_block[\"out_proj\"][\"kernel\"].shape[0]\n",
    "            head_size = hidden_size // self.num_heads\n",
    "            \n",
    "            # Calculate indices for this head\n",
    "            start_idx = head_idx * head_size\n",
    "            end_idx = (head_idx + 1) * head_size\n",
    "            \n",
    "            # Zero out the output projection for this head\n",
    "            output_proj = attn_block[\"out_proj\"][\"kernel\"]\n",
    "            zeros = jnp.zeros_like(output_proj[start_idx:end_idx, :])\n",
    "            output_proj = output_proj.at[start_idx:end_idx, :].set(zeros)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[model_path][decoder_path][layers_path][layer_key][attn_path][\"out_proj\"][\"kernel\"] = output_proj\n",
    "            \n",
    "        elif self.model_type == \"pythia\":\n",
    "            # For Pythia models (similar to GPT-2)\n",
    "            transformer_path = \"transformer\"\n",
    "            layer_path = \"h\"\n",
    "            layer_key = str(layer_idx)\n",
    "            attn_path = \"attn\"\n",
    "            \n",
    "            # Get attention block\n",
    "            attn_block = params[transformer_path][layer_path][layer_key][attn_path]\n",
    "            \n",
    "            # Calculate head dimensions\n",
    "            hidden_size = attn_block[\"proj\"][\"kernel\"].shape[0]\n",
    "            head_size = hidden_size // self.num_heads\n",
    "            \n",
    "            # Calculate indices for this head\n",
    "            start_idx = head_idx * head_size\n",
    "            end_idx = (head_idx + 1) * head_size\n",
    "            \n",
    "            # Zero out the output projection for this head\n",
    "            output_proj = attn_block[\"proj\"][\"kernel\"]\n",
    "            zeros = jnp.zeros_like(output_proj[start_idx:end_idx, :])\n",
    "            output_proj = output_proj.at[start_idx:end_idx, :].set(zeros)\n",
    "            \n",
    "            # Update parameters\n",
    "            params[transformer_path][layer_path][layer_key][attn_path][\"proj\"][\"kernel\"] = output_proj\n",
    "        \n",
    "        return params\n",
    "    \n",
    "    def evaluate_perplexity(self, params, text):\n",
    "        \"\"\"Evaluate model perplexity on text\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"jax\")\n",
    "        \n",
    "        # Get logits\n",
    "        outputs = self.model(**inputs, params=params)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        \n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[:, :-1]\n",
    "        shift_labels = input_ids[:, 1:]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        loss = jnp.mean(\n",
    "            -jnp.sum(\n",
    "                jax.nn.log_softmax(shift_logits) * jax.nn.one_hot(shift_labels, shift_logits.shape[-1]),\n",
    "                axis=-1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Return perplexity\n",
    "        return jnp.exp(loss).item()\n",
    "    \n",
    "    def generate_text(self, params, prompt, max_length=50):\n",
    "        \"\"\"Generate text using the model\"\"\"\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"jax\")\n",
    "        \n",
    "        # Generate text\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            params=params,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=40,\n",
    "            top_p=0.95,\n",
    "            temperature=0.8\n",
    "        )\n",
    "        \n",
    "        # Decode output\n",
    "        text = self.tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Strategies\n",
    "\n",
    "Let's implement different strategies for selecting heads to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PruningStrategy:\n",
    "    \"\"\"Base class for pruning strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, pruning_module):\n",
    "        self.pruning_module = pruning_module\n",
    "    \n",
    "    def get_head_importance(self, params):\n",
    "        \"\"\"Calculate importance for all heads\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement get_head_importance\")\n",
    "    \n",
    "    def prune_heads(self, params, head_indices):\n",
    "        \"\"\"Prune specified heads\"\"\"\n",
    "        pruned_params = jax.tree_util.tree_map(lambda x: x, params)  # Deep copy\n",
    "        \n",
    "        for layer_idx, head_idx in head_indices:\n",
    "            pruned_params = self.pruning_module.prune_head(pruned_params, layer_idx, head_idx)\n",
    "            \n",
    "        return pruned_params\n",
    "\n",
    "class RandomStrategy(PruningStrategy):\n",
    "    \"\"\"Random pruning strategy\"\"\"\n",
    "    \n",
    "    def get_head_importance(self, params):\n",
    "        \"\"\"Assign random importance to heads\"\"\"\n",
    "        all_head_importance = []\n",
    "        \n",
    "        for layer_idx in range(self.pruning_module.num_layers):\n",
    "            for head_idx in range(self.pruning_module.num_heads):\n",
    "                # Random importance score\n",
    "                score = random.random()\n",
    "                all_head_importance.append((layer_idx, head_idx, score))\n",
    "        \n",
    "        return all_head_importance\n",
    "\n",
    "class MagnitudeStrategy(PruningStrategy):\n",
    "    \"\"\"Magnitude-based pruning strategy\"\"\"\n",
    "    \n",
    "    def get_head_importance(self, params):\n",
    "        \"\"\"Calculate importance based on weight magnitudes\"\"\"\n",
    "        all_head_importance = []\n",
    "        model_type = self.pruning_module.model_type\n",
    "        \n",
    "        for layer_idx in range(self.pruning_module.num_layers):\n",
    "            for head_idx in range(self.pruning_module.num_heads):\n",
    "                # Get head weights based on model type\n",
    "                if model_type == \"gpt2\":\n",
    "                    # Access attention output projection\n",
    "                    transformer_path = \"transformer\"\n",
    "                    layer_path = \"h\"\n",
    "                    layer_key = str(layer_idx)\n",
    "                    attn_path = \"attn\"\n",
    "                    \n",
    "                    attn_block = params[transformer_path][layer_path][layer_key][attn_path]\n",
    "                    output_proj = attn_block[\"c_proj\"][\"kernel\"]\n",
    "                    \n",
    "                    # Calculate head dimensions\n",
    "                    head_size = output_proj.shape[0] // self.pruning_module.num_heads\n",
    "                    \n",
    "                    # Get weights for this head\n",
    "                    start_idx = head_idx * head_size\n",
    "                    end_idx = (head_idx + 1) * head_size\n",
    "                    head_weights = output_proj[start_idx:end_idx, :]\n",
    "                    \n",
    "                elif model_type == \"opt\":\n",
    "                    # For OPT models\n",
    "                    model_path = \"model\"\n",
    "                    decoder_path = \"decoder\"\n",
    "                    layers_path = \"layers\"\n",
    "                    layer_key = str(layer_idx)\n",
    "                    attn_path = \"self_attn\"\n",
    "                    \n",
    "                    attn_block = params[model_path][decoder_path][layers_path][layer_key][attn_path]\n",
    "                    output_proj = attn_block[\"out_proj\"][\"kernel\"]\n",
    "                    \n",
    "                    # Calculate head dimensions\n",
    "                    head_size = output_proj.shape[0] // self.pruning_module.num_heads\n",
    "                    \n",
    "                    # Get weights for this head\n",
    "                    start_idx = head_idx * head_size\n",
    "                    end_idx = (head_idx + 1) * head_size\n",
    "                    head_weights = output_proj[start_idx:end_idx, :]\n",
    "                    \n",
    "                elif model_type == \"pythia\":\n",
    "                    # For Pythia models\n",
    "                    transformer_path = \"transformer\"\n",
    "                    layer_path = \"h\"\n",
    "                    layer_key = str(layer_idx)\n",
    "                    attn_path = \"attn\"\n",
    "                    \n",
    "                    attn_block = params[transformer_path][layer_path][layer_key][attn_path]\n",
    "                    output_proj = attn_block[\"proj\"][\"kernel\"]\n",
    "                    \n",
    "                    # Calculate head dimensions\n",
    "                    head_size = output_proj.shape[0] // self.pruning_module.num_heads\n",
    "                    \n",
    "                    # Get weights for this head\n",
    "                    start_idx = head_idx * head_size\n",
    "                    end_idx = (head_idx + 1) * head_size\n",
    "                    head_weights = output_proj[start_idx:end_idx, :]\n",
    "                \n",
    "                # Calculate importance as L2 norm of weights\n",
    "                importance = jnp.linalg.norm(head_weights).item()\n",
    "                all_head_importance.append((layer_idx, head_idx, importance))\n",
    "        \n",
    "        return all_head_importance\n",
    "\n",
    "class AttentionEntropyStrategy(PruningStrategy):\n",
    "    \"\"\"Entropy-based pruning strategy using attention patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, pruning_module, sample_text=None):\n",
    "        super().__init__(pruning_module)\n",
    "        \n",
    "        # Sample text for evaluating attention entropy\n",
    "        if sample_text is None:\n",
    "            self.sample_text = [\n",
    "                \"The quick brown fox jumps over the lazy dog\",\n",
    "                \"Artificial intelligence is transforming the world\",\n",
    "                \"Machine learning models can process large amounts of data\",\n",
    "                \"The future of technology depends on sustainable practices\",\n",
    "                \"Researchers are working on new methods to improve efficiency\"\n",
    "            ]\n",
    "        else:\n",
    "            self.sample_text = sample_text if isinstance(sample_text, list) else [sample_text]\n",
    "    \n",
    "    def get_head_importance(self, params):\n",
    "        \"\"\"Calculate importance based on fallback to magnitude\"\"\"\n",
    "        # For simplicity and compatibility across model types,\n",
    "        # we'll just use magnitude-based pruning as a proxy\n",
    "        magnitude_strategy = MagnitudeStrategy(self.pruning_module)\n",
    "        return magnitude_strategy.get_head_importance(params)\n",
    "\n",
    "# Factory function to get strategy by name\n",
    "def get_strategy(name, pruning_module, sample_text=None):\n",
    "    \"\"\"Get pruning strategy by name\"\"\"\n",
    "    if name.lower() == \"random\":\n",
    "        return RandomStrategy(pruning_module)\n",
    "    elif name.lower() == \"magnitude\":\n",
    "        return MagnitudeStrategy(pruning_module)\n",
    "    elif name.lower() == \"entropy\":\n",
    "        return AttentionEntropyStrategy(pruning_module, sample_text)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Runner\n",
    "\n",
    "Now let's implement the main benchmark runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PruningBenchmark:\n",
    "    \"\"\"Main benchmark runner\"\"\"\n",
    "    \n",
    "    def __init__(self, results_manager):\n",
    "        self.results_manager = results_manager\n",
    "    \n",
    "    def run_single_benchmark(self, model_name, strategy_name, pruning_level, prompt):\n",
    "        \"\"\"Run a single pruning benchmark\"\"\"\n",
    "        print(f\"\\nRunning benchmark for {model_name} with {strategy_name} strategy at {pruning_level:.2f} pruning level\")\n",
    "        \n",
    "        # Initialize pruning module\n",
    "        pruning_module = PruningModule(model_name)\n",
    "        if not pruning_module.load_model():\n",
    "            print(f\"Failed to load model {model_name}\")\n",
    "            return None\n",
    "        \n",
    "        # Get strategy\n",
    "        strategy = get_strategy(strategy_name, pruning_module, prompt)\n",
    "        \n",
    "        # Get original parameters and create a copy for pruning\n",
    "        original_params = pruning_module.original_params\n",
    "        params = jax.tree_util.tree_map(lambda x: x, original_params)  # Deep copy\n",
    "        \n",
    "        # Evaluate model before pruning\n",
    "        print(\"Evaluating model before pruning...\")\n",
    "        perplexity_before = pruning_module.evaluate_perplexity(params, prompt)\n",
    "        print(f\"Perplexity before pruning: {perplexity_before:.4f}\")\n",
    "        \n",
    "        generated_before = pruning_module.generate_text(params, prompt)\n",
    "        print(f\"Generated (before pruning): {generated_before}\")\n",
    "        \n",
    "        # Calculate importance scores\n",
    "        print(\"\\nCalculating head importance...\")\n",
    "        all_head_importance = strategy.get_head_importance(params)\n",
    "        \n",
    "        # Sort by importance (ascending)\n",
    "        all_head_importance.sort(key=lambda x: x[2])\n",
    "        \n",
    "        # Determine number of heads to prune\n",
    "        total_heads = pruning_module.num_layers * pruning_module.num_heads\n",
    "        heads_to_prune = int(total_heads * pruning_level)\n",
    "        print(f\"Pruning {heads_to_prune} out of {total_heads} heads\")\n",
    "        \n",
    "        # Get head indices to prune (least important first)\n",
    "        head_indices = [(l, h) for l, h, _ in all_head_importance[:heads_to_prune]]\n",
    "        \n",
    "        # Prune heads\n",
    "        print(\"\\nPruning heads...\")\n",
    "        pruned_params = strategy.prune_heads(params, head_indices)\n",
    "        \n",
    "        # Evaluate model after pruning\n",
    "        print(\"\\nEvaluating model after pruning...\")\n",
    "        perplexity_after = pruning_module.evaluate_perplexity(pruned_params, prompt)\n",
    "        print(f\"Perplexity after pruning: {perplexity_after:.4f}\")\n",
    "        print(f\"Perplexity change: {perplexity_after - perplexity_before:.4f}\")\n",
    "        \n",
    "        generated_after = pruning_module.generate_text(pruned_params, prompt)\n",
    "        print(f\"Generated (after pruning): {generated_after}\")\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            \"model\": model_name,\n",
    "            \"strategy\": strategy_name,\n",
    "            \"pruning_level\": pruning_level,\n",
    "            \"pruned_heads\": heads_to_prune,\n",
    "            \"total_heads\": total_heads,\n",
    "            \"prompt\": prompt,\n",
    "            \"perplexity_before\": float(perplexity_before),\n",
    "            \"perplexity_after\": float(perplexity_after),\n",
    "            \"perplexity_change\": float(perplexity_after - perplexity_before),\n",
    "            \"generated_before\": generated_before,\n",
    "            \"generated_after\": generated_after,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "        \n",
    "        # Save result\n",
    "        self.results_manager.save_result(result)\n",
    "        \n",
    "        print(\"\\nBenchmark completed successfully!\")\n",
    "        return result\n",
    "    \n",
    "    def run_multiple_benchmarks(self, models=None, strategies=None, pruning_levels=None, prompt=None, max_runtime=None):\n",
    "        \"\"\"Run multiple benchmarks with different parameters\"\"\"\n",
    "        # Default values\n",
    "        if models is None:\n",
    "            models = env.get_suitable_models()\n",
    "        if strategies is None:\n",
    "            strategies = [\"random\", \"magnitude\"]\n",
    "        if pruning_levels is None:\n",
    "            pruning_levels = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "        if prompt is None:\n",
    "            prompt = \"Artificial intelligence will transform\"\n",
    "            \n",
    "        # Start time for runtime tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate all benchmark combinations\n",
    "        benchmarks = []\n",
    "        for model in models:\n",
    "            for strategy in strategies:\n",
    "                for level in pruning_levels:\n",
    "                    benchmarks.append((model, strategy, level, prompt))\n",
    "        \n",
    "        # Shuffle to get more diverse results early\n",
    "        random.shuffle(benchmarks)\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=len(benchmarks), desc=\"Running benchmarks\")\n",
    "        \n",
    "        # Run benchmarks\n",
    "        results = []\n",
    "        for i, (model, strategy, level, bench_prompt) in enumerate(benchmarks):\n",
    "            # Check if we've exceeded the runtime limit\n",
    "            if max_runtime is not None and time.time() - start_time > max_runtime:\n",
    "                print(f\"\\nReached maximum runtime of {max_runtime/3600:.1f} hours\")\n",
    "                break\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_description(f\"Running {model}, {strategy}, {level:.2f}\")\n",
    "            \n",
    "            # Run benchmark\n",
    "            try:\n",
    "                result = self.run_single_benchmark(model, strategy, level, bench_prompt)\n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Plot intermediate results every few benchmarks\n",
    "                if (i + 1) % 3 == 0 or i == len(benchmarks) - 1:\n",
    "                    self.results_manager.plot_results()\n",
    "                    plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in benchmark {model}, {strategy}, {level:.2f}: {e}\")\n",
    "                # Still update progress bar\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "        \n",
    "        # Final results\n",
    "        print(f\"\\nCompleted {len(results)} benchmarks out of {len(benchmarks)} attempted\")\n",
    "        runtime = time.time() - start_time\n",
    "        print(f\"Total runtime: {runtime/3600:.2f} hours ({runtime/60:.2f} minutes)\")\n",
    "        \n",
    "        # Plot final results\n",
    "        self.results_manager.plot_results()\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize benchmark runner\n",
    "benchmark = PruningBenchmark(results_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Single Benchmark\n",
    "\n",
    "Let's run a single benchmark to test our implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a single benchmark\n",
    "model_name = env.get_suitable_models()[0]  # Use the smallest model\n",
    "result = benchmark.run_single_benchmark(\n",
    "    model_name=model_name,\n",
    "    strategy_name=\"random\",\n",
    "    pruning_level=0.1,\n",
    "    prompt=\"Artificial intelligence will transform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Multiple Benchmarks\n",
    "\n",
    "Now let's run multiple benchmarks to collect comprehensive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get available models\n",
    "available_models = env.get_suitable_models()\n",
    "print(f\"Available models: {available_models}\")\n",
    "\n",
    "# Select a subset of models to use\n",
    "models_to_test = available_models[:2]  # Use the first 2 models\n",
    "print(f\"Using models: {models_to_test}\")\n",
    "\n",
    "# Select strategies\n",
    "strategies_to_test = [\"random\", \"magnitude\"]\n",
    "\n",
    "# Select pruning levels\n",
    "pruning_levels_to_test = [0.1, 0.3, 0.5]\n",
    "\n",
    "# Set the prompt\n",
    "prompt = \"Artificial intelligence will transform society by\"\n",
    "\n",
    "# Set maximum runtime (in seconds) - 1 hour for Colab, 20 minutes for local\n",
    "max_runtime = 3600 if env.in_colab else 1200  # 1 hour for Colab, 20 min for local\n",
    "\n",
    "# Run the benchmarks\n",
    "results = benchmark.run_multiple_benchmarks(\n",
    "    models=models_to_test,\n",
    "    strategies=strategies_to_test,\n",
    "    pruning_levels=pruning_levels_to_test,\n",
    "    prompt=prompt,\n",
    "    max_runtime=max_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View and Analyze Results\n",
    "\n",
    "Let's visualize and analyze our benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all results\n",
    "results_manager.load_results()\n",
    "\n",
    "# Print summary\n",
    "results_manager.print_summary()\n",
    "\n",
    "# Plot results\n",
    "fig = results_manager.plot_results(figsize=(14, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis\n",
    "\n",
    "Let's create some advanced visualizations to better understand our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Advanced analysis if we have results\n",
    "if results_manager.results_df is not None and not results_manager.results_df.empty:\n",
    "    # Set figure size for all plots\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # 1. Box plot of perplexity change by strategy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(x=\"strategy\", y=\"perplexity_change\", data=results_manager.results_df)\n",
    "    plt.title(\"Perplexity Change by Strategy\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # 2. Box plot of perplexity change by model\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(x=\"model\", y=\"perplexity_change\", data=results_manager.results_df)\n",
    "    plt.title(\"Perplexity Change by Model\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    # 3. Heatmap of average perplexity change (strategy vs pruning level)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    pivot_df = results_manager.results_df.pivot_table(\n",
    "        index=\"strategy\", \n",
    "        columns=\"pruning_level\", \n",
    "        values=\"perplexity_change\", \n",
    "        aggfunc=\"mean\"\n",
    "    )\n",
    "    sns.heatmap(pivot_df, annot=True, cmap=\"RdYlGn_r\", center=0)\n",
    "    plt.title(\"Average Perplexity Change by Strategy and Pruning Level\")\n",
    "    \n",
    "    # 4. Relationship between perplexity before and change\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.scatterplot(\n",
    "        x=\"perplexity_before\", \n",
    "        y=\"perplexity_change\", \n",
    "        hue=\"strategy\", \n",
    "        size=\"pruning_level\",\n",
    "        sizes=(50, 200),\n",
    "        data=results_manager.results_df\n",
    "    )\n",
    "    plt.title(\"Perplexity Change vs Initial Perplexity\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Overnight Benchmarks\n",
    "\n",
    "For running comprehensive benchmarks overnight in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration for overnight benchmarks\n",
    "OVERNIGHT_MODELS = env.get_suitable_models()  # Use all available models\n",
    "OVERNIGHT_STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "OVERNIGHT_PRUNING_LEVELS = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "OVERNIGHT_PROMPTS = [\n",
    "    \"Artificial intelligence will transform society by\",\n",
    "    \"The future of technology depends on new innovations that\",\n",
    "    \"Scientists are developing advanced systems that can\"\n",
    "]\n",
    "\n",
    "# Max runtime for overnight: 8 hours\n",
    "OVERNIGHT_MAX_RUNTIME = 8 * 3600  # 8 hours in seconds\n",
    "\n",
    "print(\"Overnight benchmark configuration:\")\n",
    "print(f\"Models: {OVERNIGHT_MODELS}\")\n",
    "print(f\"Strategies: {OVERNIGHT_STRATEGIES}\")\n",
    "print(f\"Pruning levels: {OVERNIGHT_PRUNING_LEVELS}\")\n",
    "print(f\"Number of prompts: {len(OVERNIGHT_PROMPTS)}\")\n",
    "print(f\"Maximum runtime: {OVERNIGHT_MAX_RUNTIME/3600:.1f} hours\")\n",
    "\n",
    "# Total benchmark combinations\n",
    "total_benchmarks = len(OVERNIGHT_MODELS) * len(OVERNIGHT_STRATEGIES) * len(OVERNIGHT_PRUNING_LEVELS)\n",
    "print(f\"Total benchmark combinations: {total_benchmarks}\")\n",
    "\n",
    "# Uncomment to run overnight benchmarks\n",
    "# overnight_results = benchmark.run_multiple_benchmarks(\n",
    "#     models=OVERNIGHT_MODELS,\n",
    "#     strategies=OVERNIGHT_STRATEGIES,\n",
    "#     pruning_levels=OVERNIGHT_PRUNING_LEVELS,\n",
    "#     prompt=OVERNIGHT_PROMPTS[0],  # Use the first prompt\n",
    "#     max_runtime=OVERNIGHT_MAX_RUNTIME\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}