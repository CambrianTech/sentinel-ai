{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a9cfc84",
   "metadata": {},
   "source": [
    "# Adaptive Attention Visualization\n",
    "\n",
    "This notebook visualizes how attention head activations dynamically adapt (expand or prune) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1fe5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets seaborn matplotlib torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from models.adaptive_transformer import AdaptiveTransformerModel\n",
    "from controller.controller_ann import ANNController\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962a806b",
   "metadata": {},
   "source": [
    "### Load a Pretrained Adaptive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7477e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilgpt2'\n",
    "num_layers = 6  # DistilGPT2 has 6 layers\n",
    "num_heads = 12\n",
    "\n",
    "controller = ANNController(num_layers=num_layers, num_heads=num_heads)\n",
    "model = AdaptiveTransformerModel.from_pretrained(model_name, controller).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fc57a0",
   "metadata": {},
   "source": [
    "### Visualize Attention Head Gates Before Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_values = torch.sigmoid(controller.gate_logits).detach().cpu().numpy()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(gate_values, cmap='viridis', annot=True, vmin=0, vmax=1)\n",
    "plt.title('Initial Attention Head Gate Values')\n",
    "plt.xlabel('Head')\n",
    "plt.ylabel('Layer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf30ee6",
   "metadata": {},
   "source": [
    "### Simulate Adaptive Changes and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate adaptive updates\n",
    "for step in range(5):\n",
    "    fake_metrics = {'entropy': torch.rand(num_layers, num_heads)}\n",
    "    controller.update_gates(fake_metrics)\n",
    "\n",
    "    gate_values = torch.sigmoid(controller.gate_logits).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(gate_values, cmap='viridis', annot=True, vmin=0, vmax=1)\n",
    "    plt.title(f'Attention Head Gates After Adaptation Step {step+1}')\n",
    "    plt.xlabel('Head')\n",
    "    plt.ylabel('Layer')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
