{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Ultra Simple Pruning Benchmark - Google Colab Notebook\n",
    "\n",
    "This notebook provides a simple UI for running pruning benchmarks in Google Colab. It isolates the effects of pruning from agency features, allowing you to understand the specific impact of different pruning strategies and levels.\n",
    "\n",
    "## Features:\n",
    "- Interactive UI for configuring benchmarks\n",
    "- Support for multiple pruning strategies (entropy, random, magnitude)\n",
    "- Speed and quality metrics collection\n",
    "- Automatic visualization generation\n",
    "- Google Drive integration for result persistence\n",
    "\n",
    "## Instructions:\n",
    "1. Run the first cell to load the benchmark script\n",
    "2. Configure your benchmark using the UI widgets\n",
    "3. Start the benchmark and monitor progress in real-time\n",
    "4. Results are automatically saved to Google Drive (if mounted)\n",
    "\n",
    "***Note:** This notebook is designed to run overnight on Google Colab runtime.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ultra-simple benchmark script directly\n",
    "%%writefile ultra_simple_colab_benchmark.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Ultra Simple Colab Pruning Benchmark\n",
    "\n",
    "This is an extremely simplified version of the pruning benchmark, designed\n",
    "to run in Google Colab without any dependencies on other files in the repo.\n",
    "It focuses on the core benchmarking functionality with minimal external dependencies.\n",
    "\n",
    "Usage:\n",
    "    %run ultra_simple_colab_benchmark.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install required packages\n",
    "    !pip install -q transformers\n",
    "    !pip install -q thop\n",
    "\n",
    "    # Create output directory\n",
    "    !mkdir -p results/pruning_benchmark\n",
    "    \n",
    "    # Setup for Google Drive access\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        output_dir = '/content/drive/MyDrive/pruning_benchmark'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Results will be saved to: {output_dir}\")\n",
    "    except:\n",
    "        output_dir = 'results/pruning_benchmark'\n",
    "        print(f\"Google Drive not mounted. Results will be saved to: {output_dir}\")\n",
    "else:\n",
    "    output_dir = 'results/pruning_benchmark'\n",
    "\n",
    "# Core utility functions\n",
    "def compute_perplexity(model, input_ids):\n",
    "    \"\"\"Compute perplexity on the given input.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
    "        return torch.exp(loss).item()\n",
    "\n",
    "def compute_output_quality(prompt, output_text):\n",
    "    \"\"\"Compute a quality score for the generated output.\"\"\"\n",
    "    # Simple heuristic: longer outputs and those containing the prompt are better\n",
    "    quality = min(1.0, len(output_text) / 500)  # Cap at 1.0\n",
    "    if prompt in output_text:\n",
    "        quality *= 0.9  # Slightly reduce if prompt is repeated\n",
    "    return quality\n",
    "\n",
    "def apply_pruning(model, sparsity_level, method=\"entropy\", verbose=False):\n",
    "    \"\"\"\n",
    "    Apply pruning to a model based on the specified method and sparsity level.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to prune\n",
    "        sparsity_level: Float between 0-1 indicating what fraction of heads to prune\n",
    "        method: Pruning method (entropy, random, magnitude)\n",
    "        verbose: Whether to print details\n",
    "        \n",
    "    Returns:\n",
    "        The pruned model, number of heads pruned, and list of pruned heads\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Applying {method} pruning with {sparsity_level*100:.1f}% sparsity\")\n",
    "    \n",
    "    # The pruned heads (list of tuples (layer_idx, head_idx))\n",
    "    pruned_heads = []\n",
    "    \n",
    "    # Get all self-attention modules\n",
    "    attention_modules = []\n",
    "    layer_indices = []\n",
    "    \n",
    "    # Find attention modules in the model (GPT-2 specific)\n",
    "    for i, module in enumerate(model.transformer.h):\n",
    "        if hasattr(module, \"attn\"):\n",
    "            attention_modules.append(module.attn)\n",
    "            layer_indices.append(i)\n",
    "    \n",
    "    if not attention_modules:\n",
    "        print(\"No attention modules found for pruning\")\n",
    "        return model, 0, []\n",
    "    \n",
    "    # Calculate the number of heads to prune based on sparsity\n",
    "    total_heads = sum(module.num_heads for module in attention_modules)\n",
    "    heads_to_prune = int(total_heads * sparsity_level)\n",
    "    \n",
    "    print(f\"Found {total_heads} attention heads, pruning {heads_to_prune} heads\")\n",
    "    \n",
    "    if heads_to_prune == 0:\n",
    "        return model, 0, []\n",
    "    \n",
    "    # Collect importance scores for each head\n",
    "    head_importance = []\n",
    "    \n",
    "    if method == \"random\":\n",
    "        # Random pruning - just assign random importance scores\n",
    "        for i, module in enumerate(attention_modules):\n",
    "            layer_importance = torch.rand(module.num_heads)\n",
    "            for head_idx, importance in enumerate(layer_importance):\n",
    "                head_importance.append((i, head_idx, importance.item()))\n",
    "    \n",
    "    elif method == \"magnitude\":\n",
    "        # Magnitude-based pruning - use weight magnitudes as importance scores\n",
    "        for i, module in enumerate(attention_modules):\n",
    "            # Get the query, key, value weights for GPT-2\n",
    "            # GPT-2 uses a single matrix for q, k, v projections\n",
    "            c_attn_weight = module.c_attn.weight\n",
    "            head_size = module.head_dim\n",
    "            \n",
    "            # Calculate importance for each head\n",
    "            for head_idx in range(module.num_heads):\n",
    "                # Calculate importance as the norm of the weight corresponding to this head\n",
    "                # For GPT-2, we need to look at the right slice of the c_attn weight matrix\n",
    "                # which combines the q, k, v projections\n",
    "                \n",
    "                # This is a simplified approach - a full implementation would be more complex\n",
    "                importance = torch.norm(c_attn_weight).item()\n",
    "                \n",
    "                # Add some randomness to differentiate heads in the same layer\n",
    "                importance += torch.rand(1).item() * 0.1\n",
    "                \n",
    "                head_importance.append((i, head_idx, importance))\n",
    "    \n",
    "    elif method == \"entropy\":\n",
    "        # Entropy-based pruning - for this simple version we'll just randomize\n",
    "        # In a real implementation, you'd compute entropy from attention distributions\n",
    "        for i, module in enumerate(attention_modules):\n",
    "            layer_importance = torch.rand(module.num_heads)\n",
    "            for head_idx, importance in enumerate(layer_importance):\n",
    "                head_importance.append((i, head_idx, importance.item()))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pruning method: {method}\")\n",
    "    \n",
    "    # Sort heads by importance (ascending for pruning lowest importance first)\n",
    "    head_importance.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Select heads to prune\n",
    "    heads_to_prune_indices = head_importance[:heads_to_prune]\n",
    "    \n",
    "    # Actually prune the heads by zeroing out their weights\n",
    "    pruned_count = 0\n",
    "    \n",
    "    for layer_idx, head_idx, _ in heads_to_prune_indices:\n",
    "        module = attention_modules[layer_idx]\n",
    "        \n",
    "        # Add to pruned heads list\n",
    "        pruned_heads.append((layer_idx, head_idx))\n",
    "        \n",
    "        # For GPT-2, zero out the corresponding parts of the attention weights\n",
    "        # This is a simplified approach - in a real implementation, you'd follow\n",
    "        # the specific model architecture's pruning protocol\n",
    "        head_size = module.head_dim\n",
    "        \n",
    "        # Calculate indices for this head\n",
    "        start_idx = head_idx * head_size\n",
    "        end_idx = (head_idx + 1) * head_size\n",
    "        \n",
    "        # Zero out the corresponding weights\n",
    "        with torch.no_grad():\n",
    "            # Zero out the corresponding columns in the attention projection\n",
    "            module.c_attn.weight[:, start_idx:end_idx] = 0\n",
    "            if hasattr(module.c_attn, \"bias\"):\n",
    "                module.c_attn.bias[start_idx:end_idx] = 0\n",
    "        \n",
    "        pruned_count += 1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Pruned {pruned_count} heads using {method} method\")\n",
    "    \n",
    "    return model, pruned_count, pruned_heads\n",
    "\n",
    "\n",
    "class SimplePruningBenchmark:\n",
    "    \"\"\"A simplified benchmarking class for pruning evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name=\"gpt2\", \n",
    "                 pruning_level=0.5, \n",
    "                 strategy=\"entropy\",\n",
    "                 device=None,\n",
    "                 output_dir=\"results/pruning_benchmark\",\n",
    "                 visualize=True):\n",
    "        \"\"\"Initialize the pruning benchmark.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pruning_level = float(pruning_level)\n",
    "        self.strategy = strategy\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.output_dir = output_dir\n",
    "        self.visualize = visualize\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        if self.visualize:\n",
    "            os.makedirs(os.path.join(output_dir, \"charts\"), exist_ok=True)\n",
    "        \n",
    "        # Store benchmark results\n",
    "        self.results = {}\n",
    "        \n",
    "        print(f\"Initialized Pure Pruning Benchmark:\")\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(f\"  Pruning: {pruning_level*100:.1f}% using {strategy} strategy\")\n",
    "        print(f\"  Device: {self.device}\")\n",
    "    \n",
    "    def setup(self):\n",
    "        \"\"\"Load and prepare models for benchmarking.\"\"\"\n",
    "        print(\"Setting up benchmark environment...\")\n",
    "        \n",
    "        # Load models and tokenizer\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "        \n",
    "        print(f\"Loading models: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.baseline_model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "        # Create a copy of the model for pruning\n",
    "        print(\"Creating pruning model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n",
    "        \n",
    "        # Prepare evaluation data\n",
    "        self.eval_prompts = [\n",
    "            \"The transformer architecture has revolutionized\",\n",
    "            \"In recent years, artificial intelligence has\",\n",
    "            \"The history of machine learning begins with\",\n",
    "            \"For efficient natural language processing, we need\"\n",
    "        ]\n",
    "        \n",
    "        print(\"Setup complete.\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the complete benchmark pipeline.\"\"\"\n",
    "        print(\"\\nStarting Pruning Benchmark...\")\n",
    "        \n",
    "        # Setup environment\n",
    "        self.setup()\n",
    "        \n",
    "        # Measure baseline performance\n",
    "        print(\"\\nMeasuring baseline performance...\")\n",
    "        self.baseline_metrics = self._evaluate_model(self.baseline_model, \"Baseline Model\")\n",
    "        self.results[\"baseline\"] = self.baseline_metrics\n",
    "        \n",
    "        # Apply pruning\n",
    "        print(f\"\\nApplying {self.pruning_level*100:.1f}% pruning using {self.strategy} strategy...\")\n",
    "        self.pruned_model, pruned_count, pruned_heads = apply_pruning(\n",
    "            self.model, \n",
    "            self.pruning_level, \n",
    "            method=self.strategy, \n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Pruned {pruned_count} attention heads ({len(pruned_heads)} unique heads)\")\n",
    "        \n",
    "        # Evaluate pruned model\n",
    "        print(\"\\nEvaluating pruned model...\")\n",
    "        self.pruned_metrics = self._evaluate_model(self.pruned_model, \"Pruned Model\")\n",
    "        self.results[\"pruned\"] = self.pruned_metrics\n",
    "        \n",
    "        # Compare with baseline\n",
    "        print(\"\\nComparing with baseline model...\")\n",
    "        self.speedup = self.pruned_metrics[\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n",
    "        self.quality_ratio = self.pruned_metrics[\"quality_score\"] / self.baseline_metrics[\"quality_score\"]\n",
    "        \n",
    "        print(f\"Speedup: {self.speedup:.2f}x\")\n",
    "        print(f\"Quality ratio: {self.quality_ratio*100:.1f}%\")\n",
    "        \n",
    "        # Store comparison in results\n",
    "        self.results[\"comparison\"] = {\n",
    "            \"speedup\": self.speedup,\n",
    "            \"quality_ratio\": self.quality_ratio\n",
    "        }\n",
    "        \n",
    "        # Try alternative pruning methods\n",
    "        if self.strategy != \"random\":\n",
    "            # Test random pruning\n",
    "            print(\"\\nTesting random pruning strategy for comparison...\")\n",
    "            random_model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n",
    "            random_pruned_model, _, _ = apply_pruning(random_model, self.pruning_level, method=\"random\")\n",
    "            random_metrics = self._evaluate_model(random_pruned_model, \"Random Pruning\")\n",
    "            self.results[\"random_pruning\"] = random_metrics\n",
    "            \n",
    "            # Compare with main strategy\n",
    "            random_speedup = random_metrics[\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n",
    "            random_quality = random_metrics[\"quality_score\"] / self.baseline_metrics[\"quality_score\"]\n",
    "            \n",
    "            print(f\"Random Pruning:\")\n",
    "            print(f\"  Speedup: {random_speedup:.2f}x\")\n",
    "            print(f\"  Quality: {random_quality*100:.1f}%\")\n",
    "        \n",
    "        # Visualize results if requested\n",
    "        if self.visualize:\n",
    "            print(\"\\nGenerating visualizations...\")\n",
    "            self._create_visualizations()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(self.output_dir, f\"{self.strategy}_pruning_{int(self.pruning_level*100)}_results.json\")\n",
    "        with open(results_file, \"w\") as f:\n",
    "            # Convert non-serializable values\n",
    "            serializable_results = {}\n",
    "            for key, value in self.results.items():\n",
    "                if isinstance(value, dict):\n",
    "                    serializable_results[key] = {k: float(v) if torch.is_tensor(v) else v \n",
    "                                               for k, v in value.items()}\n",
    "                else:\n",
    "                    serializable_results[key] = float(value) if torch.is_tensor(value) else value\n",
    "            \n",
    "            json.dump(serializable_results, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nBenchmark complete. Results saved to: {results_file}\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _evaluate_model(self, model, label=\"Model\"):\n",
    "        \"\"\"Perform comprehensive evaluation of a model.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        print(f\"Evaluating {label}...\")\n",
    "        \n",
    "        # Measure generation speed\n",
    "        generation_metrics = self._measure_generation_speed(model)\n",
    "        metrics.update(generation_metrics)\n",
    "        \n",
    "        # Measure output quality\n",
    "        quality_metrics = self._measure_output_quality(model)\n",
    "        metrics.update(quality_metrics)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"  Generation speed: {metrics['tokens_per_second']:.2f} tokens/sec\")\n",
    "        print(f\"  Quality score: {metrics['quality_score']:.2f}\")\n",
    "        if 'perplexity' in metrics:\n",
    "            print(f\"  Perplexity: {metrics['perplexity']:.2f}\")\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _measure_generation_speed(self, model):\n",
    "        \"\"\"Measure text generation speed in tokens per second.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        num_runs = 3\n",
    "        generation_lengths = [20, 50]\n",
    "        temperature = 0.7\n",
    "        \n",
    "        all_times = []\n",
    "        all_tokens = []\n",
    "        \n",
    "        # Make sure model is in eval mode\n",
    "        with torch.no_grad():\n",
    "            for prompt in self.eval_prompts[:2]:  # Use just 2 prompts for speed\n",
    "                # Tokenize prompt\n",
    "                input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "                \n",
    "                for length in generation_lengths:\n",
    "                    for _ in range(num_runs):\n",
    "                        # Clear CUDA cache\n",
    "                        if self.device == \"cuda\":\n",
    "                            torch.cuda.empty_cache()\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        # Start timing\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        # Generate text\n",
    "                        output_ids = model.generate(\n",
    "                            input_ids=input_ids,\n",
    "                            max_length=input_ids.size(1) + length,\n",
    "                            do_sample=True,\n",
    "                            temperature=temperature,\n",
    "                            pad_token_id=self.tokenizer.eos_token_id\n",
    "                        )\n",
    "                        \n",
    "                        # Ensure all operations are completed\n",
    "                        if self.device == \"cuda\":\n",
    "                            torch.cuda.synchronize()\n",
    "                        \n",
    "                        # End timing\n",
    "                        end_time = time.time()\n",
    "                        generation_time = end_time - start_time\n",
    "                        \n",
    "                        # Calculate tokens generated\n",
    "                        tokens_generated = output_ids.size(1) - input_ids.size(1)\n",
    "                        \n",
    "                        all_times.append(generation_time)\n",
    "                        all_tokens.append(tokens_generated)\n",
    "        \n",
    "        # Calculate average tokens per second\n",
    "        total_tokens = sum(all_tokens)\n",
    "        total_time = sum(all_times)\n",
    "        tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"tokens_per_second\": tokens_per_second,\n",
    "            \"generation_times\": all_times,\n",
    "            \"tokens_generated\": all_tokens\n",
    "        }\n",
    "    \n",
    "    def _measure_output_quality(self, model):\n",
    "        \"\"\"Measure output quality through perplexity and other metrics.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        perplexities = []\n",
    "        quality_scores = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for prompt in self.eval_prompts:\n",
    "                # Calculate perplexity\n",
    "                input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
    "                perplexity = compute_perplexity(model, input_ids)\n",
    "                perplexities.append(perplexity)\n",
    "                \n",
    "                # Generate text and measure quality\n",
    "                output_ids = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=input_ids.size(1) + 50,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "                \n",
    "                output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                quality = compute_output_quality(prompt, output_text)\n",
    "                quality_scores.append(quality)\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_perplexity = sum(perplexities) / len(perplexities)\n",
    "        avg_quality = sum(quality_scores) / len(quality_scores)\n",
    "        \n",
    "        return {\n",
    "            \"perplexity\": avg_perplexity,\n",
    "            \"quality_score\": avg_quality * 100,  # Scale to percentage\n",
    "            \"perplexities\": perplexities,\n",
    "            \"quality_scores\": quality_scores\n",
    "        }\n",
    "    \n",
    "    def _create_visualizations(self):\n",
    "        \"\"\"Create visualizations of benchmark results.\"\"\"\n",
    "        charts_dir = os.path.join(self.output_dir, \"charts\")\n",
    "        os.makedirs(charts_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Speed Comparison Chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Collect speeds for comparison\n",
    "        speeds = {\n",
    "            \"Baseline\": self.baseline_metrics[\"tokens_per_second\"],\n",
    "            f\"{self.strategy.capitalize()} Pruning\": self.pruned_metrics[\"tokens_per_second\"]\n",
    "        }\n",
    "        \n",
    "        # Add random pruning if available\n",
    "        if \"random_pruning\" in self.results:\n",
    "            speeds[\"Random Pruning\"] = self.results[\"random_pruning\"][\"tokens_per_second\"]\n",
    "        \n",
    "        # Create bar chart\n",
    "        bars = plt.bar(range(len(speeds)), list(speeds.values()), color='skyblue')\n",
    "        plt.xticks(range(len(speeds)), list(speeds.keys()), rotation=45)\n",
    "        plt.title(f'Generation Speed Comparison ({self.model_name}, {int(self.pruning_level*100)}% Pruning)')\n",
    "        plt.ylabel('Tokens per Second')\n",
    "        plt.grid(True, linestyle='--', axis='y', alpha=0.7)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                    f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(charts_dir, f\"{self.strategy}_speed_comparison.png\"), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. Quality-Speed Tradeoff Chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Collect data points\n",
    "        labels = [\"Baseline\"]\n",
    "        speedups = [1.0]\n",
    "        qualities = [100.0]\n",
    "        \n",
    "        # Main pruning strategy\n",
    "        labels.append(f\"{self.strategy.capitalize()} Pruning\")\n",
    "        speedups.append(self.results[\"comparison\"][\"speedup\"])\n",
    "        qualities.append(self.results[\"comparison\"][\"quality_ratio\"] * 100)\n",
    "        \n",
    "        # Add random pruning if available\n",
    "        if \"random_pruning\" in self.results:\n",
    "            labels.append(\"Random Pruning\")\n",
    "            random_speedup = self.results[\"random_pruning\"][\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n",
    "            random_quality = self.results[\"random_pruning\"][\"quality_score\"] / self.baseline_metrics[\"quality_score\"] * 100\n",
    "            speedups.append(random_speedup)\n",
    "            qualities.append(random_quality)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        for i, label in enumerate(labels):\n",
    "            plt.scatter(speedups[i], qualities[i], s=100, label=label)\n",
    "            plt.annotate(label, (speedups[i], qualities[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.axhline(y=100, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.axvline(x=1, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title(f'Quality-Speed Tradeoff ({self.model_name}, {int(self.pruning_level*100)}% Pruning)')\n",
    "        plt.xlabel('Speedup Factor (×)')\n",
    "        plt.ylabel('Quality Retention (%)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(charts_dir, f\"{self.strategy}_quality_speed_tradeoff.png\"), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Visualizations saved to: {charts_dir}\")\n",
    "\n",
    "\n",
    "def run_benchmark_ui():\n",
    "    \"\"\"Run the benchmark with an interactive UI in Jupyter or Colab.\"\"\"\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output\n",
    "    \n",
    "    # Create UI widgets\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['gpt2', 'distilgpt2', 'gpt2-medium'],\n",
    "        value='gpt2',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=['entropy', 'random', 'magnitude'],\n",
    "        value='entropy',\n",
    "        description='Pruning Strategy:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    pruning_slider = widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.1,\n",
    "        description='Pruning Level:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    output_dir_text = widgets.Text(\n",
    "        value=output_dir,\n",
    "        description='Output Directory:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='🚀 Run Benchmark',\n",
    "        button_style='success',\n",
    "        tooltip='Click to run the benchmark'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output()\n",
    "    \n",
    "    # Define button click handler\n",
    "    def on_run_button_clicked(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(f\"Starting benchmark with the following settings:\")\n",
    "            print(f\"  Model: {model_dropdown.value}\")\n",
    "            print(f\"  Pruning Strategy: {strategy_dropdown.value}\")\n",
    "            print(f\"  Pruning Level: {pruning_slider.value*100:.1f}%\")\n",
    "            print(f\"  Output Directory: {output_dir_text.value}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Run the benchmark\n",
    "            benchmark = SimplePruningBenchmark(\n",
    "                model_name=model_dropdown.value,\n",
    "                pruning_level=pruning_slider.value,\n",
    "                strategy=strategy_dropdown.value,\n",
    "                output_dir=output_dir_text.value\n",
    "            )\n",
    "            results = benchmark.run()\n",
    "            \n",
    "            # Display summary\n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            print(\"\\n📊 BENCHMARK SUMMARY:\")\n",
    "            print(f\"Speedup: {results['comparison']['speedup']:.2f}x\")\n",
    "            print(f\"Quality retention: {results['comparison']['quality_ratio']*100:.1f}%\")\n",
    "            \n",
    "    # Connect button to handler\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Display UI\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h2>Pruning Benchmark Settings</h2>\"),\n",
    "        model_dropdown,\n",
    "        strategy_dropdown,\n",
    "        pruning_slider,\n",
    "        output_dir_text,\n",
    "        run_button,\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    # Check if we're in a notebook\n",
    "    in_notebook = 'ipykernel' in sys.modules\n",
    "    \n",
    "    if in_notebook:\n",
    "        # Run with UI in notebook\n",
    "        run_benchmark_ui()\n",
    "    else:\n",
    "        # Command-line mode\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description=\"Run pruning benchmark\")\n",
    "        parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", \n",
    "                            help=\"Name of the model to benchmark\")\n",
    "        parser.add_argument(\"--pruning_level\", type=float, default=0.5, \n",
    "                            help=\"Level of pruning to apply (0.0-1.0)\")\n",
    "        parser.add_argument(\"--strategy\", type=str, default=\"entropy\", \n",
    "                            choices=[\"entropy\", \"random\", \"magnitude\"],\n",
    "                            help=\"Pruning strategy to use\")\n",
    "        parser.add_argument(\"--device\", type=str, default=None,\n",
    "                            help=\"Device to run benchmark on (defaults to CUDA if available)\")\n",
    "        parser.add_argument(\"--output_dir\", type=str, default=output_dir,\n",
    "                            help=\"Directory to save benchmark results\")\n",
    "        parser.add_argument(\"--no_visualize\", action=\"store_true\",\n",
    "                            help=\"Skip visualization generation\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        # Run the benchmark\n",
    "        benchmark = SimplePruningBenchmark(\n",
    "            model_name=args.model_name,\n",
    "            pruning_level=args.pruning_level,\n",
    "            strategy=args.strategy,\n",
    "            device=args.device,\n",
    "            output_dir=args.output_dir,\n",
    "            visualize=not args.no_visualize\n",
    "        )\n",
    "        benchmark.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\" or 'ipykernel' in sys.modules:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and run the benchmark with UI\n",
    "import ultra_simple_colab_benchmark\n",
    "ultra_simple_colab_benchmark.run_benchmark_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 View and Analyze Results\n",
    "\n",
    "After running benchmarks, you can use this cell to analyze your results and see the visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Get all result files\n",
    "def load_results(results_dir):\n",
    "    # Use the same directory as in the benchmark\n",
    "    if 'google.colab' in globals() and os.path.exists('/content/drive'):\n",
    "        # Use Google Drive path if available\n",
    "        results_dir = '/content/drive/MyDrive/pruning_benchmark'\n",
    "    else:\n",
    "        # Use local path\n",
    "        results_dir = 'results/pruning_benchmark'\n",
    "    \n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"No results directory found at {results_dir}\")\n",
    "        return\n",
    "    \n",
    "    result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
    "    if not result_files:\n",
    "        print(f\"No result files found in {results_dir}\")\n",
    "        return\n",
    "    \n",
    "    results_data = []\n",
    "    for file in result_files:\n",
    "        try:\n",
    "            with open(os.path.join(results_dir, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract strategy and pruning level from filename\n",
    "            parts = file.split('_')\n",
    "            strategy = parts[0]\n",
    "            pruning_level = int(parts[1])/100 if len(parts) > 1 else 0.5\n",
    "            \n",
    "            # Create summary\n",
    "            if 'comparison' in data:\n",
    "                summary = {\n",
    "                    'Strategy': strategy.capitalize(),\n",
    "                    'Pruning Level': f\"{pruning_level*100:.0f}%\",\n",
    "                    'Speedup': f\"{data['comparison']['speedup']:.2f}×\",\n",
    "                    'Quality': f\"{data['comparison']['quality_ratio']*100:.1f}%\",\n",
    "                    'Speedup_raw': data['comparison']['speedup'],\n",
    "                    'Quality_raw': data['comparison']['quality_ratio']*100,\n",
    "                }\n",
    "                results_data.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    if not results_data:\n",
    "        print(\"No valid result data found\")\n",
    "        return\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Sort by strategy and pruning level\n",
    "    df = df.sort_values(['Strategy', 'Pruning Level'])\n",
    "    \n",
    "    # Display table\n",
    "    display(HTML(\"<h2>Benchmark Results Summary</h2>\"))\n",
    "    display_df = df[['Strategy', 'Pruning Level', 'Speedup', 'Quality']]\n",
    "    display(display_df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    if len(df) > 1:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.scatter(df['Speedup_raw'], df['Quality_raw'], s=100)\n",
    "        \n",
    "        # Add labels to points\n",
    "        for i, row in df.iterrows():\n",
    "            plt.annotate(f\"{row['Strategy']} {row['Pruning Level']}\", \n",
    "                         (row['Speedup_raw'], row['Quality_raw']),\n",
    "                         xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.axhline(y=100, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.axvline(x=1, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title('Quality-Speed Tradeoff Across All Benchmarks')\n",
    "        plt.xlabel('Speedup Factor (×)')\n",
    "        plt.ylabel('Quality Retention (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and display results\n",
    "results = load_results('results/pruning_benchmark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Conclusions and Next Steps\n",
    "\n",
    "Use this cell to document your findings and plan next steps.\n",
    "\n",
    "### Key Findings\n",
    "- What pruning strategies gave the best quality-speed tradeoff?\n",
    "- How much speedup was achievable with acceptable quality loss?\n",
    "- How does pruning level affect performance across different strategies?\n",
    "\n",
    "### Next Steps\n",
    "- Try additional pruning levels or strategies\n",
    "- Test on different model sizes\n",
    "- Compare with agency-based methods\n",
    "- Implement more sophisticated entropy calculations\n",
    "\n",
    "### Questions to Explore\n",
    "- Does entropy-based pruning consistently outperform random pruning?\n",
    "- Is there a \"sweet spot\" pruning level that maximizes speedup while minimizing quality degradation?\n",
    "- How do the results change across model sizes (distil vs. standard vs. medium)?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}