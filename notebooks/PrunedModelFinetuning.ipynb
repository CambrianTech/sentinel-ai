{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Fine-tuning Pruned Models Demo\n",
        "\n",
        "This notebook demonstrates how to fine-tune pruned models to recover accuracy while maintaining the speed benefits of pruning.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. Setup environment and load models\n",
        "2. Prune a model with different strategies\n",
        "3. Benchmark the pruned model (speed and accuracy)\n",
        "4. Fine-tune the pruned model with per-head learning rates\n",
        "5. Benchmark the fine-tuned model to show accuracy recovery\n",
        "6. Visualize the improvements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's clone the repository and install dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone-repo"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/CambrianTech/sentinel-ai.git\n",
        "%cd sentinel-ai\n",
        "!pip install -r requirements.txt\n",
        "!pip install torch matplotlib pandas seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add the project to path\n",
        "sys.path.append('.')\n",
        "\n",
        "from models.loaders.loader import load_baseline_model, load_adaptive_model\n",
        "from datasets.dataset_loader import load_and_tokenize_dataset\n",
        "from utils.train_utils import compute_loss, compute_perplexity\n",
        "from utils.checkpoint import save_checkpoint, load_checkpoint\n",
        "from utils.head_lr_manager import HeadLRManager\n",
        "from utils.head_metrics import compute_attention_entropy, compute_head_importance\n",
        "from utils.generation_wrapper import generate_text\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load-model"
      },
      "source": [
        "## Load Model\n",
        "\n",
        "Let's load a small pre-trained model (distilgpt2) and adapt it with our architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-loading"
      },
      "outputs": [],
      "source": [
        "model_name = \"distilgpt2\"\n",
        "print(f\"Loading baseline model: {model_name}\")\n",
        "baseline_model = load_baseline_model(model_name, device)\n",
        "\n",
        "print(\"Creating adaptive model...\")\n",
        "model = load_adaptive_model(model_name, baseline_model, device)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Count initial parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prepare-data"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "Load a small text dataset for training and evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-loading"
      },
      "outputs": [],
      "source": [
        "# Load tiny_shakespeare dataset\n",
        "dataset_name = \"tiny_shakespeare\"\n",
        "max_length = 128  # Sequence length\n",
        "\n",
        "print(f\"Loading dataset: {dataset_name}\")\n",
        "train_ids, val_ids = load_and_tokenize_dataset(model_name, dataset_name, max_length)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8  # Increased batch size for faster training\n",
        "train_dataset = TensorDataset(torch.tensor(train_ids))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(torch.tensor(val_ids))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "original-model-evaluation"
      },
      "source": [
        "## Evaluate Original Model\n",
        "\n",
        "Let's establish a baseline by evaluating the original model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval-original"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_metrics(model, val_loader, device):\n",
        "    \"\"\"Evaluate a model's perplexity and generate sample text.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n",
        "            input_ids = batch[0].to(device)\n",
        "            targets = input_ids.clone()\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(input_ids)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = compute_loss(outputs, targets)\n",
        "            \n",
        "            # Update totals\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_tokens += input_ids.size(0) * input_ids.size(1)\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    \n",
        "    # Generate sample text\n",
        "    prompt = \"The meaning of life is\"\n",
        "    start_time = time.time()\n",
        "    output = generate_text(\n",
        "        model, tokenizer, prompt,\n",
        "        max_length=100,\n",
        "        temperature=0.8,\n",
        "        device=device\n",
        "    )\n",
        "    generation_time = time.time() - start_time\n",
        "    tokens_generated = len(tokenizer.encode(output)) - len(tokenizer.encode(prompt))\n",
        "    tokens_per_sec = tokens_generated / generation_time\n",
        "    \n",
        "    # Return metrics and sample\n",
        "    return {\n",
        "        \"perplexity\": perplexity,\n",
        "        \"tokens_per_sec\": tokens_per_sec,\n",
        "        \"generation_time\": generation_time,\n",
        "        \"generated_text\": output\n",
        "    }\n",
        "\n",
        "# Evaluate original model\n",
        "print(\"Evaluating original model...\")\n",
        "original_metrics = evaluate_model_metrics(model, val_loader, device)\n",
        "print(f\"\\nOriginal model perplexity: {original_metrics['perplexity']:.2f}\")\n",
        "print(f\"Original model speed: {original_metrics['tokens_per_sec']:.2f} tokens/sec\")\n",
        "print(f\"\\nSample text from original model:\\n{original_metrics['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "head-metrics"
      },
      "source": [
        "## Compute Head Metrics\n",
        "\n",
        "To decide which heads to prune, we need to compute importance metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compute-metrics"
      },
      "outputs": [],
      "source": [
        "# Compute entropy-based metrics for heads\n",
        "print(\"Computing entropy metrics...\")\n",
        "entropy_dict = compute_attention_entropy(model, device=device)\n",
        "\n",
        "# Compute gradient-based importance\n",
        "print(\"Computing importance metrics...\")\n",
        "importance_dict = compute_head_importance(model, val_loader, compute_loss, device=device)\n",
        "\n",
        "# Visualize head metrics\n",
        "def plot_head_metrics(entropy_dict, importance_dict):\n",
        "    num_layers = len(entropy_dict)\n",
        "    num_heads = len(entropy_dict[0])\n",
        "    \n",
        "    # Create figure with two subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    # Prepare data for heatmaps\n",
        "    entropy_data = np.zeros((num_layers, num_heads))\n",
        "    importance_data = np.zeros((num_layers, num_heads))\n",
        "    \n",
        "    for layer_idx in range(num_layers):\n",
        "        for head_idx in range(num_heads):\n",
        "            entropy_data[layer_idx, head_idx] = entropy_dict[layer_idx][head_idx].item()\n",
        "            importance_data[layer_idx, head_idx] = importance_dict[layer_idx][head_idx].item()\n",
        "    \n",
        "    # Plot entropy heatmap\n",
        "    sns.heatmap(entropy_data, ax=ax1, cmap=\"viridis\", annot=True, fmt=\".2f\")\n",
        "    ax1.set_title(\"Attention Entropy by Head\")\n",
        "    ax1.set_xlabel(\"Head Index\")\n",
        "    ax1.set_ylabel(\"Layer Index\")\n",
        "    \n",
        "    # Plot importance heatmap\n",
        "    sns.heatmap(importance_data, ax=ax2, cmap=\"plasma\", annot=True, fmt=\".2f\")\n",
        "    ax2.set_title(\"Head Importance\")\n",
        "    ax2.set_xlabel(\"Head Index\")\n",
        "    ax2.set_ylabel(\"Layer Index\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return entropy_data, importance_data\n",
        "\n",
        "entropy_data, importance_data = plot_head_metrics(entropy_dict, importance_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruning"
      },
      "source": [
        "## Prune the Model\n",
        "\n",
        "Now let's prune the model using entropy-based metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prune-model"
      },
      "outputs": [],
      "source": [
        "def prune_model_by_metrics(model, prune_ratio=0.5, strategy=\"entropy\"):\n",
        "    \"\"\"Prune a percentage of attention heads based on metrics.\"\"\"\n",
        "    num_layers = len(model.blocks)\n",
        "    num_heads = model.blocks[0][\"attn\"].num_heads\n",
        "    total_heads = num_layers * num_heads\n",
        "    heads_to_prune = int(total_heads * prune_ratio)\n",
        "    \n",
        "    print(f\"Pruning {heads_to_prune} of {total_heads} heads ({prune_ratio*100:.1f}%)\")\n",
        "    \n",
        "    # Flatten metrics for all heads\n",
        "    all_heads = []\n",
        "    for layer_idx in range(num_layers):\n",
        "        for head_idx in range(num_heads):\n",
        "            # Choose metric based on strategy\n",
        "            if strategy == \"entropy\":\n",
        "                # Lower entropy is less informative -> prune\n",
        "                metric = entropy_dict[layer_idx][head_idx].item()\n",
        "            elif strategy == \"importance\":\n",
        "                # Lower importance -> prune\n",
        "                metric = importance_dict[layer_idx][head_idx].item()\n",
        "            elif strategy == \"random\":\n",
        "                metric = np.random.random()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown pruning strategy: {strategy}\")\n",
        "            \n",
        "            all_heads.append((layer_idx, head_idx, metric))\n",
        "    \n",
        "    # Sort heads by metric (lower values first for pruning)\n",
        "    if strategy == \"entropy\":\n",
        "        # For entropy, we prune heads with lowest entropy (least informative)\n",
        "        sorted_heads = sorted(all_heads, key=lambda x: x[2])\n",
        "    elif strategy == \"importance\":\n",
        "        # For importance, we prune heads with lowest importance\n",
        "        sorted_heads = sorted(all_heads, key=lambda x: x[2])\n",
        "    else:\n",
        "        # For random, we just use the random values\n",
        "        sorted_heads = sorted(all_heads, key=lambda x: x[2])\n",
        "    \n",
        "    # Heads to prune\n",
        "    heads_to_prune = sorted_heads[:heads_to_prune]\n",
        "    \n",
        "    # Actually prune the heads\n",
        "    with torch.no_grad():\n",
        "        for layer_idx, head_idx, _ in heads_to_prune:\n",
        "            # Set gate value to 0 (pruned)\n",
        "            model.blocks[layer_idx][\"attn\"].gate[head_idx] = 0.0\n",
        "            print(f\"Pruned layer {layer_idx}, head {head_idx}\")\n",
        "    \n",
        "    # Count active heads after pruning\n",
        "    active_heads = 0\n",
        "    for layer_idx in range(num_layers):\n",
        "        for head_idx in range(num_heads):\n",
        "            if model.blocks[layer_idx][\"attn\"].gate[head_idx] > 0.01:\n",
        "                active_heads += 1\n",
        "    \n",
        "    print(f\"Active heads after pruning: {active_heads}/{total_heads} ({active_heads/total_heads*100:.1f}%)\")\n",
        "    return model\n",
        "\n",
        "# Define pruning parameters\n",
        "pruning_ratio = 0.5  # Prune 50% of heads\n",
        "pruning_strategy = \"entropy\"  # Options: \"entropy\", \"importance\", \"random\"\n",
        "\n",
        "# Create a copy of the model for pruning\n",
        "pruned_model = load_adaptive_model(model_name, baseline_model, device)\n",
        "pruned_model.load_state_dict(model.state_dict())\n",
        "\n",
        "# Apply pruning\n",
        "pruned_model = prune_model_by_metrics(pruned_model, pruning_ratio, pruning_strategy)\n",
        "\n",
        "# Save the pruned model\n",
        "checkpoint_dir = \"./checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "pruned_checkpoint_path = os.path.join(checkpoint_dir, \"pruned_model.pth\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(pruned_model.parameters())\n",
        "save_checkpoint(pruned_checkpoint_path, pruned_model, optimizer, {}, 0, 0)\n",
        "print(f\"Saved pruned model to {pruned_checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate-pruned"
      },
      "source": [
        "## Evaluate Pruned Model\n",
        "\n",
        "Let's see how pruning affects performance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval-pruned"
      },
      "outputs": [],
      "source": [
        "# Evaluate pruned model\n",
        "print(\"Evaluating pruned model...\")\n",
        "pruned_metrics = evaluate_model_metrics(pruned_model, val_loader, device)\n",
        "\n",
        "print(f\"\\nPruned model perplexity: {pruned_metrics['perplexity']:.2f} (Original: {original_metrics['perplexity']:.2f})\")\n",
        "print(f\"Pruned model speed: {pruned_metrics['tokens_per_sec']:.2f} tokens/sec (Original: {original_metrics['tokens_per_sec']:.2f})\")\n",
        "print(f\"Speed improvement: {pruned_metrics['tokens_per_sec'] / original_metrics['tokens_per_sec']:.2f}x\")\n",
        "\n",
        "print(f\"\\nSample text from pruned model:\\n{pruned_metrics['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fine-tuning"
      },
      "source": [
        "## Fine-tune the Pruned Model\n",
        "\n",
        "Now, let's fine-tune the pruned model to recover accuracy. We'll use an aggressive training approach to show improvement quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "finetune-model"
      },
      "outputs": [],
      "source": [
        "# Import the fine-tuning function\n",
        "from scripts.finetune_pruned_model import create_optimizer_with_head_params\n",
        "\n",
        "# Fine-tuning parameters - more aggressive for faster convergence\n",
        "epochs = 4  # More epochs to ensure meaningful recovery\n",
        "learning_rate = 1e-4  # Higher learning rate for faster convergence \n",
        "boost_factor = 10.0  # Much higher boost for active heads\n",
        "warmup_steps = 50  # Shorter warmup for faster initial progress\n",
        "cooldown_steps = 300  # Shorter cooldown cycle for more high-LR training\n",
        "eval_interval = 50  # Evaluate more frequently to track progress\n",
        "\n",
        "# Create optimizer with per-head parameters\n",
        "optimizer = create_optimizer_with_head_params(pruned_model, learning_rate)\n",
        "\n",
        "# Create HeadLRManager\n",
        "head_lr_manager = HeadLRManager(\n",
        "    model=pruned_model,\n",
        "    optimizer=optimizer,\n",
        "    base_lr=learning_rate,\n",
        "    boost_factor=boost_factor,\n",
        "    decay_factor=0.9,\n",
        "    warmup_steps=warmup_steps,\n",
        "    cooldown_steps=cooldown_steps\n",
        ")\n",
        "\n",
        "# Create dummy gate status to initialize head status\n",
        "with torch.no_grad():\n",
        "    dummy_gates = torch.zeros((len(pruned_model.blocks), pruned_model.blocks[0][\"attn\"].num_heads))\n",
        "    \n",
        "    # Set gates based on actual model gates\n",
        "    for layer_idx in range(len(pruned_model.blocks)):\n",
        "        for head_idx in range(pruned_model.blocks[0][\"attn\"].num_heads):\n",
        "            gate_value = pruned_model.blocks[layer_idx][\"attn\"].gate[head_idx].item()\n",
        "            dummy_gates[layer_idx, head_idx] = 1.0 if gate_value > 0.01 else 0.0\n",
        "\n",
        "# Initialize head status with current gates\n",
        "head_lr_manager.update_head_status(dummy_gates)\n",
        "\n",
        "# Create learning rate scheduler - use OneCycleLR for faster convergence\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "steps_per_epoch = len(train_loader)\n",
        "lr_scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate * 3,  # Peak learning rate 3x the base rate\n",
        "    total_steps=epochs * steps_per_epoch,\n",
        "    pct_start=0.3,  # Spend 30% of steps in warmup\n",
        "    div_factor=25.0,  # Initial LR is max_lr/25\n",
        "    final_div_factor=10000.0  # Final LR is max_lr/10000\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "pruned_model.train()\n",
        "step = 0\n",
        "history = {\n",
        "    \"loss\": [],\n",
        "    \"perplexity\": [],\n",
        "    \"lr\": [],\n",
        "    \"eval_steps\": []\n",
        "}\n",
        "\n",
        "print(f\"Fine-tuning pruned model for {epochs} epochs...\")\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # Training\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        step += 1\n",
        "        \n",
        "        # Get batch\n",
        "        input_ids = batch[0].to(device)\n",
        "        targets = input_ids.clone()\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = pruned_model(input_ids)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = compute_loss(outputs, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update head learning rates every 5 steps\n",
        "        if step % 5 == 0:\n",
        "            lr_info = head_lr_manager.update_learning_rates()\n",
        "        \n",
        "        # Update learning rate scheduler\n",
        "        lr_scheduler.step()\n",
        "        \n",
        "        # Update progress\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({\n",
        "            \"loss\": loss.item(),\n",
        "            \"avg_loss\": epoch_loss / (batch_idx + 1),\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "        })\n",
        "        \n",
        "        # Save metrics\n",
        "        history[\"loss\"].append(loss.item())\n",
        "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "        \n",
        "        # Evaluate periodically\n",
        "        if step % eval_interval == 0 or batch_idx == len(train_loader) - 1:\n",
        "            pruned_model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_tokens = 0\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                # Only evaluate on a subset of validation data to save time\n",
        "                eval_subset = list(val_loader)[:10]  # Just use first 10 batches\n",
        "                for val_batch in tqdm(eval_subset, desc=\"Evaluating\", leave=False):\n",
        "                    val_input_ids = val_batch[0].to(device)\n",
        "                    val_targets = val_input_ids.clone()\n",
        "                    \n",
        "                    # Forward pass\n",
        "                    val_outputs = pruned_model(val_input_ids)\n",
        "                    \n",
        "                    # Compute loss\n",
        "                    val_loss_batch = compute_loss(val_outputs, val_targets)\n",
        "                    \n",
        "                    # Update totals\n",
        "                    val_loss += val_loss_batch.item() * val_input_ids.size(0)\n",
        "                    val_tokens += val_input_ids.size(0) * val_input_ids.size(1)\n",
        "            \n",
        "            # Calculate perplexity\n",
        "            val_avg_loss = val_loss / val_tokens\n",
        "            val_perplexity = torch.exp(torch.tensor(val_avg_loss)).item()\n",
        "            \n",
        "            print(f\"\\nStep {step}, Validation perplexity: {val_perplexity:.2f}\")\n",
        "            history[\"perplexity\"].append(val_perplexity)\n",
        "            history[\"eval_steps\"].append(step)\n",
        "            \n",
        "            # Return to training mode\n",
        "            pruned_model.train()\n",
        "            \n",
        "            # Early stopping check - if perplexity is at or better than original model, we can stop\n",
        "            if val_perplexity <= original_metrics[\"perplexity\"] * 1.05:  # Within 5% of original\n",
        "                print(f\"Perplexity recovered to within 5% of original model! Early stopping.\")\n",
        "                break\n",
        "    \n",
        "    # End of epoch\n",
        "    print(f\"Epoch {epoch+1}/{epochs} completed. Avg loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "    \n",
        "    # Early stopping check at epoch level\n",
        "    if len(history[\"perplexity\"]) > 0 and history[\"perplexity\"][-1] <= original_metrics[\"perplexity\"] * 1.05:\n",
        "        print(\"Early stopping after sufficient accuracy recovery.\")\n",
        "        break\n",
        "\n",
        "# Save fine-tuned model\n",
        "finetuned_checkpoint_path = os.path.join(checkpoint_dir, \"finetuned_model.pth\")\n",
        "save_checkpoint(\n",
        "    finetuned_checkpoint_path,\n",
        "    pruned_model,\n",
        "    optimizer,\n",
        "    head_lr_manager.save_state_dict(),\n",
        "    epochs,\n",
        "    step\n",
        ")\n",
        "print(f\"Saved fine-tuned model to {finetuned_checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "learning-curve"
      },
      "source": [
        "## Visualize Learning Progress\n",
        "\n",
        "Let's plot the learning curve to see how fine-tuning improves perplexity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-learning"
      },
      "outputs": [],
      "source": [
        "# Plot learning curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot perplexity\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history[\"eval_steps\"], history[\"perplexity\"], marker='o')\n",
        "plt.axhline(y=original_metrics[\"perplexity\"], color='r', linestyle='--', label=\"Original Model\")\n",
        "plt.axhline(y=pruned_metrics[\"perplexity\"], color='g', linestyle='--', label=\"Pruned Model\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Perplexity (lower is better)\")\n",
        "plt.title(\"Perplexity During Fine-tuning\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history[\"loss\"], alpha=0.3, label=\"Raw\")\n",
        "window_size = 20\n",
        "smoothed_loss = [sum(history[\"loss\"][max(0, i-window_size):i]) / min(i, window_size) for i in range(1, len(history[\"loss\"])+1)]\n",
        "plt.plot(smoothed_loss, label=\"Smoothed\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate-finetuned"
      },
      "source": [
        "## Evaluate Fine-tuned Model\n",
        "\n",
        "Now let's see how much accuracy we've recovered:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval-finetuned"
      },
      "outputs": [],
      "source": [
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_metrics = evaluate_model_metrics(pruned_model, val_loader, device)\n",
        "\n",
        "print(f\"\\nFine-tuned model perplexity: {finetuned_metrics['perplexity']:.2f}\")\n",
        "print(f\"Fine-tuned model speed: {finetuned_metrics['tokens_per_sec']:.2f} tokens/sec\")\n",
        "\n",
        "print(f\"\\nSample text from fine-tuned model:\\n{finetuned_metrics['generated_text']}\")\n",
        "\n",
        "# Compare all models\n",
        "models = [\"Original\", \"Pruned\", \"Fine-tuned\"]\n",
        "perplexities = [\n",
        "    original_metrics[\"perplexity\"],\n",
        "    pruned_metrics[\"perplexity\"],\n",
        "    finetuned_metrics[\"perplexity\"]\n",
        "]\n",
        "speeds = [\n",
        "    original_metrics[\"tokens_per_sec\"],\n",
        "    pruned_metrics[\"tokens_per_sec\"],\n",
        "    finetuned_metrics[\"tokens_per_sec\"]\n",
        "]\n",
        "\n",
        "# Calculate relative improvements\n",
        "perplexity_change_pruned = (pruned_metrics[\"perplexity\"] - original_metrics[\"perplexity\"]) / original_metrics[\"perplexity\"] * 100\n",
        "perplexity_change_finetuned = (finetuned_metrics[\"perplexity\"] - original_metrics[\"perplexity\"]) / original_metrics[\"perplexity\"] * 100\n",
        "perplexity_recovery = (pruned_metrics[\"perplexity\"] - finetuned_metrics[\"perplexity\"]) / (pruned_metrics[\"perplexity\"] - original_metrics[\"perplexity\"]) * 100 if pruned_metrics[\"perplexity\"] != original_metrics[\"perplexity\"] else 0\n",
        "\n",
        "speed_change_pruned = (pruned_metrics[\"tokens_per_sec\"] - original_metrics[\"tokens_per_sec\"]) / original_metrics[\"tokens_per_sec\"] * 100\n",
        "speed_change_finetuned = (finetuned_metrics[\"tokens_per_sec\"] - original_metrics[\"tokens_per_sec\"]) / original_metrics[\"tokens_per_sec\"] * 100\n",
        "\n",
        "print(\"\\nModel Comparison:\")\n",
        "print(f\"Pruned model perplexity change: {perplexity_change_pruned:.1f}% (higher is worse)\")\n",
        "print(f\"Fine-tuned model perplexity change: {perplexity_change_finetuned:.1f}% (higher is worse)\")\n",
        "print(f\"Accuracy recovery: {perplexity_recovery:.1f}% of the gap closed\")\n",
        "print(f\"\\nPruned model speed change: +{speed_change_pruned:.1f}%\")\n",
        "print(f\"Fine-tuned model speed change: +{speed_change_finetuned:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization"
      },
      "source": [
        "## Results Visualization\n",
        "\n",
        "Let's create some visualizations to summarize our results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-results"
      },
      "outputs": [],
      "source": [
        "# Create a comparison plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# Perplexity comparison\n",
        "plt.subplot(1, 2, 1)\n",
        "colors = ['blue', 'red', 'green']\n",
        "plt.bar(models, perplexities, color=colors)\n",
        "plt.ylabel(\"Perplexity (lower is better)\")\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "for i, perplexity in enumerate(perplexities):\n",
        "    plt.text(i, perplexity + 1, f\"{perplexity:.2f}\", ha='center')\n",
        "\n",
        "# Speed comparison\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(models, speeds, color=colors)\n",
        "plt.ylabel(\"Generation Speed (tokens/sec)\")\n",
        "plt.title(\"Model Speed Comparison\")\n",
        "for i, speed in enumerate(speeds):\n",
        "    plt.text(i, speed + 0.5, f\"{speed:.2f}\", ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Create a head usage visualization\n",
        "def plot_head_usage(model):\n",
        "    num_layers = len(model.blocks)\n",
        "    num_heads = model.blocks[0][\"attn\"].num_heads\n",
        "    \n",
        "    # Get gate values\n",
        "    gate_values = np.zeros((num_layers, num_heads))\n",
        "    for layer_idx in range(num_layers):\n",
        "        for head_idx in range(num_heads):\n",
        "            gate_values[layer_idx, head_idx] = model.blocks[layer_idx][\"attn\"].gate[head_idx].item()\n",
        "    \n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(gate_values, cmap=\"RdYlGn\", annot=True, fmt=\".2f\")\n",
        "    plt.title(\"Attention Head Gate Values (0 = pruned)\")\n",
        "    plt.xlabel(\"Head Index\")\n",
        "    plt.ylabel(\"Layer Index\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Count active heads\n",
        "    active_heads = np.sum(gate_values > 0.01)\n",
        "    total_heads = num_layers * num_heads\n",
        "    print(f\"Active heads: {active_heads}/{total_heads} ({active_heads/total_heads*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nFine-tuned model head usage:\")\n",
        "plot_head_usage(pruned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "side-by-side"
      },
      "source": [
        "## Side-by-Side Text Generation Comparison\n",
        "\n",
        "Let's compare text generated by all three models with the same prompts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "compare-generation"
      },
      "outputs": [],
      "source": [
        "# Compare generation results\n",
        "prompts = [\n",
        "    \"The meaning of life is\",\n",
        "    \"In a world where technology\",\n",
        "    \"Once upon a time in a distant kingdom\"\n",
        "]\n",
        "\n",
        "# Reload original model\n",
        "original_model = load_adaptive_model(model_name, baseline_model, device)\n",
        "\n",
        "# Save pruned outputs separately for later comparison\n",
        "pruned_outputs = []\n",
        "for i, prompt in enumerate(prompts):\n",
        "    pruned_output = generate_text(pruned_model, tokenizer, prompt, max_length=100, temperature=0.8, device=device)\n",
        "    pruned_outputs.append(pruned_output)\n",
        "\n",
        "# Now compare side by side\n",
        "for i, prompt in enumerate(prompts):\n",
        "    print(f\"\\nPrompt {i+1}: '{prompt}'\\n\")\n",
        "    \n",
        "    # Generate text with original model\n",
        "    original_text = generate_text(original_model, tokenizer, prompt, max_length=100, temperature=0.8, device=device)\n",
        "    \n",
        "    # Retrieve previously generated pruned text\n",
        "    pruned_text = pruned_outputs[i]\n",
        "    \n",
        "    # Generate text with fine-tuned model\n",
        "    finetuned_text = generate_text(pruned_model, tokenizer, prompt, max_length=100, temperature=0.8, device=device)\n",
        "    \n",
        "    # Print side by side\n",
        "    print(\"Original model:\")\n",
        "    print(original_text)\n",
        "    print(\"\\nPruned model (before fine-tuning):\")\n",
        "    print(pruned_text)\n",
        "    print(\"\\nFine-tuned model:\")\n",
        "    print(finetuned_text)\n",
        "    print(\"\\n\" + \"-\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to fine-tune pruned transformer models to recover accuracy while maintaining the speed benefits. We showed that:\n",
        "\n",
        "1. Pruning models can significantly improve inference speed (typically 1.5-2x faster)\n",
        "2. Pruning initially degrades accuracy (higher perplexity)\n",
        "3. With specialized fine-tuning using per-head learning rates, we can recover much of the lost accuracy\n",
        "4. The fine-tuned model retains the speed benefits while producing higher quality outputs\n",
        "\n",
        "This approach is particularly important for deployment to resource-constrained environments where both speed and quality matter.\n",
        "\n",
        "Key optimizations in this implementation:\n",
        "1. Higher boost factor (10.0) for active heads to accelerate learning\n",
        "2. OneCycleLR scheduler for faster convergence\n",
        "3. Early stopping when quality is sufficiently recovered\n",
        "4. Aggressive parameter tuning for fast demonstration\n",
        "\n",
        "In production settings, you might want to use more epochs and a less aggressive learning rate schedule for maximum quality recovery, especially for higher pruning ratios."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}