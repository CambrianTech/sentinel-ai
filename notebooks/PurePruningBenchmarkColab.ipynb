{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Pure Pruning Benchmark - Google Colab Notebook\n",
    "\n",
    "This notebook provides a comprehensive UI for running overnight pruning benchmarks in Google Colab. It isolates the effects of pruning from agency features, allowing you to understand the specific impact of different pruning strategies and levels.\n",
    "\n",
    "## Features:\n",
    "- Interactive UI for configuring benchmarks\n",
    "- Support for multiple pruning strategies (entropy, random, magnitude)\n",
    "- Comprehensive metrics collection (speed, memory, quality)\n",
    "- Automatic visualization generation\n",
    "- Progress tracking and periodic result saving\n",
    "- Google Drive integration for result persistence\n",
    "\n",
    "## Instructions:\n",
    "1. Run the setup cell to clone the repository and install dependencies\n",
    "2. Configure your benchmark using the UI widgets\n",
    "3. Start the benchmark and monitor progress in real-time\n",
    "4. Results are automatically saved to Google Drive (if mounted)\n",
    "\n",
    "***Note:** This notebook is designed to run overnight on Google Colab runtime.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup Environment\n",
    "\n",
    "First, we will clone the repository and set up the environment. If you are running this notebook from GitHub, the branch will be automatically detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab environment\")\n",
    "    \n",
    "    # Clone the repository with specific branch\n",
    "    \!git clone --single-branch --branch feature/colab-overnight https://github.com/CambrianTech/sentinel-ai.git\n",
    "    %cd sentinel-ai\n",
    "    \n",
    "    # Install required packages\n",
    "    \!pip install -r requirements.txt\n",
    "    \n",
    "    # Install additional packages for visualization and UI\n",
    "    \!pip install ipywidgets plotly thop\n",
    "    \n",
    "    # Add repository to Python path\n",
    "    import sys\n",
    "    sys.path.append(\"/content/sentinel-ai\")\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "    # For local development, ensure the repository is in the path\n",
    "    import os\n",
    "    if not os.path.exists(\"scripts/pure_pruning_benchmark.py\"):\n",
    "        # Navigate to root directory if needed\n",
    "        %cd ..\n",
    "        if not os.path.exists(\"scripts/pure_pruning_benchmark.py\"):\n",
    "            raise ValueError(\"Please run this notebook from the repository root or notebooks directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Mount Google Drive (Optional)\n",
    "\n",
    "Mounting Google Drive will allow us to save benchmark results permanently, even if the Colab runtime disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for result storage\n",
    "MOUNT_DRIVE = True  # Set to False to skip mounting\n",
    "\n",
    "if IN_COLAB and MOUNT_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    \n",
    "    # Create directory for results\n",
    "    DRIVE_RESULTS_DIR = \"/content/drive/MyDrive/sentinel_ai_benchmarks\"\n",
    "    \!mkdir -p {DRIVE_RESULTS_DIR}\n",
    "    print(f\"Results will be saved to: {DRIVE_RESULTS_DIR}\")\n",
    "else:\n",
    "    DRIVE_RESULTS_DIR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Import Libraries\n",
    "\n",
    "Now we will import all the necessary libraries for the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# Import benchmark class\n",
    "from scripts.pure_pruning_benchmark import PruningBenchmark, parse_args\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    # Display GPU info\n",
    "    \!nvidia-smi\n",
    "else:\n",
    "    print(\"WARNING: Running on CPU. Benchmarks will be much slower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Configure Benchmark Settings\n",
    "\n",
    "Use the interactive UI below to configure your benchmark settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure interactive widgets\n",
    "model_options = [\"gpt2\", \"distilgpt2\", \"gpt2-medium\"] \n",
    "strategy_options = [\"entropy\", \"random\", \"magnitude\"]\n",
    "pruning_levels = [0.3, 0.5, 0.7]  # 30%, 50%, 70%\n",
    "\n",
    "# Main configuration widgets\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    value=\"gpt2\",\n",
    "    description=\"Model:\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "strategy_dropdown = widgets.Dropdown(\n",
    "    options=strategy_options,\n",
    "    value=\"entropy\",\n",
    "    description=\"Pruning Strategy:\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "pruning_slider = widgets.SelectionSlider(\n",
    "    options=[(f\"{int(x*100)}%\", x) for x in pruning_levels],\n",
    "    value=0.5,\n",
    "    description=\"Pruning Level:\",\n",
    "    continuous_update=False,\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "# Additional configuration\n",
    "comparison_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Compare with baseline\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "hardware_metrics_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Measure hardware metrics\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "visualize_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Generate visualizations\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "# Output directory configuration\n",
    "if IN_COLAB and DRIVE_RESULTS_DIR:\n",
    "    default_output_dir = os.path.join(DRIVE_RESULTS_DIR, \"pruning_benchmark_results\")\n",
    "else:\n",
    "    default_output_dir = \"results/pure_pruning\"\n",
    "\n",
    "output_dir_text = widgets.Text(\n",
    "    value=default_output_dir,\n",
    "    description=\"Output directory:\",\n",
    "    style={\"description_width\": \"initial\"}\n",
    ")\n",
    "\n",
    "# Run button\n",
    "run_button = widgets.Button(\n",
    "    description=\"üöÄ Run Benchmark\",\n",
    "    button_style=\"success\", \n",
    "    icon=\"play\"\n",
    ")\n",
    "\n",
    "# Layout the widgets\n",
    "main_config = widgets.VBox([\n",
    "    widgets.HTML(value=\"<h3>Model Configuration</h3>\"),\n",
    "    model_dropdown,\n",
    "    strategy_dropdown,\n",
    "    pruning_slider\n",
    "])\n",
    "\n",
    "additional_config = widgets.VBox([\n",
    "    widgets.HTML(value=\"<h3>Additional Settings</h3>\"),\n",
    "    comparison_checkbox,\n",
    "    hardware_metrics_checkbox,\n",
    "    visualize_checkbox,\n",
    "    output_dir_text\n",
    "])\n",
    "\n",
    "# Progress output area\n",
    "output_area = widgets.Output(layout={\"border\": \"1px solid black\", \"height\": \"300px\", \"overflow_y\": \"scroll\"})\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.HBox([main_config, additional_config]))\n",
    "display(run_button)\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Benchmark Execution Code\n",
    "\n",
    "This cell contains the logic for running the benchmark when you click the Run button above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run when button is clicked\n",
    "def run_benchmark(b):\n",
    "    # Disable the button to prevent multiple runs\n",
    "    run_button.disabled = True\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(f\"Starting benchmark at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "        print(f\"Model: {model_dropdown.value}\")\n",
    "        print(f\"Pruning strategy: {strategy_dropdown.value}\")\n",
    "        print(f\"Pruning level: {pruning_slider.value * 100}%\")\n",
    "        print(f\"Output directory: {output_dir_text.value}\")\n",
    "        print(\"\n\" + \"-\"*50 + \"\n\")\n",
    "        \n",
    "        try:\n",
    "            # Create output directory\n",
    "            os.makedirs(output_dir_text.value, exist_ok=True)\n",
    "            \n",
    "            # Initialize benchmark\n",
    "            benchmark = PruningBenchmark(\n",
    "                model_name=model_dropdown.value,\n",
    "                pruning_level=pruning_slider.value,\n",
    "                strategy=strategy_dropdown.value,\n",
    "                device=device,\n",
    "                output_dir=output_dir_text.value,\n",
    "                visualize=visualize_checkbox.value,\n",
    "                baseline_comparison=comparison_checkbox.value,\n",
    "                hardware_metrics=hardware_metrics_checkbox.value\n",
    "            )\n",
    "            \n",
    "            # Run the benchmark\n",
    "            results = benchmark.run()\n",
    "            \n",
    "            # Generate results summary\n",
    "            print(\"\n\" + \"-\"*50)\n",
    "            print(\"\nüìä BENCHMARK SUMMARY:\n\")\n",
    "            \n",
    "            if comparison_checkbox.value:\n",
    "                print(f\"Speedup: {results[\"comparison\"][\"speedup\"]:.2f}x\")\n",
    "                print(f\"Memory reduction: {results[\"comparison\"][\"memory_reduction\"]*100:.1f}%\")\n",
    "                print(f\"Quality retention: {results[\"comparison\"][\"quality_ratio\"]*100:.1f}%\")\n",
    "            \n",
    "            # Display a link to the results directory if on Google Drive\n",
    "            if IN_COLAB and DRIVE_RESULTS_DIR and output_dir_text.value.startswith(\"/content/drive\"):\n",
    "                print(\"\nResults saved to Google Drive. They will persist even if the runtime disconnects.\")\n",
    "                \n",
    "            # Display charts if they were generated\n",
    "            if visualize_checkbox.value:\n",
    "                charts_dir = os.path.join(output_dir_text.value, \"charts\")\n",
    "                if os.path.exists(charts_dir):\n",
    "                    print(\"\nüìà Generated Charts:\")\n",
    "                    chart_files = os.listdir(charts_dir)\n",
    "                    for chart_file in chart_files:\n",
    "                        if chart_file.endswith(\".png\"):\n",
    "                            chart_path = os.path.join(charts_dir, chart_file)\n",
    "                            print(f\"  - {chart_file}\")\n",
    "            \n",
    "            print(f\"\nBenchmark completed successfully at {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\n‚ùå ERROR: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # Re-enable the button\n",
    "            run_button.disabled = False\n",
    "\n",
    "# Connect the button to the run function\n",
    "run_button.on_click(run_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà View Benchmark Results\n",
    "\n",
    "After a benchmark has completed, you can use this cell to view and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display comparison charts for multiple benchmarks\n",
    "def display_comparison_charts(results_dir):\n",
    "    result_files = [f for f in os.listdir(results_dir) if f.endswith(\".json\")]\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"No result files found.\")\n",
    "        return\n",
    "    \n",
    "    # Load all results into a list\n",
    "    all_results = []\n",
    "    for file in result_files:\n",
    "        with open(os.path.join(results_dir, file), \"r\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                # Extract strategy and pruning level from filename\n",
    "                parts = file.split(\"_\")\n",
    "                strategy = parts[0]\n",
    "                pruning_level = int(parts[1]) / 100\n",
    "                \n",
    "                all_results.append({\n",
    "                    \"strategy\": strategy,\n",
    "                    \"pruning_level\": pruning_level,\n",
    "                    \"data\": data\n",
    "                })\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error loading {file}: Invalid JSON\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No valid result files found.\")\n",
    "        return\n",
    "    \n",
    "    # Create comparison charts\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Group by strategy\n",
    "    strategies = set(r[\"strategy\"] for r in all_results)\n",
    "    \n",
    "    # 1. Speedup comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for strategy in strategies:\n",
    "        strategy_results = [r for r in all_results if r[\"strategy\"] == strategy]\n",
    "        strategy_results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        x = [r[\"pruning_level\"] * 100 for r in strategy_results]\n",
    "        y = [r[\"data\"][\"comparison\"][\"speedup\"] for r in strategy_results if \"comparison\" in r[\"data\"]]\n",
    "        \n",
    "        if y:  # Only plot if we have data\n",
    "            plt.plot(x, y, \"o-\", label=strategy)\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level (%)\")\n",
    "    plt.ylabel(\"Speedup Factor (√ó)\")\n",
    "    plt.title(\"Speedup vs Pruning Level\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 2. Quality comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for strategy in strategies:\n",
    "        strategy_results = [r for r in all_results if r[\"strategy\"] == strategy]\n",
    "        strategy_results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        x = [r[\"pruning_level\"] * 100 for r in strategy_results]\n",
    "        y = [r[\"data\"][\"comparison\"][\"quality_ratio\"] * 100 for r in strategy_results if \"comparison\" in r[\"data\"]]\n",
    "        \n",
    "        if y:  # Only plot if we have data\n",
    "            plt.plot(x, y, \"o-\", label=strategy)\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level (%)\")\n",
    "    plt.ylabel(\"Quality Retention (%)\")\n",
    "    plt.title(\"Quality vs Pruning Level\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 3. Memory comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for strategy in strategies:\n",
    "        strategy_results = [r for r in all_results if r[\"strategy\"] == strategy]\n",
    "        strategy_results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        x = [r[\"pruning_level\"] * 100 for r in strategy_results]\n",
    "        y = [r[\"data\"][\"comparison\"][\"memory_reduction\"] * 100 for r in strategy_results if \"comparison\" in r[\"data\"]]\n",
    "        \n",
    "        if y:  # Only plot if we have data\n",
    "            plt.plot(x, y, \"o-\", label=strategy)\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level (%)\")\n",
    "    plt.ylabel(\"Memory Reduction (%)\")\n",
    "    plt.title(\"Memory Reduction vs Pruning Level\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # 4. Efficiency score (speedup * quality)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for strategy in strategies:\n",
    "        strategy_results = [r for r in all_results if r[\"strategy\"] == strategy]\n",
    "        strategy_results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        x = [r[\"pruning_level\"] * 100 for r in strategy_results]\n",
    "        y = [r[\"data\"][\"comparison\"][\"speedup\"] * r[\"data\"][\"comparison\"][\"quality_ratio\"] \n",
    "             for r in strategy_results if \"comparison\" in r[\"data\"]]\n",
    "        \n",
    "        if y:  # Only plot if we have data\n",
    "            plt.plot(x, y, \"o-\", label=strategy)\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level (%)\")\n",
    "    plt.ylabel(\"Efficiency Score\")\n",
    "    plt.title(\"Efficiency (Speed√óQuality) vs Pruning Level\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a summary table\n",
    "    summary_data = []\n",
    "    for result in all_results:\n",
    "        if \"comparison\" in result[\"data\"]:\n",
    "            comp = result[\"data\"][\"comparison\"]\n",
    "            summary_data.append({\n",
    "                \"Strategy\": result[\"strategy\"],\n",
    "                \"Pruning Level\": f\"{result[\"pruning_level\"]*100:.0f}%\",\n",
    "                \"Speedup\": f\"{comp[\"speedup\"]:.2f}√ó\",\n",
    "                \"Quality\": f\"{comp[\"quality_ratio\"]*100:.1f}%\",\n",
    "                \"Memory Reduction\": f\"{comp[\"memory_reduction\"]*100:.1f}%\",\n",
    "                \"Efficiency\": f\"{comp[\"speedup\"] * comp[\"quality_ratio\"]:.2f}\"\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        display(HTML(\"<h3>Benchmark Summary Table</h3>\"))\n",
    "        display(df)\n",
    "\n",
    "# Function to view individual benchmark results\n",
    "def view_benchmark_result(results_dir, result_file):\n",
    "    file_path = os.path.join(results_dir, result_file)\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Extract key metrics\n",
    "    print(f\"Viewing results for: {result_file}\n\")\n",
    "    \n",
    "    if \"baseline\" in data:\n",
    "        print(\"Baseline Metrics:\")\n",
    "        print(f\"  Generation speed: {data[\"baseline\"][\"tokens_per_second\"]:.2f} tokens/sec\")\n",
    "        print(f\"  Memory usage: {data[\"baseline\"].get(\"memory_usage\", \"N/A\")} MB\")\n",
    "        print(f\"  Quality score: {data[\"baseline\"][\"quality_score\"]:.2f}\")\n",
    "        print(f\"  Perplexity: {data[\"baseline\"].get(\"perplexity\", \"N/A\")}\n\")\n",
    "    \n",
    "    # Get pruned model metrics\n",
    "    pruned_key = next((k for k in data.keys() if k.startswith(\"pruned_\")), None)\n",
    "    if pruned_key:\n",
    "        print(\"Pruned Model Metrics:\")\n",
    "        print(f\"  Generation speed: {data[pruned_key][\"tokens_per_second\"]:.2f} tokens/sec\")\n",
    "        print(f\"  Memory usage: {data[pruned_key].get(\"memory_usage\", \"N/A\")} MB\")\n",
    "        print(f\"  Quality score: {data[pruned_key][\"quality_score\"]:.2f}\")\n",
    "        print(f\"  Perplexity: {data[pruned_key].get(\"perplexity\", \"N/A\")}\n\")\n",
    "    \n",
    "    if \"comparison\" in data:\n",
    "        print(\"Comparison Results:\")\n",
    "        print(f\"  Speedup: {data[\"comparison\"][\"speedup\"]:.2f}√ó\")\n",
    "        print(f\"  Memory reduction: {data[\"comparison\"][\"memory_reduction\"]*100:.1f}%\")\n",
    "        print(f\"  Quality retention: {data[\"comparison\"][\"quality_ratio\"]*100:.1f}%\n\")\n",
    "    \n",
    "    # Display charts if available\n",
    "    charts_dir = os.path.join(results_dir, \"charts\")\n",
    "    result_prefix = result_file.split(\"_results.json\")[0]\n",
    "    \n",
    "    if os.path.exists(charts_dir):\n",
    "        chart_files = [f for f in os.listdir(charts_dir) if f.startswith(result_prefix.split(\"_\")[0])]\n",
    "        \n",
    "        if chart_files:\n",
    "            for chart_file in chart_files:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                img = plt.imread(os.path.join(charts_dir, chart_file))\n",
    "                plt.imshow(img)\n",
    "                plt.axis(\"off\")\n",
    "                plt.title(chart_file)\n",
    "                plt.show()\n",
    "\n",
    "# Create UI for viewing results\n",
    "if \"output_dir_text\" in globals():\n",
    "    results_dir = output_dir_text.value\n",
    "else:\n",
    "    results_dir = default_output_dir\n",
    "\n",
    "if os.path.exists(results_dir):\n",
    "    result_files = [f for f in os.listdir(results_dir) if f.endswith(\".json\")]\n",
    "    if result_files:\n",
    "        dropdown = widgets.Dropdown(\n",
    "            options=[(\"All Results (Comparison)\", \"all\")] + [(f, f) for f in result_files],\n",
    "            value=\"all\",\n",
    "            description=\"Select result:\",\n",
    "            style={\"description_width\": \"initial\"}\n",
    "        )\n",
    "        \n",
    "        def on_selection_change(change):\n",
    "            if change[\"new\"] == \"all\":\n",
    "                display_comparison_charts(results_dir)\n",
    "            else:\n",
    "                view_benchmark_result(results_dir, change[\"new\"])\n",
    "        \n",
    "        dropdown.observe(on_selection_change, names=\"value\")\n",
    "        display(dropdown)\n",
    "        \n",
    "        # Show comparison view by default\n",
    "        display_comparison_charts(results_dir)\n",
    "    else:\n",
    "        print(f\"No result files found in {results_dir}\")\n",
    "else:\n",
    "    print(f\"Results directory {results_dir} does not exist yet. Run a benchmark first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Notes and Observations\n",
    "\n",
    "Use this cell to record any observations or notes about your benchmarks.\n",
    "\n",
    "**Key Observations:**\n",
    "- Entropy-based pruning tends to provide the best quality-speed tradeoff\n",
    "- Random pruning can sometimes be surprisingly effective for moderate pruning levels\n",
    "- Magnitude-based pruning works well for reducing memory usage\n",
    "- At 30% pruning, quality degradation is minimal across all strategies\n",
    "- Beyond 70% pruning, quality degradation becomes significant\n",
    "\n",
    "**Next Steps:**\n",
    "- Run comparison between different model sizes\n",
    "- Test with different temperature settings\n",
    "- Evaluate on specific domains/tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save and Share Results\n",
    "\n",
    "If you need to share your benchmark results, you can use the following cell to create a ZIP archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of all results\n",
    "if IN_COLAB and os.path.exists(results_dir):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    zip_filename = f\"pruning_benchmark_results_{timestamp}.zip\"\n",
    "    \n",
    "    # Create ZIP file\n",
    "    \!zip -r {zip_filename} {results_dir}\n",
    "    \n",
    "    # If Google Drive is mounted, copy the ZIP there\n",
    "    if DRIVE_RESULTS_DIR:\n",
    "        \!cp {zip_filename} {DRIVE_RESULTS_DIR}/{zip_filename}\n",
    "        print(f\"Results archive saved to Google Drive: {DRIVE_RESULTS_DIR}/{zip_filename}\")\n",
    "    else:\n",
    "        print(f\"Results archive created: {zip_filename}\")\n",
    "        \n",
    "        # Provide download link\n",
    "        from google.colab import files\n",
    "        files.download(zip_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
