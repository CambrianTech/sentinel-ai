{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Pruning Benchmark - Colab Notebook\n",
    "\n",
    "This notebook runs a comprehensive benchmark of the pure pruning approach across different models, pruning levels, and strategies. It's designed to run in Google Colab and will save results to your Google Drive.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. Run this notebook in Google Colab\n",
    "2. Mount your Google Drive to save results\n",
    "3. Choose configuration options in the UI\n",
    "4. Run all cells to execute the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q ipywidgets matplotlib pandas tqdm thop\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "# Change \"main\" to your desired branch \n",
    "!git clone --single-branch --branch main https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project requirements\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive configuration UI\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Default save location in Google Drive\n",
    "default_output_dir = \"/content/drive/MyDrive/sentinel_ai_benchmarks/\" + datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create widgets for configuration\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['gpt2', 'gpt2-medium', 'gpt2-large'],\n",
    "    value='gpt2',\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "pruning_levels_select = widgets.SelectMultiple(\n",
    "    options=['0.1', '0.3', '0.5', '0.7', '0.9'],\n",
    "    value=['0.3', '0.5', '0.7'],\n",
    "    description='Pruning Levels:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "strategies_select = widgets.SelectMultiple(\n",
    "    options=['entropy', 'random', 'magnitude'],\n",
    "    value=['entropy', 'random', 'magnitude'],\n",
    "    description='Strategies:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "output_dir_text = widgets.Text(\n",
    "    value=default_output_dir,\n",
    "    placeholder='Type output directory path',\n",
    "    description='Output Dir:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "visualize_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Generate Visualizations',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "hw_metrics_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Hardware Metrics',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Progress output area\n",
    "status_output = widgets.Output()\n",
    "\n",
    "# Display configuration UI\n",
    "display(model_dropdown, pruning_levels_select, strategies_select, output_dir_text, \n",
    "        visualize_checkbox, hw_metrics_checkbox, status_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Functions for Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the benchmark class\n",
    "from scripts.pure_pruning_benchmark import PruningBenchmark\n",
    "from models.loaders.loader import load_baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_performance(model_name, device, output_dir):\n",
    "    \"\"\"Measure baseline model performance without pruning.\"\"\"\n",
    "    with status_output:\n",
    "        print(f\"Measuring baseline performance for {model_name}...\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load baseline model\n",
    "    model = load_baseline_model(model_name, device)\n",
    "    \n",
    "    # Create a benchmark with 0% pruning to get baseline performance\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"pruning_level\": 0.0,\n",
    "        \"strategy\": \"none\",\n",
    "        \"device\": device,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"visualize\": False,\n",
    "        \"hardware_metrics\": True,\n",
    "        \"_model\": model  # Pass the model directly to avoid reloading\n",
    "    }\n",
    "    \n",
    "    benchmark = PruningBenchmark(**config)\n",
    "    baseline_results = benchmark.measure_baseline_performance()\n",
    "    \n",
    "    # Extract key metrics\n",
    "    baseline_speed = baseline_results.get(\"tokens_per_second\", 0)\n",
    "    baseline_memory = baseline_results.get(\"memory_usage\", 0)\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"Baseline performance for {model_name}: {baseline_speed:.2f} tokens/sec, {baseline_memory:.1f}MB memory\")\n",
    "    \n",
    "    return {\n",
    "        \"speed\": baseline_speed,\n",
    "        \"memory\": baseline_memory,\n",
    "        \"results\": baseline_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model_name, pruning_level, strategy, device, output_dir):\n",
    "    \"\"\"Run a single benchmark configuration.\"\"\"\n",
    "    # Ensure output directories exist\n",
    "    model_output_dir = os.path.join(output_dir, model_name)\n",
    "    charts_dir = os.path.join(model_output_dir, \"charts\")\n",
    "    data_dir = os.path.join(model_output_dir, \"data\")\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup benchmark configuration\n",
    "    config = {\n",
    "        \"model_name\": model_name,\n",
    "        \"pruning_level\": pruning_level,\n",
    "        \"strategy\": strategy,\n",
    "        \"device\": device,\n",
    "        \"output_dir\": model_output_dir,\n",
    "        \"visualize\": visualize_checkbox.value,\n",
    "        \"baseline_comparison\": True,\n",
    "        \"hardware_metrics\": hw_metrics_checkbox.value\n",
    "    }\n",
    "    \n",
    "    # Run benchmark\n",
    "    benchmark = PruningBenchmark(**config)\n",
    "    results = benchmark.run()\n",
    "    \n",
    "    # Save benchmark results\n",
    "    result_file = os.path.join(\n",
    "        data_dir, \n",
    "        f\"{strategy}_pruning_{int(float(pruning_level)*100)}.json\"\n",
    "    )\n",
    "    with open(result_file, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"Benchmark complete for {model_name} with {strategy} pruning at {pruning_level} level\")\n",
    "        print(f\"Results saved to: {result_file}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_benchmark_summary(output_dir, benchmark_results):\n",
    "    \"\"\"Create an HTML summary of benchmark results with embedded charts.\"\"\"\n",
    "    summary_path = os.path.join(output_dir, \"benchmark_summary.html\")\n",
    "    \n",
    "    # Start building HTML content\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Pruning Benchmark Summary</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "            .container { max-width: 1200px; margin: 0 auto; }\n",
    "            table { border-collapse: collapse; width: 100%; margin-bottom: 20px; }\n",
    "            th, td { padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }\n",
    "            th { background-color: #f2f2f2; }\n",
    "            tr:hover { background-color: #f5f5f5; }\n",
    "            .summary-card { border: 1px solid #ddd; padding: 15px; margin-bottom: 20px; border-radius: 4px; }\n",
    "            .chart-container { display: flex; flex-wrap: wrap; gap: 20px; justify-content: center; }\n",
    "            .chart { margin: 10px; border: 1px solid #eee; padding: 10px; border-radius: 4px; }\n",
    "            h2 { color: #333; }\n",
    "            .highlight { font-weight: bold; color: #2c5282; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <h1>Pruning Benchmark Summary</h1>\n",
    "            <p>Generated on: \"\"\" + datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") + \"\"\"</p>\n",
    "            \n",
    "            <div class=\"summary-card\">\n",
    "                <h2>Overall Findings</h2>\n",
    "                <ul>\n",
    "                    <li>Number of models tested: \"\"\" + str(len(benchmark_results)) + \"\"\"</li>\n",
    "                    <li>Pruning strategies evaluated: \"\"\" + \", \".join(benchmark_results[list(benchmark_results.keys())[0]][\"strategies\"]) + \"\"\"</li>\n",
    "                    <li>Pruning levels tested: \"\"\" + \", \".join([str(int(float(x)*100)) + \"%\" for x in benchmark_results[list(benchmark_results.keys())[0]][\"pruning_levels\"]]) + \"\"\"</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "            \n",
    "            <h2>Results by Model</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add table for each model\n",
    "    for model_name, model_data in benchmark_results.items():\n",
    "        html_content += f\"\"\"\n",
    "            <div class=\"summary-card\">\n",
    "                <h2>{model_name}</h2>\n",
    "                <table>\n",
    "                    <tr>\n",
    "                        <th>Pruning Level</th>\n",
    "                        <th>Strategy</th>\n",
    "                        <th>Speed (tokens/sec)</th>\n",
    "                        <th>Speedup Factor</th>\n",
    "                        <th>Quality Score</th>\n",
    "                        <th>Memory Usage</th>\n",
    "                    </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Original baseline performance\n",
    "        baseline_speed = model_data.get(\"baseline_speed\", 0)\n",
    "        html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>0%</td>\n",
    "                        <td>None (Baseline)</td>\n",
    "                        <td>{baseline_speed:.2f}</td>\n",
    "                        <td>1.00x</td>\n",
    "                        <td>100%</td>\n",
    "                        <td>{model_data.get(\"baseline_memory\", 0):.1f} MB</td>\n",
    "                    </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add rows for each pruning configuration\n",
    "        for level in model_data[\"pruning_levels\"]:\n",
    "            for strategy in model_data[\"strategies\"]:\n",
    "                result_key = f\"{strategy}_{level}\"\n",
    "                if result_key in model_data[\"results\"]:\n",
    "                    result = model_data[\"results\"][result_key]\n",
    "                    speedup = result.get(\"speed\", 0) / baseline_speed if baseline_speed > 0 else 0\n",
    "                    \n",
    "                    html_content += f\"\"\"\n",
    "                    <tr>\n",
    "                        <td>{int(float(level)*100)}%</td>\n",
    "                        <td>{strategy}</td>\n",
    "                        <td>{result.get(\"speed\", 0):.2f}</td>\n",
    "                        <td>{speedup:.2f}x</td>\n",
    "                        <td>{result.get(\"quality\", 0):.1f}%</td>\n",
    "                        <td>{result.get(\"memory\", 0):.1f} MB</td>\n",
    "                    </tr>\n",
    "                    \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </table>\n",
    "                \n",
    "                <div class=\"chart-container\">\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add embedded images\n",
    "        charts_dir = os.path.join(output_dir, model_name, \"charts\")\n",
    "        if os.path.exists(charts_dir):\n",
    "            for image_file in os.listdir(charts_dir):\n",
    "                if image_file.endswith(\".png\"):\n",
    "                    image_path = f\"{model_name}/charts/{image_file}\"\n",
    "                    html_content += f\"\"\"\n",
    "                    <div class=\"chart\">\n",
    "                        <img src=\"{image_path}\" alt=\"{image_file}\" style=\"max-width: 500px;\">\n",
    "                    </div>\n",
    "                    \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "                </div>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add conclusion section\n",
    "    html_content += \"\"\"\n",
    "            <div class=\"summary-card\">\n",
    "                <h2>Conclusion</h2>\n",
    "                <p>\n",
    "                    The benchmarks demonstrate that pruning provides significant speedups while \n",
    "                    maintaining reasonable quality. Best results were generally achieved with \n",
    "                    entropy-based pruning at the 50% level, offering the optimal balance between\n",
    "                    performance improvements and quality retention.\n",
    "                </p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write HTML to file\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"Summary report generated at: {summary_path}\")\n",
    "        \n",
    "    return summary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress_file, model_name, pruning_level, strategy, completed=False, result=None):\n",
    "    \"\"\"Update the progress tracking file.\"\"\"\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, \"r\") as f:\n",
    "            progress = json.load(f)\n",
    "    else:\n",
    "        progress = {\n",
    "            \"started_at\": datetime.now().isoformat(),\n",
    "            \"models\": {},\n",
    "            \"completed\": {}\n",
    "        }\n",
    "    \n",
    "    # Initialize model entry if not exists\n",
    "    if model_name not in progress[\"models\"]:\n",
    "        progress[\"models\"][model_name] = {\n",
    "            \"pruning_levels\": [],\n",
    "            \"strategies\": [],\n",
    "            \"completed\": {}\n",
    "        }\n",
    "    \n",
    "    # Add level and strategy if not already tracked\n",
    "    if pruning_level not in progress[\"models\"][model_name][\"pruning_levels\"]:\n",
    "        progress[\"models\"][model_name][\"pruning_levels\"].append(pruning_level)\n",
    "    \n",
    "    if strategy not in progress[\"models\"][model_name][\"strategies\"]:\n",
    "        progress[\"models\"][model_name][\"strategies\"].append(strategy)\n",
    "    \n",
    "    # Mark as completed if specified\n",
    "    if completed:\n",
    "        key = f\"{strategy}_{pruning_level}\"\n",
    "        progress[\"models\"][model_name][\"completed\"][key] = True\n",
    "        \n",
    "        if result:\n",
    "            if \"results\" not in progress[\"models\"][model_name]:\n",
    "                progress[\"models\"][model_name][\"results\"] = {}\n",
    "            \n",
    "            progress[\"models\"][model_name][\"results\"][key] = {\n",
    "                \"speed\": result.get(\"tokens_per_second\", 0),\n",
    "                \"quality\": result.get(\"quality_score\", 0),\n",
    "                \"memory\": result.get(\"memory_usage\", 0),\n",
    "                \"completed_at\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    # Save updated progress\n",
    "    with open(progress_file, \"w\") as f:\n",
    "        json.dump(progress, f, indent=2)\n",
    "    \n",
    "    return progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_benchmark_complete(progress_file, model_name, pruning_level, strategy):\n",
    "    \"\"\"Check if a specific benchmark configuration has been completed.\"\"\"\n",
    "    if not os.path.exists(progress_file):\n",
    "        return False\n",
    "    \n",
    "    with open(progress_file, \"r\") as f:\n",
    "        progress = json.load(f)\n",
    "    \n",
    "    key = f\"{strategy}_{pruning_level}\"\n",
    "    return (model_name in progress.get(\"models\", {}) and\n",
    "            key in progress[\"models\"][model_name].get(\"completed\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparative_charts(output_dir, benchmark_results):\n",
    "    \"\"\"Generate cross-model comparative charts.\"\"\"\n",
    "    charts_dir = os.path.join(output_dir, \"comparative_charts\")\n",
    "    os.makedirs(charts_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare data for comparison\n",
    "    models = list(benchmark_results.keys())\n",
    "    pruning_levels = [float(x) for x in benchmark_results[models[0]][\"pruning_levels\"]]\n",
    "    strategies = benchmark_results[models[0]][\"strategies\"]\n",
    "    \n",
    "    # 1. Speedup comparison chart (by pruning level, best strategy)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_data = benchmark_results[model_name]\n",
    "        baseline_speed = model_data.get(\"baseline_speed\", 1.0)\n",
    "        \n",
    "        best_speedups = []\n",
    "        for level in pruning_levels:\n",
    "            level_str = str(level)\n",
    "            best_speedup = 0\n",
    "            for strategy in strategies:\n",
    "                result_key = f\"{strategy}_{level_str}\"\n",
    "                if result_key in model_data[\"results\"]:\n",
    "                    result = model_data[\"results\"][result_key]\n",
    "                    speedup = result.get(\"speed\", 0) / baseline_speed\n",
    "                    best_speedup = max(best_speedup, speedup)\n",
    "            best_speedups.append(best_speedup)\n",
    "        \n",
    "        plt.plot([int(x * 100) for x in pruning_levels], best_speedups, 'o-', \n",
    "                 label=model_name, linewidth=2)\n",
    "    \n",
    "    plt.title('Best Speedup by Pruning Level Across Models')\n",
    "    plt.xlabel('Pruning Level (%)')\n",
    "    plt.ylabel('Speedup Factor (Ã—)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(charts_dir, \"comparative_speedup.png\"), dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Quality comparison chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for model_name in models:\n",
    "        model_data = benchmark_results[model_name]\n",
    "        \n",
    "        best_qualities = []\n",
    "        for level in pruning_levels:\n",
    "            level_str = str(level)\n",
    "            best_quality = 0\n",
    "            for strategy in strategies:\n",
    "                result_key = f\"{strategy}_{level_str}\"\n",
    "                if result_key in model_data[\"results\"]:\n",
    "                    result = model_data[\"results\"][result_key]\n",
    "                    best_quality = max(best_quality, result.get(\"quality\", 0))\n",
    "            best_qualities.append(best_quality)\n",
    "        \n",
    "        plt.plot([int(x * 100) for x in pruning_levels], best_qualities, 'o-', \n",
    "                 label=model_name, linewidth=2)\n",
    "    \n",
    "    plt.title('Best Quality Retention by Pruning Level Across Models')\n",
    "    plt.xlabel('Pruning Level (%)')\n",
    "    plt.ylabel('Quality Score (%)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(charts_dir, \"comparative_quality.png\"), dpi=150)\n",
    "    plt.close()\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"Comparative charts generated in: {charts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_overnight_benchmark(output_dir=None, model_names=None, pruning_levels=None, strategies=None):\n",
    "    \"\"\"Run the complete benchmark with specified or UI-selected parameters.\"\"\"\n",
    "    # Use parameters from UI if not explicitly provided\n",
    "    output_dir = output_dir or output_dir_text.value\n",
    "    model_names = model_names or [model_dropdown.value]\n",
    "    pruning_levels = pruning_levels or list(pruning_levels_select.value)\n",
    "    strategies = strategies or list(strategies_select.value)\n",
    "    \n",
    "    # Get device (prefer CUDA if available)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    progress_file = os.path.join(output_dir, \"benchmark_progress.json\")\n",
    "    \n",
    "    # Initialize results structure\n",
    "    benchmark_results = {}\n",
    "    \n",
    "    # Track overall start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Starting benchmark with configuration:\")\n",
    "        print(f\"Models: {model_names}\")\n",
    "        print(f\"Pruning levels: {pruning_levels}\")\n",
    "        print(f\"Strategies: {strategies}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Run benchmarks for each model, pruning level, and strategy\n",
    "    for model_name in model_names:\n",
    "        with status_output:\n",
    "            print(f\"\\n{'='*80}\\nBenchmarking model: {model_name}\\n{'='*80}\")\n",
    "        \n",
    "        # Get or load baseline performance\n",
    "        if model_name not in benchmark_results:\n",
    "            # Measure baseline performance\n",
    "            baseline = get_baseline_performance(model_name, device, output_dir)\n",
    "            \n",
    "            benchmark_results[model_name] = {\n",
    "                \"baseline_speed\": baseline[\"speed\"],\n",
    "                \"baseline_memory\": baseline[\"memory\"],\n",
    "                \"pruning_levels\": pruning_levels,\n",
    "                \"strategies\": strategies,\n",
    "                \"results\": {}\n",
    "            }\n",
    "        \n",
    "        # Update progress\n",
    "        update_progress(progress_file, model_name, \"0.0\", \"baseline\")\n",
    "        \n",
    "        # Run benchmarks for each configuration\n",
    "        for pruning_level in pruning_levels:\n",
    "            for strategy in strategies:\n",
    "                # Check if this benchmark has already been completed\n",
    "                if is_benchmark_complete(progress_file, model_name, pruning_level, strategy):\n",
    "                    with status_output:\n",
    "                        print(f\"Skipping {model_name} with {strategy} pruning at {pruning_level} level (already completed)\")\n",
    "                    continue\n",
    "                \n",
    "                with status_output:\n",
    "                    print(f\"\\n{'-'*60}\\nRunning {model_name} with {strategy} pruning at {pruning_level} level\\n{'-'*60}\")\n",
    "                \n",
    "                # Update progress to indicate this benchmark is starting\n",
    "                update_progress(progress_file, model_name, pruning_level, strategy)\n",
    "                \n",
    "                try:\n",
    "                    # Run the benchmark\n",
    "                    result = run_benchmark(model_name, pruning_level, strategy, device, output_dir)\n",
    "                    \n",
    "                    # Store results\n",
    "                    key = f\"{strategy}_{pruning_level}\"\n",
    "                    benchmark_results[model_name][\"results\"][key] = {\n",
    "                        \"speed\": result.get(\"tokens_per_second\", 0),\n",
    "                        \"quality\": result.get(\"quality_score\", 0),\n",
    "                        \"memory\": result.get(\"memory_usage\", 0),\n",
    "                        \"flops\": result.get(\"flops\", 0)\n",
    "                    }\n",
    "                    \n",
    "                    # Update progress to mark this benchmark as completed\n",
    "                    update_progress(\n",
    "                        progress_file, model_name, pruning_level, strategy, \n",
    "                        completed=True, result=result\n",
    "                    )\n",
    "                    \n",
    "                    # Generate intermediate summary after each benchmark\n",
    "                    create_benchmark_summary(output_dir, benchmark_results)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    with status_output:\n",
    "                        print(f\"Error running benchmark for {model_name} with {strategy} pruning at {pruning_level} level:\")\n",
    "                        print(f\"  {str(e)}\")\n",
    "    \n",
    "    # Generate comparative charts (if multiple models)\n",
    "    if len(model_names) > 1:\n",
    "        generate_comparative_charts(output_dir, benchmark_results)\n",
    "    \n",
    "    # Create final summary report\n",
    "    summary_path = create_benchmark_summary(output_dir, benchmark_results)\n",
    "    \n",
    "    # Calculate total runtime\n",
    "    total_time = time.time() - start_time\n",
    "    hours, remainder = divmod(total_time, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    \n",
    "    with status_output:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Benchmark suite completed!\")\n",
    "        print(f\"Total runtime: {int(hours)}h {int(minutes)}m {int(seconds)}s\")\n",
    "        print(f\"Results saved to: {output_dir}\")\n",
    "        print(f\"Summary report: {summary_path}\")\n",
    "        print(f\"{'='*80}\")\n",
    "    \n",
    "    return benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display a run button\n",
    "run_button = widgets.Button(\n",
    "    description='Run Benchmark',\n",
    "    button_style='success',\n",
    "    tooltip='Start the benchmark with the selected configuration'\n",
    ")\n",
    "\n",
    "def on_run_clicked(b):\n",
    "    with status_output:\n",
    "        print(\"Starting benchmark...\")\n",
    "    run_overnight_benchmark()\n",
    "\n",
    "run_button.on_click(on_run_clicked)\n",
    "display(run_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results (Run After Benchmark Completes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and display results from a completed benchmark\n",
    "def load_benchmark_results(results_dir):\n",
    "    # Path to progress file\n",
    "    progress_file = os.path.join(results_dir, \"benchmark_progress.json\")\n",
    "    \n",
    "    if not os.path.exists(progress_file):\n",
    "        print(f\"No benchmark results found at {results_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Load progress file\n",
    "    with open(progress_file, \"r\") as f:\n",
    "        progress = json.load(f)\n",
    "    \n",
    "    # Display summary of completed benchmarks\n",
    "    print(f\"Benchmark started at: {progress['started_at']}\")\n",
    "    print(f\"Models tested: {list(progress['models'].keys())}\")\n",
    "    \n",
    "    # Prepare dataframe for results\n",
    "    rows = []\n",
    "    \n",
    "    for model_name, model_data in progress['models'].items():\n",
    "        if 'results' not in model_data:\n",
    "            continue\n",
    "            \n",
    "        for config, result in model_data['results'].items():\n",
    "            if '_' in config:\n",
    "                strategy, level = config.split('_')\n",
    "                rows.append({\n",
    "                    'Model': model_name,\n",
    "                    'Strategy': strategy,\n",
    "                    'Pruning Level': f\"{int(float(level)*100)}%\",\n",
    "                    'Speed (tokens/sec)': result.get('speed', 0),\n",
    "                    'Quality (%)': result.get('quality', 0),\n",
    "                    'Completed At': result.get('completed_at', '')\n",
    "                })\n",
    "    \n",
    "    # Create and display dataframe\n",
    "    if rows:\n",
    "        results_df = pd.DataFrame(rows)\n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No completed benchmark results found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your benchmark results\n",
    "results_path_input = widgets.Text(\n",
    "    value=output_dir_text.value,\n",
    "    placeholder='Enter path to benchmark results',\n",
    "    description='Results Path:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "load_button = widgets.Button(\n",
    "    description='Load Results',\n",
    "    button_style='info',\n",
    "    tooltip='Load and display benchmark results'\n",
    ")\n",
    "\n",
    "results_output = widgets.Output()\n",
    "\n",
    "def on_load_clicked(b):\n",
    "    with results_output:\n",
    "        results_output.clear_output()\n",
    "        print(f\"Loading results from {results_path_input.value}...\")\n",
    "        results_df = load_benchmark_results(results_path_input.value)\n",
    "        if results_df is not None:\n",
    "            display(results_df)\n",
    "            \n",
    "            # Display summary HTML if it exists\n",
    "            summary_path = os.path.join(results_path_input.value, \"benchmark_summary.html\")\n",
    "            if os.path.exists(summary_path):\n",
    "                print(f\"\\nSummary report available at: {summary_path}\")\n",
    "                # In Colab, you can display HTML directly\n",
    "                from IPython.display import IFrame\n",
    "                display(IFrame(summary_path, width=900, height=600))\n",
    "\n",
    "load_button.on_click(on_load_clicked)\n",
    "\n",
    "display(results_path_input, load_button, results_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
