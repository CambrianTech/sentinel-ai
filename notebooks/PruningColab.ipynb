{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Benchmark Notebook (Colab-Optimized)\n",
    "\n",
    "This notebook is designed to run pruning benchmarks on Google Colab overnight, leveraging JAX/Flax for stability and performance.\n",
    "\n",
    "## Features\n",
    "- Automatically detects and utilizes TPU/GPU when available\n",
    "- Uses JAX/Flax for stable operation (works on M1/M2 Macs as well)\n",
    "- Progressive visualization during benchmark runs\n",
    "- Comprehensive analysis after completion\n",
    "- Supports multiple models and pruning strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import the pruning library\n",
    "from utils.pruning import (\n",
    "    Environment,\n",
    "    ResultsManager,\n",
    "    PruningBenchmark\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize results manager\n",
    "results_manager = ResultsManager(\"pruning_results\")\n",
    "results_manager.load_results()\n",
    "results_manager.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize benchmark runner\n",
    "benchmark = PruningBenchmark(results_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Single Benchmark\n",
    "\n",
    "Let's run a single benchmark to test our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a single benchmark\n",
    "model_name = env.get_suitable_models()[0]  # Use the smallest model\n",
    "result = benchmark.run_single_benchmark(\n",
    "    model_name=model_name,\n",
    "    strategy_name=\"random\",\n",
    "    pruning_level=0.1,\n",
    "    prompt=\"Artificial intelligence will transform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the results so far\n",
    "results_manager.plot_results(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Pruning Test\n",
    "\n",
    "Test how much we can prune before the model breaks down completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "MODELS = env.get_suitable_models()[:2]  # Use the first 2 models\n",
    "STRATEGIES = [\"random\", \"magnitude\"]\n",
    "PRUNING_LEVELS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "PROMPT = \"Artificial intelligence will revolutionize\"\n",
    "MAX_RUNTIME = 3600  # 1 hour\n",
    "\n",
    "print(f\"Running progressive pruning test with:\")\n",
    "print(f\"  Models: {MODELS}\")\n",
    "print(f\"  Strategies: {STRATEGIES}\")\n",
    "print(f\"  Pruning levels: {PRUNING_LEVELS}\")\n",
    "print(f\"  Prompt: '{PROMPT}'\")\n",
    "print(f\"  Maximum runtime: {MAX_RUNTIME/3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the benchmarks\n",
    "results = benchmark.run_multiple_benchmarks(\n",
    "    models=MODELS,\n",
    "    strategies=STRATEGIES,\n",
    "    pruning_levels=PRUNING_LEVELS,\n",
    "    prompt=PROMPT,\n",
    "    max_runtime=MAX_RUNTIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Model Comparison\n",
    "\n",
    "Compare how different model architectures respond to pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get all available models\n",
    "ALL_MODELS = env.get_suitable_models()\n",
    "print(f\"Available models: {ALL_MODELS}\")\n",
    "\n",
    "# Configuration\n",
    "STRATEGY = \"magnitude\"  # Most stable strategy\n",
    "COMPARISON_LEVELS = [0.1, 0.3, 0.5, 0.7]\n",
    "COMPARISON_PROMPT = \"Artificial intelligence will transform society by\"\n",
    "COMPARISON_RUNTIME = 7200  # 2 hours\n",
    "\n",
    "print(f\"\\nRunning multi-model comparison with:\")\n",
    "print(f\"  Models: {ALL_MODELS}\")\n",
    "print(f\"  Strategy: {STRATEGY}\")\n",
    "print(f\"  Pruning levels: {COMPARISON_LEVELS}\")\n",
    "print(f\"  Prompt: '{COMPARISON_PROMPT}'\")\n",
    "print(f\"  Maximum runtime: {COMPARISON_RUNTIME/3600:.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run the multi-model comparison\n",
    "comparison_results = benchmark.run_multiple_benchmarks(\n",
    "    models=ALL_MODELS,\n",
    "    strategies=[STRATEGY],\n",
    "    pruning_levels=COMPARISON_LEVELS,\n",
    "    prompt=COMPARISON_PROMPT,\n",
    "    max_runtime=COMPARISON_RUNTIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Benchmark\n",
    "\n",
    "Run a comprehensive overnight benchmark testing all combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration for overnight run\n",
    "OVERNIGHT_MODELS = env.get_suitable_models()  # Use all available models\n",
    "OVERNIGHT_STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "OVERNIGHT_LEVELS = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "OVERNIGHT_PROMPT = \"Artificial intelligence will revolutionize industries by\"\n",
    "OVERNIGHT_RUNTIME = 8 * 3600  # 8 hours\n",
    "\n",
    "# Calculate number of benchmarks\n",
    "TOTAL_BENCHMARKS = len(OVERNIGHT_MODELS) * len(OVERNIGHT_STRATEGIES) * len(OVERNIGHT_LEVELS)\n",
    "\n",
    "print(f\"Overnight benchmark configuration:\")\n",
    "print(f\"  Models: {OVERNIGHT_MODELS}\")\n",
    "print(f\"  Strategies: {OVERNIGHT_STRATEGIES}\")\n",
    "print(f\"  Pruning levels: {OVERNIGHT_LEVELS}\")\n",
    "print(f\"  Prompt: '{OVERNIGHT_PROMPT}'\")\n",
    "print(f\"  Maximum runtime: {OVERNIGHT_RUNTIME/3600:.1f} hours\")\n",
    "print(f\"  Total benchmarks: {TOTAL_BENCHMARKS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Uncomment to run overnight benchmarks\n",
    "# overnight_results = benchmark.run_multiple_benchmarks(\n",
    "#     models=OVERNIGHT_MODELS,\n",
    "#     strategies=OVERNIGHT_STRATEGIES,\n",
    "#     pruning_levels=OVERNIGHT_LEVELS,\n",
    "#     prompt=OVERNIGHT_PROMPT,\n",
    "#     max_runtime=OVERNIGHT_RUNTIME\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis\n",
    "\n",
    "Analyze all benchmark results collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all results\n",
    "results_manager.load_results()\n",
    "results_manager.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic visualization\n",
    "fig = results_manager.plot_results(figsize=(14, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Advanced analysis\n",
    "if hasattr(results_manager, 'plot_advanced_analysis'):\n",
    "    results_manager.plot_advanced_analysis(figsize=(14, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Model Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model Comparison Plot\n",
    "if results_manager.results_df is not None and not results_manager.results_df.empty:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Get unique models and strategies\n",
    "    models = results_manager.results_df[\"model\"].unique()\n",
    "    strategies = results_manager.results_df[\"strategy\"].unique()\n",
    "    \n",
    "    # For each model and strategy combination\n",
    "    for model in models:\n",
    "        for strategy in strategies:\n",
    "            # Filter data\n",
    "            data = results_manager.results_df[\n",
    "                (results_manager.results_df[\"model\"] == model) &\n",
    "                (results_manager.results_df[\"strategy\"] == strategy)\n",
    "            ]\n",
    "            \n",
    "            if not data.empty:\n",
    "                # Sort by pruning level\n",
    "                data = data.sort_values(\"pruning_level\")\n",
    "                \n",
    "                # Plot\n",
    "                plt.plot(\n",
    "                    data[\"pruning_level\"],\n",
    "                    data[\"perplexity_change\"],\n",
    "                    marker=\"o\",\n",
    "                    label=f\"{model} - {strategy}\"\n",
    "                )\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level\")\n",
    "    plt.ylabel(\"Perplexity Change\")\n",
    "    plt.title(\"Effect of Pruning on Different Models\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}