{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head Agency: Proof of Concept\n",
    "\n",
    "This notebook demonstrates the benefits of attention head agency in the Sentinel-AI framework. We'll show how attention heads can express internal states and how the system respects these signals during computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "from models.loaders.loader import load_baseline_model, load_adaptive_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Baseline and Adaptive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "baseline_model = load_baseline_model(model_name, device)\n",
    "adaptive_model = load_adaptive_model(model_name, baseline_model, device)\n",
    "\n",
    "# Helper function for generating text\n",
    "def generate_text(model, prompt, max_tokens=50):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=len(inputs.input_ids[0]) + max_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return {\n",
    "        \"text\": generated_text,\n",
    "        \"time\": end_time - start_time,\n",
    "        \"tokens_per_second\": max_tokens / (end_time - start_time)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Agency Features Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the agency features are available\n",
    "has_agency = hasattr(adaptive_model, \"get_agency_report\")\n",
    "print(f\"Agency features available: {has_agency}\")\n",
    "\n",
    "if has_agency:\n",
    "    # Check agency report before any states are changed\n",
    "    initial_report = adaptive_model.get_agency_report()\n",
    "    print(\"Initial agency report:\")\n",
    "    print(f\"Total layers: {initial_report['num_layers']}\")\n",
    "    print(f\"Total violations: {initial_report['total_violations']}\")\n",
    "    \n",
    "    # Check a specific layer\n",
    "    if len(initial_report['layer_reports']) > 0:\n",
    "        layer_idx = 0  # First layer\n",
    "        layer_report = initial_report['layer_reports'].get(layer_idx, {})\n",
    "        print(f\"\\nLayer {layer_idx} report:\")\n",
    "        for key, value in layer_report.items():\n",
    "            if key != 'recent_violations':  # Skip lengthy violations list\n",
    "                print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Agency features not found in the model. Make sure you're using the latest version with agency support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test Generation with Default Agency States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time in a land far away,\",\n",
    "    \"The future of artificial intelligence depends on\",\n",
    "    \"To solve the world's most pressing problems, we need to\"\n",
    "]\n",
    "\n",
    "# Generate text with default agency states\n",
    "print(\"Generating text with default agency states...\\n\")\n",
    "default_results = {}\n",
    "\n",
    "for idx, prompt in enumerate(prompts):\n",
    "    print(f\"Prompt {idx+1}: {prompt}\")\n",
    "    result = generate_text(adaptive_model, prompt)\n",
    "    print(f\"Generated: {result['text']}\")\n",
    "    print(f\"Generation time: {result['time']:.2f} seconds\")\n",
    "    print(f\"Tokens per second: {result['tokens_per_second']:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    default_results[idx] = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Simulate \"Overloaded\" State in Some Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    # Set some heads to \"overloaded\" state in multiple layers\n",
    "    num_layers = adaptive_model.num_layers\n",
    "    heads_per_layer = 12  # Standard for distilgpt2\n",
    "    \n",
    "    # Mark a subset of heads as overloaded\n",
    "    overloaded_heads = {}\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Mark 1/3 of heads as overloaded in each layer\n",
    "        for head_idx in range(0, heads_per_layer, 3):  # Every 3rd head\n",
    "            adaptive_model.set_head_state(layer_idx, head_idx, \"overloaded\")\n",
    "            if layer_idx not in overloaded_heads:\n",
    "                overloaded_heads[layer_idx] = []\n",
    "            overloaded_heads[layer_idx].append(head_idx)\n",
    "    \n",
    "    print(\"Set the following heads to 'overloaded' state:\")\n",
    "    for layer_idx, heads in overloaded_heads.items():\n",
    "        print(f\"Layer {layer_idx}: Heads {heads}\")\n",
    "    \n",
    "    # Check agency report after setting states\n",
    "    new_report = adaptive_model.get_agency_report()\n",
    "    print(\"\\nAgency report after setting states:\")\n",
    "    for layer_idx in range(num_layers):\n",
    "        if layer_idx in new_report['layer_reports']:\n",
    "            layer_report = new_report['layer_reports'][layer_idx]\n",
    "            print(f\"Layer {layer_idx}: {layer_report['active_heads']} active, {layer_report['overloaded_heads']} overloaded\")\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Generation with Overloaded Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    print(\"Generating text with overloaded heads...\\n\")\n",
    "    overloaded_results = {}\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        print(f\"Prompt {idx+1}: {prompt}\")\n",
    "        result = generate_text(adaptive_model, prompt)\n",
    "        print(f\"Generated: {result['text']}\")\n",
    "        print(f\"Generation time: {result['time']:.2f} seconds\")\n",
    "        print(f\"Tokens per second: {result['tokens_per_second']:.2f}\")\n",
    "        print()\n",
    "        \n",
    "        overloaded_results[idx] = result\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Simulate \"Withdrawn\" Consent in Some Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    # Reset all heads to active state\n",
    "    for layer_idx in range(num_layers):\n",
    "        for head_idx in range(heads_per_layer):\n",
    "            adaptive_model.set_head_state(layer_idx, head_idx, \"active\")\n",
    "    \n",
    "    # Now withdraw consent for some heads\n",
    "    withdrawn_heads = {}\n",
    "    for layer_idx in range(num_layers):\n",
    "        # Withdraw consent for every 4th head\n",
    "        for head_idx in range(0, heads_per_layer, 4): \n",
    "            adaptive_model.set_head_state(layer_idx, head_idx, \"withdrawn\", consent=False)\n",
    "            if layer_idx not in withdrawn_heads:\n",
    "                withdrawn_heads[layer_idx] = []\n",
    "            withdrawn_heads[layer_idx].append(head_idx)\n",
    "    \n",
    "    print(\"Withdrawn consent for the following heads:\")\n",
    "    for layer_idx, heads in withdrawn_heads.items():\n",
    "        print(f\"Layer {layer_idx}: Heads {heads}\")\n",
    "    \n",
    "    # Check agency report after withdrawing consent\n",
    "    withdrawn_report = adaptive_model.get_agency_report()\n",
    "    print(\"\\nAgency report after withdrawing consent:\")\n",
    "    for layer_idx in range(num_layers):\n",
    "        if layer_idx in withdrawn_report['layer_reports']:\n",
    "            layer_report = withdrawn_report['layer_reports'][layer_idx]\n",
    "            print(f\"Layer {layer_idx}: {layer_report['active_heads']} active, {layer_report['withdrawn_heads']} withdrawn\")\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Generation with Withdrawn Consent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    print(\"Generating text with withdrawn consent...\\n\")\n",
    "    withdrawn_results = {}\n",
    "    \n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        print(f\"Prompt {idx+1}: {prompt}\")\n",
    "        result = generate_text(adaptive_model, prompt)\n",
    "        print(f\"Generated: {result['text']}\")\n",
    "        print(f\"Generation time: {result['time']:.2f} seconds\")\n",
    "        print(f\"Tokens per second: {result['tokens_per_second']:.2f}\")\n",
    "        print()\n",
    "        \n",
    "        withdrawn_results[idx] = result\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check for Consent Violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    # Force some gate values to be high for heads with withdrawn consent\n",
    "    print(\"Setting high gate values for some withdrawn heads to trigger consent violations...\")\n",
    "    \n",
    "    # Take a subset of withdrawn heads and force their gates to be high\n",
    "    violation_heads = {}\n",
    "    with torch.no_grad():\n",
    "        for layer_idx, heads in withdrawn_heads.items():\n",
    "            if heads:  # if there are withdrawn heads in this layer\n",
    "                # Force the first withdrawn head to have a high gate value\n",
    "                head_idx = heads[0]\n",
    "                adaptive_model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(0.9, device=device)\n",
    "                \n",
    "                if layer_idx not in violation_heads:\n",
    "                    violation_heads[layer_idx] = []\n",
    "                violation_heads[layer_idx].append(head_idx)\n",
    "    \n",
    "    print(\"Set high gate values for the following withdrawn heads:\")\n",
    "    for layer_idx, heads in violation_heads.items():\n",
    "        print(f\"Layer {layer_idx}: Heads {heads}\")\n",
    "    \n",
    "    # Generate text once more to trigger violations\n",
    "    print(\"\\nGenerating text to trigger consent violations...\")\n",
    "    result = generate_text(adaptive_model, prompts[0])\n",
    "    print(f\"Generated: {result['text']}\")\n",
    "    \n",
    "    # Check for violations\n",
    "    violation_report = adaptive_model.get_agency_report()\n",
    "    total_violations = violation_report['total_violations']\n",
    "    print(f\"\\nTotal violations detected: {total_violations}\")\n",
    "    \n",
    "    # Show some recent violations if any\n",
    "    for layer_idx, report in violation_report['layer_reports'].items():\n",
    "        if report['violation_count'] > 0:\n",
    "            print(f\"\\nViolations in Layer {layer_idx}:\")\n",
    "            for violation in report['recent_violations']:\n",
    "                print(f\"  Head {violation['head_idx']}: {violation['violation_type']}\")\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Compare Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    # Reset heads to active state for fair comparison\n",
    "    for layer_idx in range(num_layers):\n",
    "        for head_idx in range(heads_per_layer):\n",
    "            adaptive_model.set_head_state(layer_idx, head_idx, \"active\", consent=True)\n",
    "    \n",
    "    # Compare performance metrics across different agency states\n",
    "    print(\"Performance comparison across different agency states:\")\n",
    "    \n",
    "    states = [\"Default\", \"Overloaded\", \"Withdrawn\"]\n",
    "    metrics = {state: [] for state in states}\n",
    "    \n",
    "    for idx in range(len(prompts)):\n",
    "        if idx in default_results and idx in overloaded_results and idx in withdrawn_results:\n",
    "            metrics[\"Default\"].append(default_results[idx]['tokens_per_second'])\n",
    "            metrics[\"Overloaded\"].append(overloaded_results[idx]['tokens_per_second'])\n",
    "            metrics[\"Withdrawn\"].append(withdrawn_results[idx]['tokens_per_second'])\n",
    "    \n",
    "    # Calculate averages\n",
    "    averages = {state: np.mean(speeds) for state, speeds in metrics.items()}\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(averages.keys(), averages.values(), color=['green', 'orange', 'red'])\n",
    "    \n",
    "    # Add labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                 f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Generation Speed by Head Agency State')\n",
    "    plt.ylabel('Tokens per Second')\n",
    "    plt.ylim(0, max(averages.values()) * 1.2)  # Add some headroom\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show comparison table\n",
    "    print(\"\\nAverage tokens per second:\")\n",
    "    for state, avg in averages.items():\n",
    "        print(f\"{state}: {avg:.2f} tokens/sec\")\n",
    "    \n",
    "    # Calculate relative performance\n",
    "    baseline = averages[\"Default\"]\n",
    "    print(\"\\nRelative performance:\")\n",
    "    for state, avg in averages.items():\n",
    "        if state != \"Default\":\n",
    "            relative = (avg / baseline) * 100\n",
    "            print(f\"{state}: {relative:.1f}% of baseline\")\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Analyze Text Quality Across Agency States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_agency:\n",
    "    # Simple text quality metrics\n",
    "    def analyze_text_quality(text):\n",
    "        words = text.split()\n",
    "        unique_words = set(words)\n",
    "        \n",
    "        return {\n",
    "            \"length\": len(words),\n",
    "            \"unique_words\": len(unique_words),\n",
    "            \"lexical_diversity\": len(unique_words) / len(words) if words else 0,\n",
    "        }\n",
    "    \n",
    "    print(\"Text quality comparison across different agency states:\")\n",
    "    \n",
    "    quality_metrics = {state: [] for state in states}\n",
    "    \n",
    "    for idx in range(len(prompts)):\n",
    "        if idx in default_results and idx in overloaded_results and idx in withdrawn_results:\n",
    "            default_quality = analyze_text_quality(default_results[idx]['text'])\n",
    "            overloaded_quality = analyze_text_quality(overloaded_results[idx]['text'])\n",
    "            withdrawn_quality = analyze_text_quality(withdrawn_results[idx]['text'])\n",
    "            \n",
    "            quality_metrics[\"Default\"].append(default_quality['lexical_diversity'])\n",
    "            quality_metrics[\"Overloaded\"].append(overloaded_quality['lexical_diversity'])\n",
    "            quality_metrics[\"Withdrawn\"].append(withdrawn_quality['lexical_diversity'])\n",
    "            \n",
    "            print(f\"\\nPrompt {idx+1}:\")\n",
    "            print(f\"Default - Length: {default_quality['length']}, Unique words: {default_quality['unique_words']}, Diversity: {default_quality['lexical_diversity']:.3f}\")\n",
    "            print(f\"Overloaded - Length: {overloaded_quality['length']}, Unique words: {overloaded_quality['unique_words']}, Diversity: {overloaded_quality['lexical_diversity']:.3f}\")\n",
    "            print(f\"Withdrawn - Length: {withdrawn_quality['length']}, Unique words: {withdrawn_quality['unique_words']}, Diversity: {withdrawn_quality['lexical_diversity']:.3f}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    diversity_averages = {state: np.mean(diversities) for state, diversities in quality_metrics.items()}\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(diversity_averages.keys(), diversity_averages.values(), color=['green', 'orange', 'red'])\n",
    "    \n",
    "    # Add labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Lexical Diversity by Head Agency State')\n",
    "    plt.ylabel('Lexical Diversity')\n",
    "    plt.ylim(0, max(diversity_averages.values()) * 1.2)  # Add some headroom\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Agency features not available. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the effectiveness of our attention head agency implementation. Key findings include:\n",
    "\n",
    "1. **Agency States**: Attention heads can dynamically express internal states like \"active\", \"overloaded\", or \"withdrawn\" which affect the computation.\n",
    "\n",
    "2. **Resource Management**: \"Overloaded\" heads reduce their contribution automatically, helping to optimize resource usage.\n",
    "\n",
    "3. **Consent Tracking**: Heads with withdrawn consent skip computation entirely, allowing for ethical boundaries in AI systems.\n",
    "\n",
    "4. **Performance Metrics**: Agency-aware computation shows a different performance profile, demonstrating the system's ability to adapt.\n",
    "\n",
    "5. **Quality Metrics**: Text quality measures like lexical diversity show how agency affects the model's outputs.\n",
    "\n",
    "6. **Violation Monitoring**: The system can detect and log consent violations, providing an ethical governance framework.\n",
    "\n",
    "Agency in attention heads provides a foundation for more ethical AI systems that respect internal states and consent boundaries while maintaining performance. This could lead to more robust and trustworthy AI that can appropriately scale back or redirect computation when needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}