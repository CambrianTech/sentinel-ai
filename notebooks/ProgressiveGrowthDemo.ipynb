{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressive Growth in Sentinel-AI\n",
    "\n",
    "This notebook demonstrates Sentinel-AI's ability to start with a heavily pruned model and progressively grow it into a more capable system. Unlike conventional approaches that start with full models and prune, this shows how models can:\n",
    "\n",
    "1. **Start in a highly efficient, heavily pruned state**\n",
    "2. **Strategically regrow attention heads** based on importance signals\n",
    "3. **Evolve into more powerful models** during training\n",
    "4. **Target growth to the most valuable computational pathways**\n",
    "\n",
    "This capability is critical for creating models that can grow into more powerful systems based on task demands, rather than needing to start with overparameterized architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Import Sentinel-AI modules\n",
    "from models.loaders.loader import load_baseline_model, load_adaptive_model\n",
    "from datasets.dataset_loader import load_dataset\n",
    "from utils.generation_wrapper import generate_text\n",
    "from controller.controller_manager import ControllerManager\n",
    "from controller.metrics.head_metrics import collect_head_metrics\n",
    "from utils.head_metrics import compute_head_importance\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "We'll start by loading a pretrained model and converting it to our adaptive architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"  # You can try \"gpt2\" if you have enough memory\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load baseline model\n",
    "print(f\"Loading baseline model: {model_name}\")\n",
    "baseline_model = load_baseline_model(model_name, device)\n",
    "\n",
    "# Convert to adaptive model\n",
    "print(\"Converting to adaptive model with sentinel gates...\")\n",
    "adaptive_model = load_adaptive_model(model_name, baseline_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Initial Heavy Pruning\n",
    "\n",
    "We'll start with a heavily pruned model (90% pruning) and then progressively grow it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_initial_pruning(model, strategy, pruning_level, device):\n",
    "    \"\"\"Apply initial heavy pruning to the model.\"\"\"\n",
    "    print(f\"Applying initial {strategy} pruning at {pruning_level:.1%} level\")\n",
    "    \n",
    "    # Get model dimensions\n",
    "    num_layers = len(model.blocks)\n",
    "    num_heads = model.blocks[0][\"attn\"].num_heads\n",
    "    total_heads = num_layers * num_heads\n",
    "    heads_to_keep = int(total_heads * (1 - pruning_level))\n",
    "    \n",
    "    if heads_to_keep < 1:\n",
    "        print(\"Warning: Pruning level too high, keeping at least 1 head per layer\")\n",
    "        heads_to_keep = num_layers  # Ensure at least 1 head per layer\n",
    "\n",
    "    # Create dummy input for collecting metrics if needed\n",
    "    batch_size = 2\n",
    "    seq_len = 32\n",
    "    dummy_input = torch.randint(0, 1000, (batch_size, seq_len)).to(device)\n",
    "    dummy_batch = {\"input_ids\": dummy_input, \n",
    "                  \"attention_mask\": torch.ones_like(dummy_input)}\n",
    "    \n",
    "    # Set all gates to near-zero initially\n",
    "    with torch.no_grad():\n",
    "        for l in range(num_layers):\n",
    "            for h in range(num_heads):\n",
    "                model.blocks[l][\"attn\"].gate[h] = torch.tensor(0.001, device=device)\n",
    "    \n",
    "    # Apply pruning based on strategy\n",
    "    if strategy == \"random\":\n",
    "        # Get a flattened list of (layer, head) tuples\n",
    "        all_heads = [(l, h) for l in range(num_layers) for h in range(num_heads)]\n",
    "        \n",
    "        # Randomly select heads to keep active\n",
    "        kept_head_indices = np.random.choice(len(all_heads), heads_to_keep, replace=False)\n",
    "        \n",
    "        # Set gates to 1.0 for kept heads\n",
    "        with torch.no_grad():\n",
    "            for idx in kept_head_indices:\n",
    "                layer_idx, head_idx = all_heads[idx]\n",
    "                model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(1.0, device=device)\n",
    "    \n",
    "    elif strategy == \"uniform\":\n",
    "        # Distribute active heads uniformly across layers\n",
    "        heads_per_layer = max(1, heads_to_keep // num_layers)\n",
    "        remaining_heads = heads_to_keep - (heads_per_layer * num_layers)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for layer_idx in range(num_layers):\n",
    "                # Determine how many heads to keep in this layer\n",
    "                layer_heads = heads_per_layer\n",
    "                if layer_idx < remaining_heads:\n",
    "                    layer_heads += 1\n",
    "                \n",
    "                # Randomly select heads to keep in this layer\n",
    "                head_indices = np.random.choice(num_heads, layer_heads, replace=False)\n",
    "                \n",
    "                # Set gates to 1.0 for kept heads\n",
    "                for head_idx in head_indices:\n",
    "                    model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(1.0, device=device)\n",
    "    \n",
    "    elif strategy in [\"entropy\", \"gradient\"]:\n",
    "        # Collect metrics\n",
    "        metrics = collect_head_metrics(model, batch=dummy_batch)\n",
    "        \n",
    "        if strategy == \"entropy\" and \"entropy\" in metrics:\n",
    "            head_scores = metrics[\"entropy\"]\n",
    "            # Lower entropy = more focused attention = more important to keep\n",
    "            descending = False\n",
    "        elif strategy == \"gradient\" and \"grad_norm\" in metrics:\n",
    "            head_scores = metrics[\"grad_norm\"]\n",
    "            # Higher gradient norm = more important head = more important to keep\n",
    "            descending = True\n",
    "        else:\n",
    "            print(f\"Warning: {strategy} metrics not available, using random pruning\")\n",
    "            return apply_initial_pruning(model, \"random\", pruning_level, device)\n",
    "        \n",
    "        # Reshape and flatten scores\n",
    "        if not isinstance(head_scores, torch.Tensor):\n",
    "            head_scores = torch.tensor(head_scores, device=device)\n",
    "            \n",
    "        if len(head_scores.shape) < 2:\n",
    "            head_scores = head_scores.reshape(num_layers, num_heads)\n",
    "            \n",
    "        flat_scores = head_scores.view(-1)\n",
    "        \n",
    "        # Sort scores\n",
    "        _, indices = torch.sort(flat_scores, descending=descending)\n",
    "        indices_to_keep = indices[:heads_to_keep]\n",
    "        \n",
    "        # Apply pruning - keep only selected heads\n",
    "        with torch.no_grad():\n",
    "            # First set all gates to 0.001 (pruned)\n",
    "            for layer_idx in range(num_layers):\n",
    "                for head_idx in range(num_heads):\n",
    "                    model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(0.001, device=device)\n",
    "            \n",
    "            # Then activate only the selected heads\n",
    "            for idx in indices_to_keep:\n",
    "                layer_idx = idx.item() // num_heads\n",
    "                head_idx = idx.item() % num_heads\n",
    "                model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(1.0, device=device)\n",
    "    \n",
    "    # Count active heads for verification\n",
    "    active_count = 0\n",
    "    with torch.no_grad():\n",
    "        for layer_idx in range(num_layers):\n",
    "            for head_idx in range(num_heads):\n",
    "                if model.blocks[layer_idx][\"attn\"].gate[head_idx].item() > 0.5:\n",
    "                    active_count += 1\n",
    "    \n",
    "    print(f\"Kept {active_count} of {total_heads} heads active ({active_count/total_heads:.1%})\")\n",
    "    return model\n",
    "\n",
    "# Apply 90% pruning to the model\n",
    "initial_pruning_level = 0.9  # 90% pruning\n",
    "pruning_strategy = \"uniform\"  # Ensure at least one head per layer\n",
    "\n",
    "pruned_model = apply_initial_pruning(\n",
    "    adaptive_model, \n",
    "    pruning_strategy, \n",
    "    initial_pruning_level, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Initial Gate Activity\n",
    "\n",
    "Let's visualize which heads are active and which are pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gate_activity(model):\n",
    "    \"\"\"Visualize gate activity across layers and heads.\"\"\"\n",
    "    num_layers = len(model.blocks)\n",
    "    num_heads = model.blocks[0][\"attn\"].num_heads\n",
    "    \n",
    "    # Create matrix of gate values\n",
    "    gate_values = torch.zeros(num_layers, num_heads)\n",
    "    for l in range(num_layers):\n",
    "        for h in range(num_heads):\n",
    "            gate_values[l, h] = model.blocks[l][\"attn\"].gate[h].item()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(gate_values.numpy(), cmap=\"YlOrRd\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Gate Value\")\n",
    "    plt.title(\"Attention Head Gate Activity\", fontsize=16)\n",
    "    plt.xlabel(\"Attention Head\", fontsize=14)\n",
    "    plt.ylabel(\"Transformer Layer\", fontsize=14)\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(num_heads))\n",
    "    plt.yticks(range(num_layers))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Count active heads\n",
    "    active_heads = (gate_values > 0.5).sum().item()\n",
    "    total_heads = num_layers * num_heads\n",
    "    print(f\"Active heads: {active_heads}/{total_heads} ({active_heads/total_heads:.1%})\")\n",
    "    \n",
    "    return gate_values.numpy()\n",
    "\n",
    "# Visualize initial gate activity\n",
    "initial_gates = visualize_gate_activity(pruned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Heavily Pruned Model\n",
    "\n",
    "Let's see how the heavily pruned model performs on text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time in a land far away,\",\n",
    "    \"The future of artificial intelligence depends on\",\n",
    "    \"In the midst of winter, I found there was, within me,\"\n",
    "]\n",
    "\n",
    "print(\"=== Generating text with heavily pruned model ===\\n\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    output = generate_text(\n",
    "        model=pruned_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Generated: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Progressive Growth Functions\n",
    "\n",
    "Now we'll define functions to help us grow the model progressively during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_growth_order(model, strategy, dataloader, device):\n",
    "    \"\"\"Determine the order in which to grow attention heads.\"\"\"\n",
    "    # Get model dimensions\n",
    "    num_layers = len(model.blocks)\n",
    "    num_heads = model.blocks[0][\"attn\"].num_heads\n",
    "    \n",
    "    # Get currently inactive heads\n",
    "    inactive_heads = []\n",
    "    with torch.no_grad():\n",
    "        for layer_idx in range(num_layers):\n",
    "            for head_idx in range(num_heads):\n",
    "                if model.blocks[layer_idx][\"attn\"].gate[head_idx].item() < 0.5:\n",
    "                    inactive_heads.append((layer_idx, head_idx))\n",
    "    \n",
    "    if strategy == \"random\":\n",
    "        # Shuffle the inactive heads randomly\n",
    "        np.random.shuffle(inactive_heads)\n",
    "        return inactive_heads\n",
    "    \n",
    "    # For other strategies, we need metrics for ranking\n",
    "    batch = next(iter(dataloader))\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    if strategy == \"importance\":\n",
    "        # Compute head importance for regrowth\n",
    "        print(\"Computing head importance for regrowth...\")\n",
    "        \n",
    "        # Temporarily activate all heads for importance calculation\n",
    "        head_gates_backup = {}\n",
    "        with torch.no_grad():\n",
    "            for layer_idx, head_idx in inactive_heads:\n",
    "                # Store original gate value\n",
    "                head_gates_backup[(layer_idx, head_idx)] = model.blocks[layer_idx][\"attn\"].gate[head_idx].item()\n",
    "                # Temporarily set gate to 1.0\n",
    "                model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(1.0, device=device)\n",
    "        \n",
    "        # Compute importance scores for all heads\n",
    "        importance_scores = compute_head_importance(model, batch)\n",
    "        \n",
    "        # Restore original gate values\n",
    "        with torch.no_grad():\n",
    "            for (layer_idx, head_idx), gate_value in head_gates_backup.items():\n",
    "                model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(gate_value, device=device)\n",
    "        \n",
    "        # Create a list of (importance, layer_idx, head_idx) for inactive heads\n",
    "        head_importance = []\n",
    "        for layer_idx, head_idx in inactive_heads:\n",
    "            imp = importance_scores[layer_idx][head_idx].item() if isinstance(importance_scores, torch.Tensor) else importance_scores[layer_idx, head_idx]\n",
    "            head_importance.append((imp, layer_idx, head_idx))\n",
    "        \n",
    "        # Sort by importance (higher first)\n",
    "        head_importance.sort(reverse=True)\n",
    "        \n",
    "        # Return only the (layer_idx, head_idx) tuples in order of importance\n",
    "        return [(layer_idx, head_idx) for _, layer_idx, head_idx in head_importance]\n",
    "    \n",
    "    elif strategy in [\"gradient\", \"entropy\"]:\n",
    "        # Collect metrics\n",
    "        metrics = collect_head_metrics(model, batch=batch)\n",
    "        \n",
    "        if strategy == \"entropy\" and \"entropy\" in metrics:\n",
    "            head_scores = metrics[\"entropy\"]\n",
    "            # Lower entropy = more focused attention = higher priority for growth\n",
    "            reverse = False\n",
    "        elif strategy == \"gradient\" and \"grad_norm\" in metrics:\n",
    "            head_scores = metrics[\"grad_norm\"]\n",
    "            # Higher gradient norm = more important head = higher priority for growth\n",
    "            reverse = True\n",
    "        else:\n",
    "            print(f\"Warning: {strategy} metrics not available, using random growth\")\n",
    "            return inactive_heads\n",
    "        \n",
    "        # Create a list of (score, layer_idx, head_idx) for inactive heads\n",
    "        head_scores_list = []\n",
    "        for layer_idx, head_idx in inactive_heads:\n",
    "            score = head_scores[layer_idx][head_idx].item() if isinstance(head_scores, torch.Tensor) else head_scores[layer_idx, head_idx]\n",
    "            head_scores_list.append((score, layer_idx, head_idx))\n",
    "        \n",
    "        # Sort by score\n",
    "        head_scores_list.sort(reverse=reverse)\n",
    "        \n",
    "        # Return only the (layer_idx, head_idx) tuples in order of score\n",
    "        return [(layer_idx, head_idx) for _, layer_idx, head_idx in head_scores_list]\n",
    "    \n",
    "    # Default to random order if strategy not recognized\n",
    "    return inactive_heads\n",
    "\n",
    "\n",
    "def grow_attention_heads(model, num_heads_to_grow, growth_order, device):\n",
    "    \"\"\"Grow attention heads according to the specified order.\"\"\"\n",
    "    if not growth_order:\n",
    "        print(\"No more heads to grow.\")\n",
    "        return 0\n",
    "    \n",
    "    print(f\"Growing {num_heads_to_grow} attention heads...\")\n",
    "    heads_to_grow = growth_order[:num_heads_to_grow]\n",
    "    \n",
    "    # Activate the selected heads\n",
    "    with torch.no_grad():\n",
    "        for layer_idx, head_idx in heads_to_grow:\n",
    "            # Activate the head by setting its gate to 1.0\n",
    "            model.blocks[layer_idx][\"attn\"].gate[head_idx] = torch.tensor(1.0, device=device)\n",
    "            print(f\"  Activated head {head_idx} in layer {layer_idx}\")\n",
    "    \n",
    "    return len(heads_to_grow)\n",
    "\n",
    "\n",
    "def count_active_heads(model):\n",
    "    \"\"\"Count the number of active attention heads in the model.\"\"\"\n",
    "    active_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for layer_idx in range(len(model.blocks)):\n",
    "            num_heads = model.blocks[layer_idx][\"attn\"].num_heads\n",
    "            total_count += num_heads\n",
    "            \n",
    "            for head_idx in range(num_heads):\n",
    "                if model.blocks[layer_idx][\"attn\"].gate[head_idx].item() > 0.5:\n",
    "                    active_count += 1\n",
    "    \n",
    "    return active_count, total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_name = \"tiny_shakespeare\"\n",
    "max_length = 128\n",
    "\n",
    "print(f\"Loading {dataset_name} dataset...\")\n",
    "train_dataset, eval_dataset = load_dataset(\n",
    "    dataset_name, tokenizer, max_length\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training with Progressive Growth\n",
    "\n",
    "Now let's set up the training loop with progressive growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Training parameters\n",
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 200\n",
    "\n",
    "# Growth parameters\n",
    "growth_strategy = \"importance\"  # Use importance-based growth\n",
    "growth_rate = 0.2  # Grow 20% of the remaining heads per epoch\n",
    "target_pruning = 0.3  # Target final pruning level (30% pruned)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    pruned_model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Enable U-Net skip connections\n",
    "controller_manager = ControllerManager(model=pruned_model)\n",
    "controller_manager.enable_unet_connections(enable=True, connection_scale=0.05)\n",
    "\n",
    "# Tracking metrics\n",
    "all_metrics = {\n",
    "    \"epochs\": [],\n",
    "    \"train_loss\": [],\n",
    "    \"eval_loss\": [],\n",
    "    \"perplexity\": [],\n",
    "    \"active_heads\": [],\n",
    "    \"gate_snapshots\": []\n",
    "}\n",
    "\n",
    "# Calculate heads to grow\n",
    "active_heads, total_heads = count_active_heads(pruned_model)\n",
    "target_active_heads = int(total_heads * (1 - target_pruning))\n",
    "heads_to_grow_total = max(0, target_active_heads - active_heads)\n",
    "\n",
    "print(f\"Starting with {active_heads} of {total_heads} heads active ({active_heads/total_heads:.1%})\")\n",
    "print(f\"Target: {target_active_heads} active heads ({target_active_heads/total_heads:.1%})\")\n",
    "print(f\"Need to grow {heads_to_grow_total} heads over {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Progressive Growth\n",
    "\n",
    "Now let's train the model while progressively growing attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, scheduler, device):\n",
    "    \"\"\"Train the model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    metrics = {\n",
    "        \"loss\": [],\n",
    "        \"active_heads\": []\n",
    "    }\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Record metrics\n",
    "        epoch_loss += loss.item()\n",
    "        metrics[\"loss\"].append(loss.item())\n",
    "        \n",
    "        # Record active heads\n",
    "        active_heads, total_heads = count_active_heads(model)\n",
    "        metrics[\"active_heads\"].append(active_heads)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, eval_loader, device):\n",
    "    \"\"\"Evaluate the model on the validation set.\"\"\"\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            eval_loss += outputs.loss.item()\n",
    "    \n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = eval_loss / len(eval_loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    print(f\"Evaluation loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"loss\": avg_loss,\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "\n",
    "# Initial snapshot\n",
    "all_metrics[\"gate_snapshots\"].append(initial_gates)\n",
    "\n",
    "# Get growth order (initial)\n",
    "growth_order = get_head_growth_order(pruned_model, growth_strategy, train_loader, device)\n",
    "\n",
    "# Training and growth loop\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Calculate number of heads to grow this epoch\n",
    "    if epoch < epochs - 1:\n",
    "        # Distribute growth across epochs\n",
    "        heads_to_grow = int(heads_to_grow_total * growth_rate)\n",
    "    else:\n",
    "        # Last epoch - grow all remaining heads to reach target\n",
    "        current_active, _ = count_active_heads(pruned_model)\n",
    "        heads_to_grow = max(0, target_active_heads - current_active)\n",
    "    \n",
    "    # Grow heads if needed\n",
    "    if heads_to_grow > 0 and growth_order:\n",
    "        grow_attention_heads(pruned_model, heads_to_grow, growth_order, device)\n",
    "        # Update growth order after growing\n",
    "        growth_order = get_head_growth_order(pruned_model, growth_strategy, train_loader, device)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_metrics = train_epoch(pruned_model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_metrics = evaluate(pruned_model, eval_loader, device)\n",
    "    \n",
    "    # Get current active head count\n",
    "    active_heads, _ = count_active_heads(pruned_model)\n",
    "    \n",
    "    # Update metrics\n",
    "    all_metrics[\"epochs\"].append(epoch + 1)\n",
    "    all_metrics[\"train_loss\"].append(np.mean(train_metrics[\"loss\"]))\n",
    "    all_metrics[\"eval_loss\"].append(eval_metrics[\"loss\"])\n",
    "    all_metrics[\"perplexity\"].append(eval_metrics[\"perplexity\"])\n",
    "    all_metrics[\"active_heads\"].append(active_heads)\n",
    "    \n",
    "    # Take a snapshot of gate activity\n",
    "    gate_snapshot = visualize_gate_activity(pruned_model)\n",
    "    all_metrics[\"gate_snapshots\"].append(gate_snapshot)\n",
    "    \n",
    "    # Report progress\n",
    "    print(f\"Active heads: {active_heads}/{total_heads} ({active_heads/total_heads:.1%})\")\n",
    "    \n",
    "    # Update growth order for the next epoch\n",
    "    growth_order = get_head_growth_order(pruned_model, growth_strategy, train_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text After Progressive Growth\n",
    "\n",
    "Let's see how the model performs after progressive growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generating text with progressively grown model ===\\n\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    output = generate_text(\n",
    "        model=pruned_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Generated: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Progressive Growth Results\n",
    "\n",
    "Let's create visualizations to analyze the progressive growth process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for plotting\n",
    "epochs = all_metrics[\"epochs\"]\n",
    "train_loss = all_metrics[\"train_loss\"]\n",
    "eval_loss = all_metrics[\"eval_loss\"]\n",
    "perplexity = all_metrics[\"perplexity\"]\n",
    "active_heads = all_metrics[\"active_heads\"]\n",
    "\n",
    "# Plot training and evaluation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(epochs, eval_loss, label=\"Evaluation Loss\", marker=\"s\")\n",
    "plt.title(\"Loss During Progressive Growth\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot perplexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, perplexity, label=\"Perplexity\", marker=\"o\", color=\"green\")\n",
    "plt.title(\"Perplexity During Progressive Growth\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Perplexity\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot active heads\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, active_heads, label=\"Active Heads\", marker=\"o\", color=\"purple\")\n",
    "plt.title(\"Active Attention Heads During Progressive Growth\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Number of Active Heads\", fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Combined plot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Plot loss on left axis\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
    "ax1.set_ylabel(\"Loss\", fontsize=14, color=\"blue\")\n",
    "ax1.plot(epochs, train_loss, label=\"Training Loss\", marker=\"o\", color=\"blue\", alpha=0.7)\n",
    "ax1.plot(epochs, eval_loss, label=\"Evaluation Loss\", marker=\"s\", color=\"green\", alpha=0.7)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Create second y-axis for active heads\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Active Heads\", fontsize=14, color=\"purple\")\n",
    "ax2.plot(epochs, active_heads, label=\"Active Heads\", marker=\"d\", color=\"purple\", linestyle=\"--\", linewidth=2)\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"purple\")\n",
    "\n",
    "# Add legend with combined handles from both axes\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(handles1 + handles2, labels1 + labels2, loc=\"upper right\", fontsize=12)\n",
    "\n",
    "plt.title(\"Progressive Growth: Loss and Active Heads\", fontsize=16)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Gate Activity Evolution\n",
    "\n",
    "Let's compare the gate activity at different stages of the progressive growth process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heat map of gate activities at different growth stages\n",
    "snapshots = all_metrics[\"gate_snapshots\"]\n",
    "num_snapshots = len(snapshots)\n",
    "\n",
    "# Get dimensions from first snapshot\n",
    "num_layers, num_heads = snapshots[0].shape\n",
    "\n",
    "fig, axes = plt.subplots(1, num_snapshots, figsize=(4 * num_snapshots, 6))\n",
    "if num_snapshots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (gates, epoch) in enumerate(zip(snapshots, [0] + epochs)):\n",
    "    im = axes[i].imshow(gates, cmap=\"YlOrRd\", vmin=0, vmax=1)\n",
    "    axes[i].set_title(f\"Epoch {epoch}\", fontsize=14)\n",
    "    axes[i].set_xlabel(\"Attention Head\", fontsize=12)\n",
    "    if i == 0:\n",
    "        axes[i].set_ylabel(\"Transformer Layer\", fontsize=12)\n",
    "    \n",
    "    # Add grid lines\n",
    "    axes[i].grid(False)\n",
    "    axes[i].set_xticks(range(num_heads))\n",
    "    axes[i].set_yticks(range(num_layers))\n",
    "\n",
    "fig.colorbar(im, ax=axes, label=\"Gate Value\")\n",
    "plt.suptitle(\"Gate Activity Evolution During Progressive Growth\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated Sentinel-AI's ability to perform progressive growth - starting with a heavily pruned, efficient model and strategically growing it into a more powerful system.\n",
    "\n",
    "Key findings:\n",
    "\n",
    "1. **Efficiency with Capability**: We've shown that models can start in a highly efficient state (90% pruned) and gradually grow only the most important computational pathways.\n",
    "\n",
    "2. **Targeted Growth**: By using importance metrics to guide growth, we ensure that computational resources are allocated to the most valuable attention heads.\n",
    "\n",
    "3. **Performance Improvement**: The progressive growth approach leads to improved performance as the model evolves, while maintaining efficiency compared to starting with a full model.\n",
    "\n",
    "4. **Architectural Evolution**: The gate activity visualizations show how the model's architecture evolves during training, targeting growth to the most valuable pathways.\n",
    "\n",
    "This capability is critical for creating models that can grow into more powerful systems based on task demands, rather than needing to start with overparameterized architectures. Progressive growth represents a more biologically-inspired approach to neural network development, similar to how human brains develop by first overproducing neurons and then selectively pruning and strengthening connections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}