{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning Benchmark Notebook\n",
    "\n",
    "This notebook implements a stable pruning benchmark that works on both Google Colab and local machines (including M1/M2 Macs).\n",
    "\n",
    "Note: This implementation includes special fixes for Apple Silicon Macs that may experience BLAS crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Set environment variables to avoid BLAS crashes on Mac\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Further limit PyTorch's threading\n",
    "torch.set_num_threads(1)\n",
    "if hasattr(torch, 'set_num_interop_threads'):\n",
    "    torch.set_num_interop_threads(1)\n",
    "\n",
    "# Import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Strategies\n",
    "\n",
    "We'll implement two pruning strategies:\n",
    "1. Random - Prune random heads\n",
    "2. Entropy - Prune heads based on their importance, measured as weight magnitude\n",
    "\n",
    "Both strategies are designed for stability especially on M1/M2 Macs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PruningStrategy:\n",
    "    \"\"\"Base class for pruning strategies\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def get_head_importance(self, layer_idx):\n",
    "        \"\"\"Get importance scores for all heads in a layer\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement get_head_importance\")\n",
    "    \n",
    "    def prune_heads(self, layer_idx, head_idxs):\n",
    "        \"\"\"Prune specific heads in a layer\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement prune_heads\")\n",
    "\n",
    "class RandomPruningStrategy(PruningStrategy):\n",
    "    \"\"\"Random pruning strategy\"\"\"\n",
    "    def get_head_importance(self, layer_idx):\n",
    "        attn = self.model.transformer.h[layer_idx].attn\n",
    "        num_heads = attn.num_heads\n",
    "        # Generate random importance scores\n",
    "        return np.random.rand(num_heads)\n",
    "    \n",
    "    def prune_heads(self, layer_idx, head_idxs):\n",
    "        print(f\"Pruning layer {layer_idx}, heads {head_idxs} using Random strategy\")\n",
    "        self._zero_out_heads(layer_idx, head_idxs)\n",
    "        return self.model\n",
    "    \n",
    "    def _zero_out_heads(self, layer_idx, head_idxs):\n",
    "        \"\"\"Zero out the output projection for specific heads\"\"\"\n",
    "        attn = self.model.transformer.h[layer_idx].attn\n",
    "        hidden_size = attn.embed_dim\n",
    "        num_heads = attn.num_heads\n",
    "        head_size = hidden_size // num_heads\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for head_idx in head_idxs:\n",
    "                # Calculate start and end indices for this head\n",
    "                start_idx = head_idx * head_size\n",
    "                end_idx = (head_idx + 1) * head_size\n",
    "                \n",
    "                # Zero out the output projection weights for this head\n",
    "                attn.c_proj.weight[:, start_idx:end_idx] = 0\n",
    "                \n",
    "                # If there's a bias, zero it out too\n",
    "                if hasattr(attn.c_proj, 'bias') and attn.c_proj.bias is not None:\n",
    "                    attn.c_proj.bias[start_idx:end_idx] = 0\n",
    "        \n",
    "        print(f\"Successfully pruned heads {head_idxs} in layer {layer_idx}\")\n",
    "\n",
    "class EntropyPruningStrategy(PruningStrategy):\n",
    "    \"\"\"Entropy-based pruning strategy\"\"\"\n",
    "    def __init__(self, model, tokenizer, sample_text=None):\n",
    "        super().__init__(model)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Use default sample text if none provided\n",
    "        if sample_text is None:\n",
    "            self.sample_text = [\n",
    "                \"The quick brown fox jumps over the lazy dog\",\n",
    "                \"Artificial intelligence is transforming the world\",\n",
    "                \"Machine learning models can process large amounts of data\"\n",
    "            ]\n",
    "        else:\n",
    "            self.sample_text = sample_text\n",
    "    \n",
    "    def get_head_importance(self, layer_idx):\n",
    "        \"\"\"Calculate entropy-based importance for each head\"\"\"\n",
    "        print(f\"Calculating entropy-based importance for layer {layer_idx}...\")\n",
    "        \n",
    "        # This is a simplified proxy for entropy-based importance\n",
    "        # In a real implementation, we would calculate attention entropy\n",
    "        # For stability on M1/M2 Mac, we're using a simpler approach\n",
    "        attn = self.model.transformer.h[layer_idx].attn\n",
    "        num_heads = attn.num_heads\n",
    "        head_importance = np.zeros(num_heads)\n",
    "        \n",
    "        for head_idx in range(num_heads):\n",
    "            # To avoid complex BLAS operations, we'll estimate importance\n",
    "            # based on the magnitude of output projection weights\n",
    "            head_dim = attn.head_dim\n",
    "            start_idx = head_idx * head_dim\n",
    "            end_idx = (head_idx + 1) * head_dim\n",
    "            \n",
    "            # Get L2 norm of weights as a proxy for importance\n",
    "            weight_norm = torch.norm(attn.c_proj.weight[:, start_idx:end_idx]).item()\n",
    "            head_importance[head_idx] = weight_norm\n",
    "        \n",
    "        # Normalize importance scores\n",
    "        if np.sum(head_importance) > 0:\n",
    "            head_importance = head_importance / np.sum(head_importance)\n",
    "        \n",
    "        return head_importance\n",
    "    \n",
    "    def prune_heads(self, layer_idx, head_idxs):\n",
    "        print(f\"Pruning layer {layer_idx}, heads {head_idxs} using Entropy strategy\")\n",
    "        self._zero_out_heads(layer_idx, head_idxs)\n",
    "        return self.model\n",
    "    \n",
    "    def _zero_out_heads(self, layer_idx, head_idxs):\n",
    "        \"\"\"Same implementation as RandomPruningStrategy\"\"\"\n",
    "        attn = self.model.transformer.h[layer_idx].attn\n",
    "        hidden_size = attn.embed_dim\n",
    "        num_heads = attn.num_heads\n",
    "        head_size = hidden_size // num_heads\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for head_idx in head_idxs:\n",
    "                # Calculate start and end indices for this head\n",
    "                start_idx = head_idx * head_size\n",
    "                end_idx = (head_idx + 1) * head_size\n",
    "                \n",
    "                # Zero out the output projection weights for this head\n",
    "                attn.c_proj.weight[:, start_idx:end_idx] = 0\n",
    "                \n",
    "                # If there's a bias, zero it out too\n",
    "                if hasattr(attn.c_proj, 'bias') and attn.c_proj.bias is not None:\n",
    "                    attn.c_proj.bias[start_idx:end_idx] = 0\n",
    "        \n",
    "        print(f\"Successfully pruned heads {head_idxs} in layer {layer_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions\n",
    "\n",
    "Now we'll define functions to evaluate model performance before and after pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_perplexity(model, tokenizer, text, device=\"cpu\"):\n",
    "    \"\"\"Evaluate model perplexity on a text sample\"\"\"\n",
    "    # Encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Create a mask to avoid padding tokens\n",
    "    attention_mask = torch.ones(inputs[\"input_ids\"].shape, device=device)\n",
    "    \n",
    "    # Evaluate perplexity\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"], \n",
    "            attention_mask=attention_mask,\n",
    "            labels=inputs[\"input_ids\"]\n",
    "        )\n",
    "        \n",
    "    return torch.exp(outputs.loss).item()\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, device=\"cpu\"):\n",
    "    \"\"\"Generate text using the model\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    attention_mask = torch.ones(inputs[\"input_ids\"].shape, device=device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def save_results(strategy_name, pruning_level, results):\n",
    "    \"\"\"Save results to a JSON file\"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    results_dir = \"pruning_results\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f\"{strategy_name}_{pruning_level}_{timestamp}.json\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Save results\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filepath}\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Pruning Benchmark\n",
    "\n",
    "Now we'll implement the main function to run pruning benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_pruning_benchmark(model_name=\"distilgpt2\", strategy_name=\"random\", \n",
    "                         pruning_level=0.3, prompt=\"Artificial intelligence is\",\n",
    "                         device=\"cpu\"):\n",
    "    \"\"\"Run pruning benchmark with specified parameters\"\"\"\n",
    "    print(f\"Running pruning benchmark with:\\n\" + \n",
    "          f\"  Model: {model_name}\\n\" +\n",
    "          f\"  Strategy: {strategy_name}\\n\" +\n",
    "          f\"  Pruning level: {pruning_level}\\n\" +\n",
    "          f\"  Device: {device}\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"\\nLoading {model_name}...\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Display model information\n",
    "    num_layers = len(model.transformer.h)\n",
    "    num_heads = model.transformer.h[0].attn.num_heads\n",
    "    print(f\"Model loaded: {model_name}\")\n",
    "    print(f\"Layers: {num_layers}, Heads per layer: {num_heads}\")\n",
    "    \n",
    "    # Create pruning strategy\n",
    "    if strategy_name.lower() == \"random\":\n",
    "        strategy = RandomPruningStrategy(model)\n",
    "    elif strategy_name.lower() == \"entropy\":\n",
    "        strategy = EntropyPruningStrategy(model, tokenizer)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pruning strategy: {strategy_name}\")\n",
    "    \n",
    "    # Calculate number of heads to prune\n",
    "    total_heads = num_layers * num_heads\n",
    "    heads_to_prune = int(total_heads * pruning_level)\n",
    "    print(f\"Pruning level: {pruning_level} ({heads_to_prune} out of {total_heads} heads)\")\n",
    "    \n",
    "    # Evaluate model before pruning\n",
    "    print(\"\\nEvaluating model before pruning...\")\n",
    "    \n",
    "    # Generate text\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    generated_before = generate_text(model, tokenizer, prompt, device=device)\n",
    "    print(f\"Generated (before pruning): {generated_before}\")\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity_before = evaluate_perplexity(model, tokenizer, prompt, device=device)\n",
    "    print(f\"Perplexity (before pruning): {perplexity_before:.4f}\")\n",
    "    \n",
    "    # Perform pruning\n",
    "    print(\"\\nPerforming pruning...\")\n",
    "    \n",
    "    # Get importance scores for all heads\n",
    "    all_head_importance = []\n",
    "    for layer_idx in range(num_layers):\n",
    "        importance = strategy.get_head_importance(layer_idx)\n",
    "        for head_idx, score in enumerate(importance):\n",
    "            all_head_importance.append((layer_idx, head_idx, score))\n",
    "    \n",
    "    # Sort heads by importance (ascending)\n",
    "    all_head_importance.sort(key=lambda x: x[2])\n",
    "    \n",
    "    # Prune least important heads\n",
    "    pruned_heads = all_head_importance[:heads_to_prune]\n",
    "    \n",
    "    # Group by layer for efficient pruning\n",
    "    pruned_by_layer = {}\n",
    "    for layer_idx, head_idx, _ in pruned_heads:\n",
    "        if layer_idx not in pruned_by_layer:\n",
    "            pruned_by_layer[layer_idx] = []\n",
    "        pruned_by_layer[layer_idx].append(head_idx)\n",
    "    \n",
    "    # Prune heads layer by layer\n",
    "    for layer_idx, head_idxs in pruned_by_layer.items():\n",
    "        strategy.prune_heads(layer_idx, head_idxs)\n",
    "    \n",
    "    # Evaluate model after pruning\n",
    "    print(\"\\nEvaluating model after pruning...\")\n",
    "    \n",
    "    # Generate text\n",
    "    generated_after = generate_text(model, tokenizer, prompt, device=device)\n",
    "    print(f\"Generated (after pruning): {generated_after}\")\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    perplexity_after = evaluate_perplexity(model, tokenizer, prompt, device=device)\n",
    "    print(f\"Perplexity (after pruning): {perplexity_after:.4f}\")\n",
    "    print(f\"Perplexity change: {perplexity_after - perplexity_before:.4f}\")\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        \"model\": model_name,\n",
    "        \"strategy\": strategy_name,\n",
    "        \"pruning_level\": pruning_level,\n",
    "        \"total_heads\": total_heads,\n",
    "        \"pruned_heads\": heads_to_prune,\n",
    "        \"pruned_head_details\": [(l, h) for l, h, _ in pruned_heads],\n",
    "        \"prompt\": prompt,\n",
    "        \"generated_before\": generated_before,\n",
    "        \"generated_after\": generated_after,\n",
    "        \"perplexity_before\": perplexity_before,\n",
    "        \"perplexity_after\": perplexity_after,\n",
    "        \"perplexity_change\": perplexity_after - perplexity_before,\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "    \n",
    "    # Save results\n",
    "    save_results(strategy_name, pruning_level, results)\n",
    "    \n",
    "    print(\"\\nPruning benchmark completed successfully!\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Single Benchmark Test\n",
    "\n",
    "Let's run a single benchmark test to make sure everything is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a single test with minimal pruning\n",
    "results = run_pruning_benchmark(\n",
    "    model_name=\"distilgpt2\",\n",
    "    strategy_name=\"random\",\n",
    "    pruning_level=0.1,\n",
    "    prompt=\"Artificial intelligence is\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results\n",
    "\n",
    "This function loads and displays results from previous benchmark runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def view_results(results_dir=\"pruning_results\"):\n",
    "    \"\"\"Load and display results from previous benchmark runs\"\"\"\n",
    "    # Create results directory if it doesn't exist\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Find all result files\n",
    "    result_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(f\"No result files found in {results_dir}\")\n",
    "        return []\n",
    "    \n",
    "    # Load results\n",
    "    all_results = []\n",
    "    \n",
    "    for filepath in result_files:\n",
    "        try:\n",
    "            # Extract strategy and pruning level from filename\n",
    "            filename = os.path.basename(filepath)\n",
    "            \n",
    "            # Try to match both formats:\n",
    "            # 1. New format: \"strategy_level_timestamp.json\"\n",
    "            # 2. Old format: \"strategy_pruning_level_results.json\"\n",
    "            match1 = re.match(r\"(\\w+)_(\\d+\\.\\d+)_\\d+\\.json\", filename)\n",
    "            match2 = re.match(r\"(\\w+)_pruning_(\\d+\\.\\d+)_results\\.json\", filename)\n",
    "            \n",
    "            if match1:\n",
    "                # New format\n",
    "                strategy = match1.group(1)\n",
    "                pruning_level = float(match1.group(2))\n",
    "            elif match2:\n",
    "                # Old format\n",
    "                strategy = match2.group(1)\n",
    "                pruning_level = float(match2.group(2))\n",
    "            else:\n",
    "                print(f\"Warning: Couldn't parse filename {filename}, skipping...\")\n",
    "                continue\n",
    "                \n",
    "            # Load the results file\n",
    "            with open(filepath, \"r\") as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            # Add strategy and pruning level if not in results\n",
    "            if \"strategy\" not in results:\n",
    "                results[\"strategy\"] = strategy\n",
    "            if \"pruning_level\" not in results:\n",
    "                results[\"pruning_level\"] = pruning_level\n",
    "                \n",
    "            all_results.append(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "    \n",
    "    # Sort results by strategy and pruning level\n",
    "    all_results.sort(key=lambda x: (x[\"strategy\"], x[\"pruning_level\"]))\n",
    "    \n",
    "    # Group results by strategy\n",
    "    strategies = set(r[\"strategy\"] for r in all_results)\n",
    "    grouped_results = {strategy: [] for strategy in strategies}\n",
    "    \n",
    "    for result in all_results:\n",
    "        grouped_results[result[\"strategy\"]].append(result)\n",
    "    \n",
    "    # Display summary\n",
    "    print(f\"Found {len(all_results)} result files:\\n\")\n",
    "    \n",
    "    for strategy, results in grouped_results.items():\n",
    "        print(f\"Strategy: {strategy}\")\n",
    "        for result in results:\n",
    "            perplexity_change = result.get(\"perplexity_change\", \"N/A\")\n",
    "            if isinstance(perplexity_change, (int, float)):\n",
    "                perplexity_change = f\"{perplexity_change:.4f}\"\n",
    "            \n",
    "            print(f\"  Pruning level: {result['pruning_level']}, \" + \n",
    "                  f\"Perplexity change: {perplexity_change}\")\n",
    "        print()\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Load and display results\n",
    "all_results = view_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results\n",
    "\n",
    "Let's visualize the results by plotting perplexity change vs pruning level for each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_results(all_results):\n",
    "    \"\"\"Plot perplexity change vs pruning level for each strategy\"\"\"\n",
    "    if not all_results:\n",
    "        print(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Group results by strategy\n",
    "    strategies = set(r[\"strategy\"] for r in all_results)\n",
    "    grouped_results = {strategy: [] for strategy in strategies}\n",
    "    \n",
    "    for result in all_results:\n",
    "        # Only include results with perplexity change\n",
    "        if \"perplexity_change\" in result and isinstance(result[\"perplexity_change\"], (int, float)):\n",
    "            grouped_results[result[\"strategy\"]].append(result)\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\"]\n",
    "    markers = [\"o\", \"s\", \"^\", \"d\", \"x\"]\n",
    "    \n",
    "    for i, (strategy, results) in enumerate(grouped_results.items()):\n",
    "        if not results:\n",
    "            continue\n",
    "            \n",
    "        # Sort by pruning level\n",
    "        results.sort(key=lambda x: x[\"pruning_level\"])\n",
    "        \n",
    "        # Extract data for plotting\n",
    "        pruning_levels = [r[\"pruning_level\"] for r in results]\n",
    "        perplexity_changes = [r[\"perplexity_change\"] for r in results]\n",
    "        \n",
    "        # Plot\n",
    "        color = colors[i % len(colors)]\n",
    "        marker = markers[i % len(markers)]\n",
    "        plt.plot(pruning_levels, perplexity_changes, marker=marker, color=color,\n",
    "                linestyle=\"-\", label=strategy.capitalize())\n",
    "    \n",
    "    plt.xlabel(\"Pruning Level\")\n",
    "    plt.ylabel(\"Perplexity Change\")\n",
    "    plt.title(\"Effect of Pruning on Model Perplexity\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add horizontal line at y=0\n",
    "    plt.axhline(y=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results if we have any\n",
    "if all_results:\n",
    "    plot_results(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Multiple Benchmark Tests\n",
    "\n",
    "Run multiple benchmarks with different pruning levels and strategies. This can be run overnight on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_multiple_benchmarks(model_name=\"distilgpt2\", strategies=[\"random\", \"entropy\"],\n",
    "                           pruning_levels=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                           prompt=\"Artificial intelligence is\",\n",
    "                           device=\"cpu\"):\n",
    "    \"\"\"Run multiple benchmarks with different parameters\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for level in pruning_levels:\n",
    "            print(f\"\\n{'='*50}\\nRunning benchmark: {strategy}, level: {level}\\n{'='*50}\\n\")\n",
    "            \n",
    "            try:\n",
    "                result = run_pruning_benchmark(\n",
    "                    model_name=model_name,\n",
    "                    strategy_name=strategy,\n",
    "                    pruning_level=level,\n",
    "                    prompt=prompt,\n",
    "                    device=device\n",
    "                )\n",
    "                all_results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in benchmark {strategy}, level {level}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nCompleted {len(all_results)} benchmarks out of {len(strategies) * len(pruning_levels)} attempted\")\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(all_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "# Uncomment to run multiple benchmarks\n",
    "# Note: This can take a long time, especially for higher pruning levels\n",
    "# all_results = run_multiple_benchmarks(\n",
    "#     model_name=\"distilgpt2\",\n",
    "#     strategies=[\"random\", \"entropy\"],\n",
    "#     pruning_levels=[0.1, 0.3, 0.5],\n",
    "#     device=device\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}