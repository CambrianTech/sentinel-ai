{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer and Pruning Profiling Analysis\n",
    "\n",
    "This notebook analyzes and visualizes the results from profiling runs of both the optimization-integrated model and the pure pruning benchmark. It helps compare different optimization levels and pruning approaches to understand their impact on performance and efficiency.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Load and compare profiling results from multiple runs\n",
    "- Visualize performance across different pruning levels\n",
    "- Compare original vs. optimized model implementations\n",
    "- Create interactive charts for key metrics\n",
    "- Analyze component-level performance breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Profiling Results\n",
    "\n",
    "First, let's load the profiling results from both the pure pruning benchmark and the optimization profiling runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load profiling results\n",
    "def load_profiling_results(directory=\"profiling_results\"):\n",
    "    \"\"\"Load profiling results from the given directory.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found\")\n",
    "        return results\n",
    "    \n",
    "    # Load full model profiling results\n",
    "    full_model_path = os.path.join(directory, \"full_model\", \"full_model_profiling.json\")\n",
    "    if os.path.exists(full_model_path):\n",
    "        with open(full_model_path, \"r\") as f:\n",
    "            results[\"full_model\"] = json.load(f)\n",
    "            print(f\"Loaded full model profiling results\")\n",
    "    \n",
    "    # Load other profiling results\n",
    "    result_files = glob.glob(os.path.join(directory, \"*.json\"))\n",
    "    for file_path in result_files:\n",
    "        try:\n",
    "            with open(file_path, \"r\") as f:\n",
    "                file_name = os.path.basename(file_path)\n",
    "                results[file_name] = json.load(f)\n",
    "                print(f\"Loaded {file_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to load pure pruning benchmark results\n",
    "def load_pruning_results(directory=\"pure_pruning_results\"):\n",
    "    \"\"\"Load pure pruning benchmark results from the given directory.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found\")\n",
    "        return results\n",
    "    \n",
    "    # Find subdirectories that contain benchmark results\n",
    "    subdirs = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "    \n",
    "    for subdir in subdirs:\n",
    "        subdir_path = os.path.join(directory, subdir)\n",
    "        \n",
    "        # Load benchmark config\n",
    "        config_path = os.path.join(subdir_path, \"config.json\")\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, \"r\") as f:\n",
    "                config = json.load(f)\n",
    "                \n",
    "            # Load metrics\n",
    "            metrics_path = os.path.join(subdir_path, \"metrics\")\n",
    "            if os.path.exists(metrics_path):\n",
    "                metrics_files = glob.glob(os.path.join(metrics_path, \"*.json\"))\n",
    "                if metrics_files:\n",
    "                    with open(metrics_files[0], \"r\") as f:\n",
    "                        metrics = json.load(f)\n",
    "                        \n",
    "                    results[subdir] = {\n",
    "                        \"config\": config,\n",
    "                        \"metrics\": metrics\n",
    "                    }\n",
    "                    print(f\"Loaded benchmark results from {subdir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load both types of results\n",
    "profiling_results = load_profiling_results()\n",
    "pruning_results = load_pruning_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Full Model Profiling Results\n",
    "\n",
    "Let's analyze the full model profiling results to understand the performance characteristics of different optimization levels and pruning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_full_model_profiling(results):\n",
    "    \"\"\"Analyze full model profiling results.\"\"\"\n",
    "    if \"full_model\" not in results:\n",
    "        print(\"Full model profiling results not found\")\n",
    "        return\n",
    "    \n",
    "    full_model = results[\"full_model\"]\n",
    "    \n",
    "    # Display basic information\n",
    "    if \"args\" in full_model:\n",
    "        args = full_model[\"args\"]\n",
    "        display(HTML(f\"<h3>Model Information</h3>\"))\n",
    "        display(HTML(f\"<p><b>Model:</b> {args.get('model_name', 'N/A')}</p>\"))\n",
    "        display(HTML(f\"<p><b>Device:</b> {args.get('device', 'N/A')}</p>\"))\n",
    "        display(HTML(f\"<p><b>Optimization Level:</b> {args.get('optimization_level', 'N/A')}</p>\"))\n",
    "        display(HTML(f\"<p><b>Pruning Levels:</b> {args.get('pruning_levels', 'N/A')}</p>\"))\n",
    "    \n",
    "    # Model loading analysis\n",
    "    if \"model_loading\" in full_model:\n",
    "        loading = full_model[\"model_loading\"]\n",
    "        display(HTML(f\"<h3>Model Loading Comparison</h3>\"))\n",
    "        \n",
    "        # Create comparison table\n",
    "        data = {\n",
    "            \"Model Type\": [\"Baseline\", \"Original\", \"Optimized\"],\n",
    "            \"Load Time (s)\": [\n",
    "                loading[\"baseline_model\"][\"load_time\"],\n",
    "                loading[\"original_model\"][\"load_time\"],\n",
    "                loading[\"optimized_model\"][\"load_time\"]\n",
    "            ],\n",
    "            \"Parameters\": [\n",
    "                loading[\"baseline_model\"][\"parameter_count\"],\n",
    "                loading[\"original_model\"][\"parameter_count\"],\n",
    "                loading[\"optimized_model\"][\"parameter_count\"]\n",
    "            ],\n",
    "            \"Memory (MB)\": [\n",
    "                loading[\"baseline_model\"][\"memory_usage\"] / (1024**2),\n",
    "                loading[\"original_model\"][\"memory_usage\"] / (1024**2),\n",
    "                loading[\"optimized_model\"][\"memory_usage\"] / (1024**2)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        display(df)\n",
    "        \n",
    "        # Plot loading time comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(data[\"Model Type\"], data[\"Load Time (s)\"], color=[\"lightgray\", \"dodgerblue\", \"green\"])\n",
    "        plt.title(\"Model Loading Time Comparison\")\n",
    "        plt.ylabel(\"Time (seconds)\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(data[\"Load Time (s)\"]):\n",
    "            plt.text(i, v + 0.1, f\"{v:.2f}s\", ha=\"center\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Pruning comparison analysis\n",
    "    if \"pruning_comparison\" in full_model:\n",
    "        display(HTML(f\"<h3>Pruning Performance Comparison</h3>\"))\n",
    "        \n",
    "        pruning_data = full_model[\"pruning_comparison\"]\n",
    "        pruning_levels = sorted([int(level) for level in pruning_data[\"original\"].keys()])\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = {\n",
    "            \"Pruning Level\": [],\n",
    "            \"Original TPS\": [],\n",
    "            \"Optimized TPS\": [],\n",
    "            \"Speedup\": []\n",
    "        }\n",
    "        \n",
    "        for level in pruning_levels:\n",
    "            level_str = str(level)\n",
    "            original_tps = pruning_data[\"original\"][level_str][\"tokens_per_second\"]\n",
    "            optimized_tps = pruning_data[\"optimized\"][level_str][\"tokens_per_second\"]\n",
    "            speedup = optimized_tps / original_tps\n",
    "            \n",
    "            data[\"Pruning Level\"].append(f\"{level}%\")\n",
    "            data[\"Original TPS\"].append(original_tps)\n",
    "            data[\"Optimized TPS\"].append(optimized_tps)\n",
    "            data[\"Speedup\"].append(speedup)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        display(df)\n",
    "        \n",
    "        # Plot tokens per second comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(pruning_levels, [pruning_data[\"original\"][str(level)][\"tokens_per_second\"] for level in pruning_levels], \n",
    "                 'o-', label=\"Original\", color=\"dodgerblue\", linewidth=2)\n",
    "        plt.plot(pruning_levels, [pruning_data[\"optimized\"][str(level)][\"tokens_per_second\"] for level in pruning_levels], \n",
    "                 'o-', label=\"Optimized\", color=\"green\", linewidth=2)\n",
    "        \n",
    "        plt.title(\"Generation Speed vs. Pruning Level\")\n",
    "        plt.xlabel(\"Pruning Level (%)\")\n",
    "        plt.ylabel(\"Tokens per Second\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.xticks(pruning_levels)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot speedup factors\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = plt.bar(pruning_levels, [data[\"Speedup\"][i] for i in range(len(pruning_levels))], color=\"coral\")\n",
    "        plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        plt.title(\"Speedup Factor by Pruning Level\")\n",
    "        plt.xlabel(\"Pruning Level (%)\")\n",
    "        plt.ylabel(\"Speedup Factor (Optimized / Original)\")\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.xticks(pruning_levels)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.05, f\"{height:.2f}x\", ha=\"center\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    # Component breakdown analysis\n",
    "    if \"component_breakdown\" in full_model:\n",
    "        display(HTML(f\"<h3>Component-Level Performance Analysis</h3>\"))\n",
    "        \n",
    "        breakdown = full_model[\"component_breakdown\"]\n",
    "        \n",
    "        # Check if data is available\n",
    "        if \"original\" in breakdown and \"optimized\" in breakdown:\n",
    "            # Get components with percentage data\n",
    "            if \"percentages\" in breakdown[\"original\"] and \"percentages\" in breakdown[\"optimized\"]:\n",
    "                orig_pct = breakdown[\"original\"][\"percentages\"]\n",
    "                opt_pct = breakdown[\"optimized\"][\"percentages\"]\n",
    "                \n",
    "                # Get common components\n",
    "                components = list(set(orig_pct.keys()) & set(opt_pct.keys()))\n",
    "                \n",
    "                # Create DataFrame\n",
    "                data = {\n",
    "                    \"Component\": [c.replace(\"_\", \" \").title() for c in components],\n",
    "                    \"Original %\": [orig_pct.get(c, 0) for c in components],\n",
    "                    \"Optimized %\": [opt_pct.get(c, 0) for c in components],\n",
    "                    \"Change\": [opt_pct.get(c, 0) - orig_pct.get(c, 0) for c in components]\n",
    "                }\n",
    "                \n",
    "                df = pd.DataFrame(data)\n",
    "                display(df)\n",
    "                \n",
    "                # Plot component time distribution\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                x = np.arange(len(components))\n",
    "                width = 0.35\n",
    "                \n",
    "                plt.bar(x - width/2, [orig_pct.get(c, 0) for c in components], width, \n",
    "                        label=\"Original\", color=\"dodgerblue\")\n",
    "                plt.bar(x + width/2, [opt_pct.get(c, 0) for c in components], width,\n",
    "                        label=\"Optimized\", color=\"green\")\n",
    "                \n",
    "                plt.title(\"Component Time Distribution\")\n",
    "                plt.xlabel(\"Component\")\n",
    "                plt.ylabel(\"Time (%)\")\n",
    "                plt.xticks(x, [c.replace(\"_\", \" \").title() for c in components], rotation=45, ha=\"right\")\n",
    "                plt.legend()\n",
    "                plt.grid(axis=\"y\", alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    # Integration optimization tests\n",
    "    if \"integration_tests\" in full_model:\n",
    "        display(HTML(f\"<h3>Integration Optimization Results</h3>\"))\n",
    "        \n",
    "        integration_data = full_model[\"integration_tests\"]\n",
    "        configs = list(integration_data.keys())\n",
    "        \n",
    "        # Get a representative pruning level\n",
    "        if configs and integration_data[configs[0]]:\n",
    "            level = next(iter(integration_data[configs[0]].keys()))\n",
    "            \n",
    "            # Create DataFrame\n",
    "            data = {\n",
    "                \"Configuration\": [c.replace(\"_\", \" \").title() for c in configs],\n",
    "                \"Tokens/sec\": [integration_data[c][level][\"tokens_per_second\"] for c in configs]\n",
    "            }\n",
    "            \n",
    "            # Add speedup relative to original\n",
    "            if \"original\" in configs:\n",
    "                original_tps = integration_data[\"original\"][level][\"tokens_per_second\"]\n",
    "                data[\"Speedup vs. Original\"] = [integration_data[c][level][\"tokens_per_second\"] / original_tps for c in configs]\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            display(df)\n",
    "            \n",
    "            # Plot comparison bar chart\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            bars = plt.bar(data[\"Configuration\"], data[\"Tokens/sec\"], \n",
    "                          color=[\"dodgerblue\" if c == \"Original\" else \"green\" for c in data[\"Configuration\"]])\n",
    "            \n",
    "            plt.title(f\"Integration Optimization Comparison (at {level}% pruning)\")\n",
    "            plt.xlabel(\"Configuration\")\n",
    "            plt.ylabel(\"Tokens per Second\")\n",
    "            plt.grid(axis=\"y\", alpha=0.3)\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, bar in enumerate(bars):\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height + 0.5, f\"{height:.1f}\", ha=\"center\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Run the full model analysis if results are available\n",
    "if profiling_results and \"full_model\" in profiling_results:\n",
    "    analyze_full_model_profiling(profiling_results)\n",
    "else:\n",
    "    print(\"Full model profiling results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Pure Pruning Benchmark Results\n",
    "\n",
    "Let's analyze the results from the pure pruning benchmark to understand the efficiency benefits of pruning in isolation from other optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pure_pruning_results(results):\n",
    "    \"\"\"Analyze pure pruning benchmark results.\"\"\"\n",
    "    if not results:\n",
    "        print(\"No pure pruning benchmark results found\")\n",
    "        return\n",
    "    \n",
    "    # Select the first benchmark result for detailed analysis\n",
    "    benchmark_id = next(iter(results))\n",
    "    benchmark = results[benchmark_id]\n",
    "    \n",
    "    # Display basic configuration\n",
    "    config = benchmark[\"config\"]\n",
    "    display(HTML(f\"<h3>Benchmark Configuration</h3>\"))\n",
    "    display(HTML(f\"<p><b>Model:</b> {config.get('model_name', 'N/A')}</p>\"))\n",
    "    display(HTML(f\"<p><b>Pruning Strategy:</b> {config.get('pruning_strategy', 'N/A')}</p>\"))\n",
    "    display(HTML(f\"<p><b>Pruning Method:</b> {config.get('pruning_method', 'N/A')}</p>\"))\n",
    "    display(HTML(f\"<p><b>Target Sparsity:</b> {config.get('target_sparsity', 'N/A')}</p>\"))\n",
    "    display(HTML(f\"<p><b>Epochs:</b> {config.get('epochs', 'N/A')}</p>\"))\n",
    "    \n",
    "    # Analyze metrics over time\n",
    "    metrics = benchmark[\"metrics\"]\n",
    "    display(HTML(f\"<h3>Metrics Over Time</h3>\"))\n",
    "    \n",
    "    if \"epochs\" in metrics:\n",
    "        epochs = sorted([int(e) for e in metrics[\"epochs\"]])\n",
    "        \n",
    "        # Create plots for key metrics\n",
    "        key_metrics = [\n",
    "            (\"perplexity\", \"Perplexity\", \"Lower is better\"),\n",
    "            (\"active_heads_percentage\", \"Active Heads (%)\", \"Lower means more pruning\"),\n",
    "            (\"inference_latency\", \"Inference Latency (ms/token)\", \"Lower is better\"),\n",
    "            (\"lexical_diversity\", \"Lexical Diversity\", \"Higher is better\"),\n",
    "            (\"repetition_score\", \"Repetition Score\", \"Lower is better\")\n",
    "        ]\n",
    "        \n",
    "        plt.figure(figsize=(15, 12))\n",
    "        \n",
    "        for i, (metric_key, metric_name, description) in enumerate(key_metrics):\n",
    "            if metric_key in metrics:\n",
    "                metric_values = [metrics[metric_key].get(str(e), None) for e in epochs]\n",
    "                metric_values = [v for v in metric_values if v is not None]\n",
    "                \n",
    "                if metric_values:\n",
    "                    plt.subplot(3, 2, i+1)\n",
    "                    plt.plot(epochs[:len(metric_values)], metric_values, 'o-', color=f\"C{i}\", linewidth=2)\n",
    "                    plt.title(f\"{metric_name} Over Time ({description})\")\n",
    "                    plt.xlabel(\"Epoch\")\n",
    "                    plt.ylabel(metric_name)\n",
    "                    plt.grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Add phase markers\n",
    "                    if config.get('pruning_start_epoch'):\n",
    "                        plt.axvline(x=int(config['pruning_start_epoch']), color='r', linestyle='--', alpha=0.5,\n",
    "                                   label=\"Start Pruning\")\n",
    "                    if config.get('pruning_end_epoch'):\n",
    "                        plt.axvline(x=int(config['pruning_end_epoch']), color='g', linestyle='--', alpha=0.5,\n",
    "                                  label=\"Start Fine-tuning\")\n",
    "                    \n",
    "                    if i == 0:  # Only add legend to the first plot\n",
    "                        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create table of final metrics\n",
    "        if epochs:\n",
    "            last_epoch = str(max(epochs))\n",
    "            final_metrics = {}\n",
    "            \n",
    "            for metric_key, metric_name, _ in key_metrics:\n",
    "                if metric_key in metrics and last_epoch in metrics[metric_key]:\n",
    "                    final_metrics[metric_name] = metrics[metric_key][last_epoch]\n",
    "            \n",
    "            display(HTML(f\"<h4>Final Metrics (Epoch {last_epoch})</h4>\"))\n",
    "            df = pd.DataFrame(final_metrics.items(), columns=[\"Metric\", \"Value\"])\n",
    "            display(df)\n",
    "    \n",
    "    # If multiple benchmarks are available, compare them\n",
    "    if len(results) > 1:\n",
    "        display(HTML(f\"<h3>Comparison Across Benchmarks</h3>\"))\n",
    "        \n",
    "        # Create comparison data\n",
    "        comparison_data = {\n",
    "            \"Benchmark\": [],\n",
    "            \"Strategy\": [],\n",
    "            \"Method\": [],\n",
    "            \"Final Perplexity\": [],\n",
    "            \"Final Latency\": [],\n",
    "            \"Active Heads %\": []\n",
    "        }\n",
    "        \n",
    "        for bench_id, bench_data in results.items():\n",
    "            config = bench_data[\"config\"]\n",
    "            metrics = bench_data[\"metrics\"]\n",
    "            \n",
    "            # Get final epoch\n",
    "            if \"epochs\" in metrics:\n",
    "                epochs = sorted([int(e) for e in metrics[\"epochs\"]])\n",
    "                if epochs:\n",
    "                    last_epoch = str(max(epochs))\n",
    "                    \n",
    "                    # Extract final metrics\n",
    "                    final_perplexity = metrics.get(\"perplexity\", {}).get(last_epoch, None)\n",
    "                    final_latency = metrics.get(\"inference_latency\", {}).get(last_epoch, None)\n",
    "                    active_heads = metrics.get(\"active_heads_percentage\", {}).get(last_epoch, None)\n",
    "                    \n",
    "                    # Add to comparison data\n",
    "                    comparison_data[\"Benchmark\"].append(bench_id)\n",
    "                    comparison_data[\"Strategy\"].append(config.get(\"pruning_strategy\", \"N/A\"))\n",
    "                    comparison_data[\"Method\"].append(config.get(\"pruning_method\", \"N/A\"))\n",
    "                    comparison_data[\"Final Perplexity\"].append(final_perplexity)\n",
    "                    comparison_data[\"Final Latency\"].append(final_latency)\n",
    "                    comparison_data[\"Active Heads %\"].append(active_heads)\n",
    "        \n",
    "        # Create DataFrame and display\n",
    "        if comparison_data[\"Benchmark\"]:\n",
    "            df = pd.DataFrame(comparison_data)\n",
    "            display(df)\n",
    "            \n",
    "            # Plot comparison\n",
    "            if len(df) >= 2:\n",
    "                plt.figure(figsize=(15, 6))\n",
    "                \n",
    "                # Plot perplexity comparison\n",
    "                plt.subplot(1, 2, 1)\n",
    "                bars = plt.bar(df[\"Strategy\"] + \" (\" + df[\"Method\"] + \")\", df[\"Final Perplexity\"])\n",
    "                plt.title(\"Final Perplexity by Approach\")\n",
    "                plt.xlabel(\"Pruning Approach\")\n",
    "                plt.ylabel(\"Perplexity (lower is better)\")\n",
    "                plt.grid(axis=\"y\", alpha=0.3)\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                \n",
    "                # Add value labels\n",
    "                for i, bar in enumerate(bars):\n",
    "                    height = bar.get_height()\n",
    "                    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f\"{height:.2f}\", ha=\"center\")\n",
    "                \n",
    "                # Plot latency comparison\n",
    "                plt.subplot(1, 2, 2)\n",
    "                bars = plt.bar(df[\"Strategy\"] + \" (\" + df[\"Method\"] + \")\", df[\"Final Latency\"])\n",
    "                plt.title(\"Final Inference Latency by Approach\")\n",
    "                plt.xlabel(\"Pruning Approach\")\n",
    "                plt.ylabel(\"Latency (ms/token, lower is better)\")\n",
    "                plt.grid(axis=\"y\", alpha=0.3)\n",
    "                plt.xticks(rotation=45, ha=\"right\")\n",
    "                \n",
    "                # Add value labels\n",
    "                for i, bar in enumerate(bars):\n",
    "                    height = bar.get_height()\n",
    "                    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1, f\"{height:.2f}\", ha=\"center\")\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "# Run the pure pruning analysis if results are available\n",
    "if pruning_results:\n",
    "    analyze_pure_pruning_results(pruning_results)\n",
    "else:\n",
    "    print(\"Pure pruning benchmark results not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Comparison\n",
    "\n",
    "Let's compare the results from both pure pruning and optimization-integrated profiling to understand how they work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pruning_and_optimization(profiling_results, pruning_results):\n",
    "    \"\"\"Compare pruning and optimization results.\"\"\"\n",
    "    if not profiling_results or not pruning_results:\n",
    "        print(\"Both profiling and pruning results are required for comparison\")\n",
    "        return\n",
    "    \n",
    "    display(HTML(f\"<h3>Comparison Between Pure Pruning and Optimized Pruning</h3>\"))\n",
    "    \n",
    "    # Check if we have the necessary data\n",
    "    if \"full_model\" not in profiling_results or \"pruning_comparison\" not in profiling_results[\"full_model\"]:\n",
    "        print(\"Missing optimization profiling data for comparison\")\n",
    "        return\n",
    "    \n",
    "    # Get data from profiling results\n",
    "    opt_pruning = profiling_results[\"full_model\"][\"pruning_comparison\"]\n",
    "    pruning_levels = sorted([int(level) for level in opt_pruning[\"original\"].keys()])\n",
    "    \n",
    "    # Get data from pure pruning results\n",
    "    pure_benchmark_id = next(iter(pruning_results))\n",
    "    pure_metrics = pruning_results[pure_benchmark_id][\"metrics\"]\n",
    "    \n",
    "    # Create comparison data\n",
    "    comparison_data = {\n",
    "        \"Pruning Level\": [],\n",
    "        \"Original TPS\": [],\n",
    "        \"Optimized TPS\": [],\n",
    "        \"Pure Pruning TPS\": [],\n",
    "        \"Opt vs Orig\": [],\n",
    "        \"Pure vs Orig\": []\n",
    "    }\n",
    "    \n",
    "    # Get last epoch for pure pruning\n",
    "    if \"epochs\" in pure_metrics:\n",
    "        epochs = sorted([int(e) for e in pure_metrics[\"epochs\"]])\n",
    "        if epochs:\n",
    "            last_epoch = str(max(epochs))\n",
    "            \n",
    "            # Find the closest pruning level in pure pruning results\n",
    "            if \"active_heads_percentage\" in pure_metrics and last_epoch in pure_metrics[\"active_heads_percentage\"]:\n",
    "                pure_active = pure_metrics[\"active_heads_percentage\"][last_epoch]\n",
    "                pure_pruning_level = 100 - pure_active\n",
    "                pure_latency = None\n",
    "                if \"inference_latency\" in pure_metrics and last_epoch in pure_metrics[\"inference_latency\"]:\n",
    "                    pure_latency = pure_metrics[\"inference_latency\"][last_epoch]\n",
    "                    pure_tps = 1000 / pure_latency if pure_latency > 0 else 0  # Convert ms/token to tokens/sec\n",
    "                \n",
    "                # Add to comparison data\n",
    "                for level in pruning_levels:\n",
    "                    level_str = str(level)\n",
    "                    original_tps = opt_pruning[\"original\"][level_str][\"tokens_per_second\"]\n",
    "                    optimized_tps = opt_pruning[\"optimized\"][level_str][\"tokens_per_second\"]\n",
    "                    \n",
    "                    comparison_data[\"Pruning Level\"].append(f\"{level}%\")\n",
    "                    comparison_data[\"Original TPS\"].append(original_tps)\n",
    "                    comparison_data[\"Optimized TPS\"].append(optimized_tps)\n",
    "                    \n",
    "                    # Use closest pure pruning level\n",
    "                    if pure_tps is not None and abs(level - pure_pruning_level) < 20:  # If within 20%\n",
    "                        comparison_data[\"Pure Pruning TPS\"].append(pure_tps)\n",
    "                    else:\n",
    "                        comparison_data[\"Pure Pruning TPS\"].append(None)\n",
    "                    \n",
    "                    # Calculate ratios\n",
    "                    comparison_data[\"Opt vs Orig\"].append(optimized_tps / original_tps if original_tps > 0 else None)\n",
    "                    if pure_tps is not None and abs(level - pure_pruning_level) < 20 and original_tps > 0:\n",
    "                        comparison_data[\"Pure vs Orig\"].append(pure_tps / original_tps)\n",
    "                    else:\n",
    "                        comparison_data[\"Pure vs Orig\"].append(None)\n",
    "    \n",
    "    # Create DataFrame and display\n",
    "    if comparison_data[\"Pruning Level\"]:\n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        display(df)\n",
    "        \n",
    "        # Plot comparison\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Filter to only rows with pure pruning data\n",
    "        valid_rows = [i for i, val in enumerate(comparison_data[\"Pure Pruning TPS\"]) if val is not None]\n",
    "        \n",
    "        if valid_rows:\n",
    "            # Extract valid data\n",
    "            valid_levels = [pruning_levels[i] for i in valid_rows]\n",
    "            valid_orig = [comparison_data[\"Original TPS\"][i] for i in valid_rows]\n",
    "            valid_opt = [comparison_data[\"Optimized TPS\"][i] for i in valid_rows]\n",
    "            valid_pure = [comparison_data[\"Pure Pruning TPS\"][i] for i in valid_rows]\n",
    "            \n",
    "            # Plot tokens per second comparison\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(valid_levels, valid_orig, 'o-', label=\"Original\", color=\"dodgerblue\", linewidth=2)\n",
    "            plt.plot(valid_levels, valid_opt, 'o-', label=\"Optimized\", color=\"green\", linewidth=2)\n",
    "            plt.plot(valid_levels, valid_pure, 'o-', label=\"Pure Pruning\", color=\"orange\", linewidth=2)\n",
    "            \n",
    "            plt.title(\"Generation Speed Comparison\")\n",
    "            plt.xlabel(\"Pruning Level (%)\")\n",
    "            plt.ylabel(\"Tokens per Second\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.xticks(valid_levels)\n",
    "            \n",
    "            # Plot speedup ratios\n",
    "            plt.subplot(2, 1, 2)\n",
    "            valid_opt_ratio = [comparison_data[\"Opt vs Orig\"][i] for i in valid_rows]\n",
    "            valid_pure_ratio = [comparison_data[\"Pure vs Orig\"][i] for i in valid_rows]\n",
    "            \n",
    "            plt.plot(valid_levels, valid_opt_ratio, 'o-', label=\"Optimized / Original\", color=\"green\", linewidth=2)\n",
    "            plt.plot(valid_levels, valid_pure_ratio, 'o-', label=\"Pure / Original\", color=\"orange\", linewidth=2)\n",
    "            plt.axhline(y=1.0, color='k', linestyle='--', alpha=0.3)\n",
    "            \n",
    "            plt.title(\"Speedup Ratio Comparison\")\n",
    "            plt.xlabel(\"Pruning Level (%)\")\n",
    "            plt.ylabel(\"Speedup Ratio (>1 is better)\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend()\n",
    "            plt.xticks(valid_levels)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Calculate and display the best overall approach\n",
    "            best_opt = max(valid_opt_ratio) if valid_opt_ratio else 0\n",
    "            best_pure = max(valid_pure_ratio) if valid_pure_ratio else 0\n",
    "            \n",
    "            display(HTML(f\"<h4>Performance Summary</h4>\"))\n",
    "            display(HTML(f\"<p>Best speedup with optimized pruning: <b>{best_opt:.2f}x</b></p>\"))\n",
    "            display(HTML(f\"<p>Best speedup with pure pruning: <b>{best_pure:.2f}x</b></p>\"))\n",
    "            \n",
    "            if best_opt > best_pure:\n",
    "                improvement = (best_opt / best_pure - 1) * 100\n",
    "                display(HTML(f\"<p>Combined optimization provides <b>{improvement:.1f}%</b> better performance than pruning alone</p>\"))\n",
    "            else:\n",
    "                improvement = (best_pure / best_opt - 1) * 100\n",
    "                display(HTML(f\"<p>Pure pruning provides <b>{improvement:.1f}%</b> better performance than combined optimization in this case</p>\"))\n",
    "\n",
    "# Run the comparison if both sets of results are available\n",
    "if profiling_results and pruning_results:\n",
    "    compare_pruning_and_optimization(profiling_results, pruning_results)\n",
    "else:\n",
    "    print(\"Both profiling and pruning results are required for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Recommendations\n",
    "\n",
    "Based on the analysis above, we can provide recommendations for the most efficient configuration of pruning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations(profiling_results, pruning_results):\n",
    "    \"\"\"Generate recommendations based on the analysis.\"\"\"\n",
    "    display(HTML(f\"<h3>Conclusions and Recommendations</h3>\"))\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Analyze pruning level recommendations\n",
    "    if profiling_results and \"full_model\" in profiling_results and \"pruning_comparison\" in profiling_results[\"full_model\"]:\n",
    "        # Get the pruning data\n",
    "        pruning_data = profiling_results[\"full_model\"][\"pruning_comparison\"]\n",
    "        pruning_levels = sorted([int(level) for level in pruning_data[\"original\"].keys()])\n",
    "        \n",
    "        # Find the best pruning level for speed\n",
    "        best_level = 0\n",
    "        best_speedup = 0\n",
    "        for level in pruning_levels:\n",
    "            level_str = str(level)\n",
    "            if level_str in pruning_data[\"optimized\"] and level_str in pruning_data[\"original\"]:\n",
    "                speedup = pruning_data[\"optimized\"][level_str][\"tokens_per_second\"] / pruning_data[\"original\"][level_str][\"tokens_per_second\"]\n",
    "                if speedup > best_speedup:\n",
    "                    best_speedup = speedup\n",
    "                    best_level = level\n",
    "        \n",
    "        recommendations.append(f\"**Optimal Pruning Level**: {best_level}% pruning provides the best speed improvement ({best_speedup:.2f}x).\")\n",
    "    \n",
    "    # Analyze pruning strategy recommendations\n",
    "    if pruning_results and len(pruning_results) > 1:\n",
    "        # Compare strategies\n",
    "        strategy_performance = {}\n",
    "        \n",
    "        for bench_id, bench_data in pruning_results.items():\n",
    "            config = bench_data[\"config\"]\n",
    "            metrics = bench_data[\"metrics\"]\n",
    "            \n",
    "            # Get final epoch\n",
    "            if \"epochs\" in metrics:\n",
    "                epochs = sorted([int(e) for e in metrics[\"epochs\"]])\n",
    "                if epochs:\n",
    "                    last_epoch = str(max(epochs))\n",
    "                    \n",
    "                    # Extract strategy and method\n",
    "                    strategy = config.get(\"pruning_strategy\")\n",
    "                    method = config.get(\"pruning_method\")\n",
    "                    \n",
    "                    if strategy and method:\n",
    "                        key = f\"{strategy}_{method}\"\n",
    "                        \n",
    "                        # Extract metrics\n",
    "                        if \"inference_latency\" in metrics and last_epoch in metrics[\"inference_latency\"]:\n",
    "                            latency = metrics[\"inference_latency\"][last_epoch]\n",
    "                            strategy_performance[key] = {\n",
    "                                \"strategy\": strategy,\n",
    "                                \"method\": method,\n",
    "                                \"latency\": latency,\n",
    "                                \"tps\": 1000 / latency if latency > 0 else 0\n",
    "                            }\n",
    "                            \n",
    "                            if \"perplexity\" in metrics and last_epoch in metrics[\"perplexity\"]:\n",
    "                                strategy_performance[key][\"perplexity\"] = metrics[\"perplexity\"][last_epoch]\n",
    "        \n",
    "        # Find best strategy for speed\n",
    "        if strategy_performance:\n",
    "            best_strategy = max(strategy_performance.items(), key=lambda x: x[1][\"tps\"])\n",
    "            recommendations.append(f\"**Best Pruning Strategy**: {best_strategy[1]['strategy']} pruning with {best_strategy[1]['method']} method offers the highest performance ({best_strategy[1]['tps']:.2f} tokens/sec).\")\n",
    "    \n",
    "    # Analyze optimization recommendations\n",
    "    if profiling_results and \"full_model\" in profiling_results and \"integration_tests\" in profiling_results[\"full_model\"]:\n",
    "        integration_data = profiling_results[\"full_model\"][\"integration_tests\"]\n",
    "        configs = list(integration_data.keys())\n",
    "        \n",
    "        if configs and integration_data[configs[0]]:\n",
    "            level = next(iter(integration_data[configs[0]].keys()))\n",
    "            \n",
    "            # Find the best configuration\n",
    "            best_config = \"\"\n",
    "            best_tps = 0\n",
    "            for config in configs:\n",
    "                if config != \"original\":\n",
    "                    tps = integration_data[config][level][\"tokens_per_second\"]\n",
    "                    if tps > best_tps:\n",
    "                        best_tps = tps\n",
    "                        best_config = config\n",
    "            \n",
    "            if best_config:\n",
    "                recommendations.append(f\"**Optimal Integration Configuration**: The '{best_config.replace('_', ' ').title()}' configuration offers the best performance ({best_tps:.2f} tokens/sec).\")\n",
    "    \n",
    "    # Generate final recommendations\n",
    "    if recommendations:\n",
    "        for rec in recommendations:\n",
    "            display(Markdown(rec))\n",
    "        \n",
    "        # Overall recommendation\n",
    "        display(Markdown(\"\\n**Overall Recommendation**:\"))\n",
    "        display(Markdown(\"Based on the profiling and benchmark results, we recommend combining optimization with strategic pruning to achieve the best performance. The data shows that pruning efficiency depends greatly on the pruning level, strategy, and method used.\"))\n",
    "        \n",
    "        # Display specific recommendation based on what we found\n",
    "        if \"full_model\" in profiling_results and \"pruning_comparison\" in profiling_results[\"full_model\"]:\n",
    "            # Get specific recommendations from the data\n",
    "            best_level_str = next((r for r in recommendations if \"**Optimal Pruning Level**\" in r), \"\")\n",
    "            best_level_match = best_level_str.split(\"%\")[0].split(\" \")[-1] if best_level_str else \"30-50\"\n",
    "            \n",
    "            best_strategy_str = next((r for r in recommendations if \"**Best Pruning Strategy**\" in r), \"\")\n",
    "            if \"gradual\" in best_strategy_str.lower():\n",
    "                strategy_rec = \"gradual pruning during training\"\n",
    "            elif \"one_shot\" in best_strategy_str.lower():\n",
    "                strategy_rec = \"one-shot pruning followed by fine-tuning\"\n",
    "            else:\n",
    "                strategy_rec = \"iterative pruning and fine-tuning cycles\"\n",
    "            \n",
    "            best_method_match = \"entropy-based\" if \"entropy\" in best_strategy_str.lower() else \\\n",
    "                                \"magnitude-based\" if \"magnitude\" in best_strategy_str.lower() else \"random\"\n",
    "            \n",
    "            display(Markdown(f\"For the best balance of speed and quality, use **{best_level_match}% pruning** with **{strategy_rec}** using the **{best_method_match}** selection method.\"))\n",
    "    else:\n",
    "        display(Markdown(\"Not enough data to generate specific recommendations. Run more benchmarks with different configurations to get detailed recommendations.\"))\n",
    "\n",
    "# Generate recommendations if results are available\n",
    "if profiling_results or pruning_results:\n",
    "    generate_recommendations(profiling_results, pruning_results)\n",
    "else:\n",
    "    print(\"No profiling or pruning results available for generating recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Based on the analysis, here are some suggested next steps for further improving the model efficiency:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Experiment with Hybrid Approaches**: Try combining the best aspects of both optimization-integrated and pure pruning approaches.\n",
    "\n",
    "2. **Test on Larger Models**: Run benchmarks on larger models to see if the efficiency gains scale with model size.\n",
    "\n",
    "3. **Explore Dynamic Pruning**: Implement dynamic pruning that adapts to input complexity during inference.\n",
    "\n",
    "4. **Optimize Component Bottlenecks**: Focus optimization efforts on the components that take the most time in the profiling results.\n",
    "\n",
    "5. **Add Quantization**: Consider adding post-training quantization to further reduce memory usage and increase speed.\n",
    "\n",
    "6. **Real-world Task Evaluation**: Test the pruned and optimized models on real-world tasks to ensure practical benefits are maintained.\n",
    "\n",
    "7. **Hardware-specific Optimization**: Adapt the optimization strategies to target specific hardware platforms (CPU, GPU, TPU, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}