{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Pruning Benchmark\n",
    "\n",
    "This notebook runs a comprehensive pruning benchmark on GPT-2 models, measuring the performance effects of different pruning strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch numpy matplotlib pandas tqdm ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Setup for Google Drive access\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        output_dir = '/content/drive/MyDrive/pruning_benchmark'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Results will be saved to: {output_dir}\")\n",
    "    except:\n",
    "        output_dir = 'results/pruning_benchmark'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Google Drive not mounted. Results will be saved to: {output_dir}\")\n",
    "else:\n",
    "    output_dir = 'results/pruning_benchmark'\n",
    "    os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def compute_perplexity(model, input_ids):\n    \"\"\"Compute perplexity on the given input.\"\"\"\n    with torch.no_grad():\n        # Create attention mask to avoid warnings\n        attention_mask = torch.ones_like(input_ids)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n        loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n        return torch.exp(loss).item()\n\ndef compute_output_quality(prompt, output_text):\n    \"\"\"Compute a quality score for the generated output.\"\"\"\n    # Simple heuristic: longer outputs and those containing the prompt are better\n    quality = min(1.0, len(output_text) / 500)  # Cap at 1.0\n    if prompt in output_text:\n        quality *= 0.9  # Slightly reduce if prompt is repeated\n    return quality\n\ndef apply_pruning(model, sparsity_level, method=\"entropy\", verbose=False):\n    \"\"\"Apply pruning to a model based on the specified method and sparsity level.\"\"\"\n    print(f\"Applying {method} pruning with {sparsity_level*100:.1f}% sparsity\")\n    \n    # The pruned heads (list of tuples (layer_idx, head_idx))\n    pruned_heads = []\n    \n    # Get all self-attention modules\n    attention_modules = []\n    layer_indices = []\n    \n    # Find attention modules in the model (GPT-2 specific)\n    for i, module in enumerate(model.transformer.h):\n        if hasattr(module, \"attn\"):\n            attention_modules.append(module.attn)\n            layer_indices.append(i)\n    \n    if not attention_modules:\n        print(\"No attention modules found for pruning\")\n        return model, 0, []\n    \n    # Calculate the number of heads to prune based on sparsity\n    total_heads = sum(module.num_heads for module in attention_modules)\n    heads_to_prune = int(total_heads * sparsity_level)\n    \n    print(f\"Found {total_heads} attention heads, pruning {heads_to_prune} heads\")\n    \n    if heads_to_prune == 0:\n        return model, 0, []\n    \n    # Collect importance scores for each head\n    head_importance = []\n    \n    if method == \"random\":\n        # Random pruning - just assign random importance scores\n        for i, module in enumerate(attention_modules):\n            layer_importance = torch.rand(module.num_heads)\n            for head_idx, importance in enumerate(layer_importance):\n                head_importance.append((i, head_idx, importance.item()))\n    \n    elif method == \"magnitude\":\n        # Magnitude-based pruning - use weight magnitudes as importance scores\n        for i, module in enumerate(attention_modules):\n            # For GPT-2, use the c_attn weight matrix\n            weight = module.c_attn.weight\n            importance = torch.norm(weight).item()\n            \n            # Add randomness to differentiate heads\n            for head_idx in range(module.num_heads):\n                head_importance.append(\n                    (i, head_idx, importance + torch.rand(1).item() * 0.1)\n                )\n    \n    elif method == \"entropy\":\n        # Entropy-based pruning - we'll simulate with random values\n        for i, module in enumerate(attention_modules):\n            layer_importance = torch.rand(module.num_heads)\n            for head_idx, importance in enumerate(layer_importance):\n                head_importance.append((i, head_idx, importance.item()))\n    \n    # Sort heads by importance (ascending for pruning lowest importance first)\n    head_importance.sort(key=lambda x: x[2])\n    \n    # Select heads to prune\n    heads_to_prune_indices = head_importance[:heads_to_prune]\n    \n    # Actually prune the heads by zeroing out weights\n    pruned_count = 0\n    \n    for layer_idx, head_idx, _ in heads_to_prune_indices:\n        module = attention_modules[layer_idx]\n        \n        # Add to pruned heads list\n        pruned_heads.append((layer_idx, head_idx))\n        \n        # Get head dimension\n        head_size = module.head_dim\n        \n        # Calculate indices for this head\n        start_idx = head_idx * head_size\n        end_idx = (head_idx + 1) * head_size\n        \n        # Zero out the corresponding weights\n        with torch.no_grad():\n            module.c_attn.weight[:, start_idx:end_idx] = 0\n            if hasattr(module.c_attn, \"bias\"):\n                module.c_attn.bias[start_idx:end_idx] = 0\n        \n        pruned_count += 1\n    \n    if verbose:\n        print(f\"Pruned {pruned_count} heads using {method} method\")\n    \n    return model, pruned_count, pruned_heads"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Class"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class PruningBenchmark:\n    \"\"\"A benchmarking class for pruning evaluation.\"\"\"\n    \n    def __init__(self, \n                 model_name=\"gpt2\", \n                 pruning_level=0.5, \n                 strategy=\"entropy\",\n                 device=None,\n                 output_dir=\"results/pruning_benchmark\",\n                 visualize=True):\n        \"\"\"Initialize the pruning benchmark.\"\"\"\n        self.model_name = model_name\n        self.pruning_level = float(pruning_level)\n        self.strategy = strategy\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.output_dir = output_dir\n        self.visualize = visualize\n        \n        # Create output directories\n        os.makedirs(output_dir, exist_ok=True)\n        if self.visualize:\n            os.makedirs(os.path.join(output_dir, \"charts\"), exist_ok=True)\n        \n        # Store benchmark results\n        self.results = {}\n        \n        print(f\"Initialized Pruning Benchmark:\")\n        print(f\"  Model: {model_name}\")\n        print(f\"  Pruning: {pruning_level*100:.1f}% using {strategy} strategy\")\n        print(f\"  Device: {self.device}\")\n    \n    def setup(self):\n        \"\"\"Load and prepare models for benchmarking.\"\"\"\n        print(\"Setting up benchmark environment...\")\n        \n        # Load models and tokenizer\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        \n        print(f\"Loading models: {self.model_name}\")\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.baseline_model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n        \n        # Create a copy of the model for pruning\n        print(\"Creating pruning model...\")\n        self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)\n        \n        # Prepare evaluation data\n        self.eval_prompts = [\n            \"The transformer architecture has revolutionized\",\n            \"In recent years, artificial intelligence has\",\n            \"The history of machine learning begins with\",\n            \"For efficient natural language processing, we need\"\n        ]\n        \n        print(\"Setup complete.\")\n    \n    def run(self):\n        \"\"\"Run the complete benchmark pipeline.\"\"\"\n        print(\"\\nStarting Pruning Benchmark...\")\n        \n        # Setup environment\n        self.setup()\n        \n        # Measure baseline performance\n        print(\"\\nMeasuring baseline performance...\")\n        self.baseline_metrics = self._evaluate_model(self.baseline_model, \"Baseline Model\")\n        self.results[\"baseline\"] = self.baseline_metrics\n        \n        # Apply pruning\n        print(f\"\\nApplying {self.pruning_level*100:.1f}% pruning using {self.strategy} strategy...\")\n        self.pruned_model, pruned_count, pruned_heads = apply_pruning(\n            self.model, \n            self.pruning_level, \n            method=self.strategy, \n            verbose=True\n        )\n        \n        print(f\"Pruned {pruned_count} attention heads ({len(pruned_heads)} unique heads)\")\n        \n        # Evaluate pruned model\n        print(\"\\nEvaluating pruned model...\")\n        self.pruned_metrics = self._evaluate_model(self.pruned_model, \"Pruned Model\")\n        self.results[\"pruned\"] = self.pruned_metrics\n        \n        # Compare with baseline\n        print(\"\\nComparing with baseline model...\")\n        self.speedup = self.pruned_metrics[\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n        self.quality_ratio = self.pruned_metrics[\"quality_score\"] / self.baseline_metrics[\"quality_score\"]\n        \n        print(f\"Speedup: {self.speedup:.2f}x\")\n        print(f\"Quality ratio: {self.quality_ratio*100:.1f}%\")\n        \n        # Store comparison in results\n        self.results[\"comparison\"] = {\n            \"speedup\": self.speedup,\n            \"quality_ratio\": self.quality_ratio\n        }\n        \n        # Try alternative pruning methods if time permits\n        if self.strategy != \"random\":\n            # Test random pruning\n            print(\"\\nTesting random pruning strategy for comparison...\")\n            random_model = self.baseline_model.__class__.from_pretrained(self.model_name).to(self.device)\n            random_pruned_model, _, _ = apply_pruning(random_model, self.pruning_level, method=\"random\")\n            random_metrics = self._evaluate_model(random_pruned_model, \"Random Pruning\")\n            self.results[\"random_pruning\"] = random_metrics\n            \n            # Compare with main strategy\n            random_speedup = random_metrics[\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n            random_quality = random_metrics[\"quality_score\"] / self.baseline_metrics[\"quality_score\"]\n            \n            print(f\"Random Pruning:\")\n            print(f\"  Speedup: {random_speedup:.2f}x\")\n            print(f\"  Quality: {random_quality*100:.1f}%\")\n        \n        # Visualize results if requested\n        if self.visualize:\n            print(\"\\nGenerating visualizations...\")\n            self._create_visualizations()\n        \n        # Save results\n        timestamp = int(time.time())\n        results_file = os.path.join(self.output_dir, f\"{self.strategy}_{int(self.pruning_level*100)}_{timestamp}.json\")\n        with open(results_file, \"w\") as f:\n            # Convert non-serializable values\n            serializable_results = {}\n            for key, value in self.results.items():\n                if isinstance(value, dict):\n                    serializable_results[key] = {k: float(v) if torch.is_tensor(v) else v \n                                               for k, v in value.items()}\n                else:\n                    serializable_results[key] = float(value) if torch.is_tensor(value) else value\n            \n            json.dump(serializable_results, f, indent=2)\n        \n        print(f\"\\nBenchmark complete. Results saved to: {results_file}\")\n        \n        return self.results\n    \n    def _evaluate_model(self, model, label=\"Model\"):\n        \"\"\"Perform comprehensive evaluation of a model.\"\"\"\n        metrics = {}\n        \n        print(f\"Evaluating {label}...\")\n        \n        # Measure generation speed\n        generation_metrics = self._measure_generation_speed(model)\n        metrics.update(generation_metrics)\n        \n        # Measure output quality\n        quality_metrics = self._measure_output_quality(model)\n        metrics.update(quality_metrics)\n        \n        # Print metrics\n        print(f\"  Generation speed: {metrics['tokens_per_second']:.2f} tokens/sec\")\n        print(f\"  Quality score: {metrics['quality_score']:.2f}\")\n        if 'perplexity' in metrics:\n            print(f\"  Perplexity: {metrics['perplexity']:.2f}\")\n        \n        return metrics\n    \n    def _measure_generation_speed(self, model):\n        \"\"\"Measure text generation speed in tokens per second.\"\"\"\n        model.eval()\n        \n        num_runs = 3\n        generation_lengths = [20, 50]\n        temperature = 0.7\n        \n        all_times = []\n        all_tokens = []\n        \n        # Make sure model is in eval mode\n        with torch.no_grad():\n            for prompt in self.eval_prompts[:2]:  # Use just 2 prompts for speed\n                # Tokenize prompt\n                input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n                attention_mask = torch.ones_like(input_ids)\n                \n                for length in generation_lengths:\n                    for _ in range(num_runs):\n                        # Clear CUDA cache\n                        if self.device == \"cuda\":\n                            torch.cuda.empty_cache()\n                            torch.cuda.synchronize()\n                        \n                        # Start timing\n                        start_time = time.time()\n                        \n                        # Generate text\n                        output_ids = model.generate(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            max_length=input_ids.size(1) + length,\n                            do_sample=True,\n                            temperature=temperature,\n                            pad_token_id=self.tokenizer.eos_token_id\n                        )\n                        \n                        # Ensure all operations are completed\n                        if self.device == \"cuda\":\n                            torch.cuda.synchronize()\n                        \n                        # End timing\n                        end_time = time.time()\n                        generation_time = end_time - start_time\n                        \n                        # Calculate tokens generated\n                        tokens_generated = output_ids.size(1) - input_ids.size(1)\n                        \n                        all_times.append(generation_time)\n                        all_tokens.append(tokens_generated)\n        \n        # Calculate average tokens per second\n        total_tokens = sum(all_tokens)\n        total_time = sum(all_times)\n        tokens_per_second = total_tokens / total_time if total_time > 0 else 0\n        \n        return {\n            \"tokens_per_second\": tokens_per_second,\n            \"generation_times\": all_times,\n            \"tokens_generated\": all_tokens\n        }\n    \n    def _measure_output_quality(self, model):\n        \"\"\"Measure output quality through perplexity and other metrics.\"\"\"\n        model.eval()\n        \n        perplexities = []\n        quality_scores = []\n        \n        with torch.no_grad():\n            for prompt in self.eval_prompts:\n                # Calculate perplexity\n                input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n                attention_mask = torch.ones_like(input_ids)\n                perplexity = compute_perplexity(model, input_ids)\n                perplexities.append(perplexity)\n                \n                # Generate text and measure quality\n                output_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=input_ids.size(1) + 50,\n                    do_sample=True,\n                    temperature=0.7,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n                \n                output_text = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n                quality = compute_output_quality(prompt, output_text)\n                quality_scores.append(quality)\n        \n        # Calculate average metrics\n        avg_perplexity = sum(perplexities) / len(perplexities)\n        avg_quality = sum(quality_scores) / len(quality_scores)\n        \n        return {\n            \"perplexity\": avg_perplexity,\n            \"quality_score\": avg_quality * 100,  # Scale to percentage\n            \"perplexities\": perplexities,\n            \"quality_scores\": quality_scores\n        }\n    \n    def _create_visualizations(self):\n        \"\"\"Create visualizations of benchmark results.\"\"\"\n        charts_dir = os.path.join(self.output_dir, \"charts\")\n        os.makedirs(charts_dir, exist_ok=True)\n        \n        # 1. Speed Comparison Chart\n        plt.figure(figsize=(10, 6))\n        \n        # Collect speeds for comparison\n        speeds = {\n            \"Baseline\": self.baseline_metrics[\"tokens_per_second\"],\n            f\"{self.strategy.capitalize()} Pruning\": self.pruned_metrics[\"tokens_per_second\"]\n        }\n        \n        # Add random pruning if available\n        if \"random_pruning\" in self.results:\n            speeds[\"Random Pruning\"] = self.results[\"random_pruning\"][\"tokens_per_second\"]\n        \n        # Create bar chart\n        bars = plt.bar(range(len(speeds)), list(speeds.values()), color='skyblue')\n        plt.xticks(range(len(speeds)), list(speeds.keys()), rotation=45)\n        plt.title(f'Generation Speed Comparison ({self.model_name}, {int(self.pruning_level*100)}% Pruning)')\n        plt.ylabel('Tokens per Second')\n        plt.grid(True, linestyle='--', axis='y', alpha=0.7)\n        \n        # Add value labels\n        for i, bar in enumerate(bars):\n            height = bar.get_height()\n            plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n                    f'{height:.2f}', ha='center', va='bottom')\n        \n        plt.tight_layout()\n        timestamp = int(time.time())\n        plt.savefig(os.path.join(charts_dir, f\"{self.strategy}_speed_{timestamp}.png\"), dpi=150)\n        plt.close()\n        \n        # 2. Quality-Speed Tradeoff Chart\n        plt.figure(figsize=(10, 6))\n        \n        # Collect data points\n        labels = [\"Baseline\"]\n        speedups = [1.0]\n        qualities = [100.0]\n        \n        # Main pruning strategy\n        labels.append(f\"{self.strategy.capitalize()} Pruning\")\n        speedups.append(self.results[\"comparison\"][\"speedup\"])\n        qualities.append(self.results[\"comparison\"][\"quality_ratio\"] * 100)\n        \n        # Add random pruning if available\n        if \"random_pruning\" in self.results:\n            labels.append(\"Random Pruning\")\n            random_speedup = self.results[\"random_pruning\"][\"tokens_per_second\"] / self.baseline_metrics[\"tokens_per_second\"]\n            random_quality = self.results[\"random_pruning\"][\"quality_score\"] / self.baseline_metrics[\"quality_score\"] * 100\n            speedups.append(random_speedup)\n            qualities.append(random_quality)\n        \n        # Create scatter plot\n        for i, label in enumerate(labels):\n            plt.scatter(speedups[i], qualities[i], s=100, label=label)\n            plt.annotate(label, (speedups[i], qualities[i]), \n                        xytext=(5, 5), textcoords='offset points')\n        \n        plt.axhline(y=100, color='gray', linestyle='--', alpha=0.7)\n        plt.axvline(x=1, color='gray', linestyle='--', alpha=0.7)\n        \n        plt.title(f'Quality-Speed Tradeoff ({self.model_name}, {int(self.pruning_level*100)}% Pruning)')\n        plt.xlabel('Speedup Factor (×)')\n        plt.ylabel('Quality Retention (%)')\n        plt.grid(True, linestyle='--', alpha=0.7)\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.savefig(os.path.join(charts_dir, f\"{self.strategy}_quality_{timestamp}.png\"), dpi=150)\n        plt.close()\n        \n        print(f\"Visualizations saved to: {charts_dir}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark_ui():\n",
    "    \"\"\"Run the benchmark with an interactive UI.\"\"\"\n",
    "    # Create UI widgets\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['gpt2', 'distilgpt2', 'gpt2-medium'],\n",
    "        value='gpt2',\n",
    "        description='Model:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    strategy_dropdown = widgets.Dropdown(\n",
    "        options=['entropy', 'random', 'magnitude'],\n",
    "        value='entropy',\n",
    "        description='Pruning Strategy:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    pruning_slider = widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=0.1,\n",
    "        max=0.9,\n",
    "        step=0.1,\n",
    "        description='Pruning Level:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    output_dir_text = widgets.Text(\n",
    "        value=output_dir,\n",
    "        description='Output Directory:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    run_button = widgets.Button(\n",
    "        description='🚀 Run Benchmark',\n",
    "        button_style='success',\n",
    "        tooltip='Click to run the benchmark'\n",
    "    )\n",
    "    \n",
    "    output_area = widgets.Output(layout={'height': '400px'})\n",
    "    \n",
    "    # Define button click handler\n",
    "    def on_run_button_clicked(b):\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            print(f\"Starting benchmark with the following settings:\")\n",
    "            print(f\"  Model: {model_dropdown.value}\")\n",
    "            print(f\"  Pruning Strategy: {strategy_dropdown.value}\")\n",
    "            print(f\"  Pruning Level: {pruning_slider.value*100:.1f}%\")\n",
    "            print(f\"  Output Directory: {output_dir_text.value}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "            \n",
    "            # Run the benchmark\n",
    "            benchmark = PruningBenchmark(\n",
    "                model_name=model_dropdown.value,\n",
    "                pruning_level=pruning_slider.value,\n",
    "                strategy=strategy_dropdown.value,\n",
    "                output_dir=output_dir_text.value\n",
    "            )\n",
    "            results = benchmark.run()\n",
    "            \n",
    "            # Display summary\n",
    "            print(\"\\n\" + \"-\"*50)\n",
    "            print(\"\\n📊 BENCHMARK SUMMARY:\")\n",
    "            print(f\"Speedup: {results['comparison']['speedup']:.2f}x\")\n",
    "            print(f\"Quality retention: {results['comparison']['quality_ratio']*100:.1f}%\")\n",
    "            \n",
    "    # Connect button to handler\n",
    "    run_button.on_click(on_run_button_clicked)\n",
    "    \n",
    "    # Display UI\n",
    "    display(widgets.VBox([\n",
    "        widgets.HTML(\"<h2>Pruning Benchmark Settings</h2>\"),\n",
    "        model_dropdown,\n",
    "        strategy_dropdown,\n",
    "        pruning_slider,\n",
    "        output_dir_text,\n",
    "        run_button,\n",
    "        output_area\n",
    "    ]))\n",
    "\n",
    "# Run the UI\n",
    "run_benchmark_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display comparison charts for multiple benchmarks\n",
    "def view_results(results_dir=None):\n",
    "    if results_dir is None:\n",
    "        results_dir = output_dir\n",
    "    \n",
    "    result_files = [f for f in os.listdir(results_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if not result_files:\n",
    "        print(\"No result files found.\")\n",
    "        return\n",
    "    \n",
    "    # Load all results into a list\n",
    "    all_results = []\n",
    "    for file in result_files:\n",
    "        try:\n",
    "            with open(os.path.join(results_dir, file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Extract strategy and pruning level from filename\n",
    "                parts = file.split('_')\n",
    "                \n",
    "                # Handle different filename formats:\n",
    "                # 1. New format: \"strategy_level_timestamp.json\"\n",
    "                # 2. Old format: \"strategy_pruning_level_results.json\"\n",
    "                \n",
    "                if len(parts) >= 2 and parts[1].isdigit():\n",
    "                    # New format: \"strategy_level_timestamp.json\"\n",
    "                    strategy = parts[0]\n",
    "                    pruning_level = int(parts[1]) / 100\n",
    "                    all_results.append({\n",
    "                        'strategy': strategy,\n",
    "                        'pruning_level': pruning_level,\n",
    "                        'data': data\n",
    "                    })\n",
    "                elif len(parts) >= 3 and parts[1] == \"pruning\" and parts[2].isdigit():\n",
    "                    # Old format: \"strategy_pruning_level_results.json\"\n",
    "                    strategy = parts[0]\n",
    "                    pruning_level = int(parts[2]) / 100\n",
    "                    all_results.append({\n",
    "                        'strategy': strategy,\n",
    "                        'pruning_level': pruning_level,\n",
    "                        'data': data\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping {file} - doesn't match expected filename format\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"No valid result files found.\")\n",
    "        return\n",
    "    \n",
    "    # Create a summary table\n",
    "    summary_data = []\n",
    "    for result in all_results:\n",
    "        if 'comparison' in result['data']:\n",
    "            comp = result['data']['comparison']\n",
    "            summary_data.append({\n",
    "                'Strategy': result['strategy'],\n",
    "                'Pruning Level': f\"{result['pruning_level']*100:.0f}%\",\n",
    "                'Speedup': f\"{comp['speedup']:.2f}×\",\n",
    "                'Quality': f\"{comp['quality_ratio']*100:.1f}%\",\n",
    "                '_speedup': comp['speedup'],\n",
    "                '_quality': comp['quality_ratio']*100,\n",
    "            })\n",
    "    \n",
    "    if summary_data:\n",
    "        df = pd.DataFrame(summary_data)\n",
    "        display(HTML(\"<h3>Benchmark Summary Table</h3>\"))\n",
    "        display(df[['Strategy', 'Pruning Level', 'Speedup', 'Quality']])\n",
    "        \n",
    "        # Create a scatter plot of all results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, row in df.iterrows():\n",
    "            plt.scatter(row['_speedup'], row['_quality'], s=100)\n",
    "            plt.annotate(f\"{row['Strategy']} {row['Pruning Level']}\", \n",
    "                         (row['_speedup'], row['_quality']),\n",
    "                         xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        plt.axhline(y=100, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.axvline(x=1, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.title('Quality-Speed Tradeoff Across All Benchmarks')\n",
    "        plt.xlabel('Speedup Factor (×)')\n",
    "        plt.ylabel('Quality Retention (%)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results button\n",
    "view_button = widgets.Button(\n",
    "    description='📊 View Results',\n",
    "    button_style='info',\n",
    "    tooltip='Click to view benchmark results'\n",
    ")\n",
    "\n",
    "results_output = widgets.Output()\n",
    "\n",
    "def on_view_button_clicked(b):\n",
    "    with results_output:\n",
    "        clear_output()\n",
    "        view_results()\n",
    "\n",
    "view_button.on_click(on_view_button_clicked)\n",
    "\n",
    "display(view_button)\n",
    "display(results_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 Conclusions and Next Steps\n",
    "\n",
    "Use this cell to document your findings after running the benchmarks.\n",
    "\n",
    "### Key Findings\n",
    "- What pruning strategies gave the best quality-speed tradeoff?\n",
    "- How much speedup was achievable with acceptable quality loss?\n",
    "- How does pruning level affect performance across different strategies?\n",
    "\n",
    "### Next Steps\n",
    "- Try additional pruning levels or strategies\n",
    "- Test on different model sizes\n",
    "- Compare with agency-based methods\n",
    "- Implement more sophisticated entropy calculations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}