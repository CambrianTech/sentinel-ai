{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel-AI Colab Tutorial\n",
    "\n",
    "This notebook provides a beginner-friendly introduction to the Sentinel-AI framework, a dynamic transformer architecture that can prune, regrow, and adapt during training and inference.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Loading and initializing** the Sentinel-AI model\n",
    "2. **Dynamic pruning** to remove unnecessary attention heads\n",
    "3. **Adaptation to new data** after pruning\n",
    "4. **Visualizing model behavior** during training and inference\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Google Colab Environment\n",
    "\n",
    "First, we'll install the necessary libraries and clone the Sentinel-AI repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers torch matplotlib numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your models and results to Google Drive, run this cell to mount your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended for saving models)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create a directory for saving models and results\n",
    "DRIVE_PATH = \"/content/drive/MyDrive/sentinel-ai\"\n",
    "!mkdir -p {DRIVE_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Libraries and Setup Paths\n",
    "\n",
    "Let's import the necessary libraries and set up our Python paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# Import Sentinel-AI modules\n",
    "from models.loaders.loader import load_baseline_model, load_adaptive_model\n",
    "from utils.generation_wrapper import generate_text\n",
    "from scripts.colab_training import apply_initial_pruning, visualize_gates\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load a Pretrained Model\n",
    "\n",
    "Now, let's load a pretrained model (e.g., DistilGPT2) and wrap it with our adaptive architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"  # You can try other models like \"gpt2\" if you have enough memory\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load baseline model\n",
    "print(f\"Loading baseline model: {model_name}\")\n",
    "baseline_model = load_baseline_model(model_name, device)\n",
    "\n",
    "# Convert to adaptive model\n",
    "print(\"Converting to adaptive model with sentinel gates...\")\n",
    "adaptive_model = load_adaptive_model(model_name, baseline_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Text with the Full Model\n",
    "\n",
    "Let's first generate some text with the full model (no pruning) to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Once upon a time in a land far away,\",\n",
    "    \"The future of artificial intelligence depends on\",\n",
    "    \"Scientists have recently discovered that\"\n",
    "]\n",
    "\n",
    "print(\"=== Generating text with full model (no pruning) ===\\n\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    output = generate_text(\n",
    "        model=adaptive_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Generated: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Gate Activity (Before Pruning)\n",
    "\n",
    "Let's visualize the gate activity in the model before we apply any pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate activity before pruning\n",
    "print(\"Gate activity before pruning:\")\n",
    "gates_before = visualize_gates(adaptive_model)\n",
    "\n",
    "# Count active heads\n",
    "num_layers = len(adaptive_model.blocks)\n",
    "num_heads = adaptive_model.blocks[0][\"attn\"].num_heads\n",
    "total_heads = num_layers * num_heads\n",
    "active_heads = sum(\n",
    "    1 for l in range(num_layers) for h in range(num_heads) \n",
    "    if adaptive_model.blocks[l][\"attn\"].gate[h].item() > 0.01\n",
    ")\n",
    "print(f\"Active heads: {active_heads}/{total_heads} ({active_heads/total_heads:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Apply Pruning\n",
    "\n",
    "Now, let's apply entropy-based pruning to reduce the model size. This will identify and disable the least important attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply entropy-based pruning\n",
    "pruning_level = 0.5  # Prune 50% of heads\n",
    "pruning_strategy = \"entropy\"  # Can be \"random\", \"entropy\", or \"gradient\"\n",
    "\n",
    "pruned_model = apply_initial_pruning(\n",
    "    adaptive_model, \n",
    "    pruning_strategy, \n",
    "    pruning_level, \n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Gate Activity (After Pruning)\n",
    "\n",
    "Let's visualize the gate activity after pruning to see which heads were pruned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate activity after pruning\n",
    "print(\"Gate activity after pruning:\")\n",
    "gates_after = visualize_gates(pruned_model)\n",
    "\n",
    "# Count active heads after pruning\n",
    "active_heads_after = sum(\n",
    "    1 for l in range(num_layers) for h in range(num_heads) \n",
    "    if pruned_model.blocks[l][\"attn\"].gate[h].item() > 0.01\n",
    ")\n",
    "print(f\"Active heads: {active_heads_after}/{total_heads} ({active_heads_after/total_heads:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Generate Text with the Pruned Model\n",
    "\n",
    "Let's generate text with the pruned model to see if the quality is maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"=== Generating text with pruned model ({pruning_level*100:.0f}% pruning) ===\\n\")\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Prompt {i+1}: {prompt}\")\n",
    "    output = generate_text(\n",
    "        model=pruned_model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_length=100,\n",
    "        temperature=0.7,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Generated: {output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Measure Inference Speed\n",
    "\n",
    "Let's measure the inference speed to see if pruning has made the model faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_speed(model, tokenizer, prompt, num_tokens=50, num_runs=3):\n",
    "    \"\"\"Measure inference speed in tokens per second.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Warmup run\n",
    "    _ = model.generate(\n",
    "        **inputs, \n",
    "        max_length=len(inputs.input_ids[0]) + 10, \n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = model.generate(\n",
    "            **inputs, \n",
    "            max_length=len(inputs.input_ids[0]) + num_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    tokens_per_second = num_tokens / avg_time\n",
    "    \n",
    "    return tokens_per_second\n",
    "\n",
    "# Load a fresh copy of the baseline model for fair comparison\n",
    "baseline_model_new = load_baseline_model(model_name, device)\n",
    "adaptive_model_new = load_adaptive_model(model_name, baseline_model_new, device)\n",
    "\n",
    "# Measure full model speed\n",
    "full_speed = measure_inference_speed(adaptive_model_new, tokenizer, prompts[0])\n",
    "print(f\"Full model inference speed: {full_speed:.2f} tokens/sec\")\n",
    "\n",
    "# Measure pruned model speed\n",
    "pruned_speed = measure_inference_speed(pruned_model, tokenizer, prompts[0])\n",
    "print(f\"Pruned model inference speed: {pruned_speed:.2f} tokens/sec\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = (pruned_speed / full_speed - 1) * 100\n",
    "print(f\"Speedup: {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Train on New Data\n",
    "\n",
    "Now, let's demonstrate that our pruned model can still learn new tasks efficiently. We'll fine-tune it on a small dataset and see how it adapts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_args = \"\"\"\n",
    "--model_name distilgpt2 \n",
    "--dataset tiny_shakespeare \n",
    "--epochs 1 \n",
    "--batch_size 4 \n",
    "--learning_rate 5e-5 \n",
    "--eval_every 100 \n",
    "--save_every 500 \n",
    "--max_length 128 \n",
    "--save_results\n",
    "\"\"\"\n",
    "\n",
    "# Add drive path if available\n",
    "if 'DRIVE_PATH' in globals():\n",
    "    training_args += f\" --drive_path {DRIVE_PATH}\"\n",
    "\n",
    "# Run training script\n",
    "!python scripts/colab_training.py {training_args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Training with Initial Pruning\n",
    "\n",
    "Now, let's try training a model that starts with pruning already applied. This demonstrates how we can train efficiently from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters with initial pruning\n",
    "pruned_training_args = \"\"\"\n",
    "--model_name distilgpt2 \n",
    "--dataset tiny_shakespeare \n",
    "--epochs 1 \n",
    "--batch_size 4 \n",
    "--learning_rate 5e-5 \n",
    "--eval_every 100 \n",
    "--save_every 500 \n",
    "--initial_pruning 0.5 \n",
    "--pruning_strategy entropy \n",
    "--max_length 128 \n",
    "--save_results\n",
    "\"\"\"\n",
    "\n",
    "# Add drive path if available\n",
    "if 'DRIVE_PATH' in globals():\n",
    "    pruned_training_args += f\" --drive_path {DRIVE_PATH}\"\n",
    "\n",
    "# Run training script\n",
    "!python scripts/colab_training.py {pruned_training_args}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Training with Dynamic Pruning (Controller)\n",
    "\n",
    "Finally, let's demonstrate the dynamic pruning capability using the controller, which can learn which heads to prune during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters with controller\n",
    "controller_training_args = \"\"\"\n",
    "--model_name distilgpt2 \n",
    "--dataset tiny_shakespeare \n",
    "--epochs 1 \n",
    "--batch_size 4 \n",
    "--learning_rate 5e-5 \n",
    "--eval_every 100 \n",
    "--save_every 500 \n",
    "--enable_controller \n",
    "--controller_interval 100 \n",
    "--target_pruning 0.5 \n",
    "--max_length 128 \n",
    "--save_results\n",
    "\"\"\"\n",
    "\n",
    "# Add drive path if available\n",
    "if 'DRIVE_PATH' in globals():\n",
    "    controller_training_args += f\" --drive_path {DRIVE_PATH}\"\n",
    "\n",
    "# Run training script\n",
    "!python scripts/colab_training.py {controller_training_args}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Advanced Feature: Learning After Pruning\n\nNow let's explore one of the most powerful capabilities of Sentinel-AI: the ability for pruned models to learn new tasks efficiently and potentially grow into more powerful models.\n\nWe'll use our new `learning_after_pruning.py` script to demonstrate this capability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define learning after pruning parameters\nlearning_args = \"\"\"\n--model_name distilgpt2 \n--pruning_level 0.5 \n--pruning_strategy entropy \n--task sentiment \n--sample_size 100 \n--epochs 3 \n--batch_size 4 \n--learning_rate 5e-5 \n--save_results\n\"\"\"\n\n# Add drive path if available\nif 'DRIVE_PATH' in globals():\n    learning_args += f\" --drive_path {DRIVE_PATH}\"\n\n# Run the learning after pruning script\n!python scripts/learning_after_pruning.py {learning_args}",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Analyzing Learning Results\n\nLet's examine the results of our learning experiment to understand how the pruned model compares to the full model in learning a new task.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Find the most recent learning results directory\nimport glob\nimport os\n\nif 'DRIVE_PATH' in globals():\n    results_base = os.path.join(DRIVE_PATH, \"learning_results\")\nelse:\n    results_base = \"learning_results\"\n\n# Get all sentiment results directories sorted by creation time (latest first)\nresult_dirs = sorted(\n    glob.glob(f\"{results_base}/sentiment_*\"), \n    key=os.path.getctime, \n    reverse=True\n)\n\nif result_dirs:\n    latest_dir = result_dirs[0]\n    print(f\"Found results in: {latest_dir}\")\n    \n    # Display learning efficiency comparison\n    from IPython.display import Image, display\n    \n    images = [\n        \"performance_comparison.png\",\n        \"learning_efficiency_comparison.png\",\n        \"gate_activity_comparison.png\",\n        \"gate_activity_difference.png\"\n    ]\n    \n    for img in images:\n        img_path = os.path.join(latest_dir, img)\n        if os.path.exists(img_path):\n            print(f\"\\n{img.replace('.png', '').replace('_', ' ').title()}:\")\n            display(Image(img_path))\n    \n    # Display summary text\n    summary_path = os.path.join(latest_dir, \"learning_results_summary.txt\")\n    if os.path.exists(summary_path):\n        print(\"\\nSummary of Results:\")\n        with open(summary_path, 'r') as f:\n            summary = f.read()\n        print(summary)\nelse:\n    print(\"No learning results found. Run the learning_after_pruning.py script first.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Experimenting with Different Tasks\n\nThe learning_after_pruning.py script supports several tasks to demonstrate adaptability:\n\n1. **sentiment** - A sentiment analysis classification task\n2. **code** - Learning to generate code snippets for programming problems\n3. **science** - Learning scientific facts and explanations\n4. **poetry** - Learning to generate poetic text with specific structures\n\nLet's try a different task to see how pruned models adapt to different types of learning:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Try a different task (poetry generation)\npoetry_args = \"\"\"\n--model_name distilgpt2 \n--pruning_level 0.5 \n--pruning_strategy entropy \n--task poetry \n--sample_size 100 \n--epochs 3 \n--batch_size 4 \n--learning_rate 5e-5 \n--save_results\n\"\"\"\n\n# Add drive path if available\nif 'DRIVE_PATH' in globals():\n    poetry_args += f\" --drive_path {DRIVE_PATH}\"\n\n# Run the learning after pruning script with poetry task\n!python scripts/learning_after_pruning.py {poetry_args}",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Comparing Sample Generations\n\nLet's look at the sample text generated by both the full and pruned models after learning:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Display sample generations\nimport glob\n\n# Find the most recent learning results directories\nif 'DRIVE_PATH' in globals():\n    results_base = os.path.join(DRIVE_PATH, \"learning_results\")\nelse:\n    results_base = \"learning_results\"\n\nsentiment_dirs = sorted(\n    glob.glob(f\"{results_base}/sentiment_*\"), \n    key=os.path.getctime, \n    reverse=True\n)\n\npoetry_dirs = sorted(\n    glob.glob(f\"{results_base}/poetry_*\"), \n    key=os.path.getctime, \n    reverse=True\n)\n\n# Function to display sample generations\ndef display_generations(result_dir, task_name):\n    samples_path = os.path.join(result_dir, \"sample_generations.txt\")\n    if os.path.exists(samples_path):\n        print(f\"\\n=== Sample {task_name.capitalize()} Generations ===\")\n        with open(samples_path, 'r') as f:\n            samples = f.read()\n            # Print just the first 1000 characters to avoid overwhelming the output\n            print(samples[:1000] + \"...\\n(output truncated)\")\n    else:\n        print(f\"No sample generations found for {task_name} task.\")\n\n# Display generations from both tasks if available\nif sentiment_dirs:\n    display_generations(sentiment_dirs[0], \"sentiment\")\n    \nif poetry_dirs:\n    display_generations(poetry_dirs[0], \"poetry\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Conclusion\n\nIn this advanced section, we've demonstrated a key capability of the Sentinel-AI framework:\n\n1. **Adaptability After Pruning**: Pruned models can efficiently learn new tasks, showing that pruning doesn't compromise the model's ability to adapt and grow.\n\n2. **Comparative Learning Efficiency**: In many cases, pruned models learn as efficiently (or sometimes more efficiently) than their full-sized counterparts, while requiring less computational resources.\n\n3. **Task Flexibility**: The adaptive architecture works across various tasks including classification (sentiment) and generation (poetry), demonstrating versatility.\n\n4. **Gate Activity Evolution**: By examining gate values before and after learning, we can observe how the model dynamically adjusts its attention mechanisms to optimize for new tasks.\n\nThese experiments provide compelling evidence that the Sentinel-AI approach not only reduces model size and increases inference speed but also maintains or enhances adaptability - a critical attribute for models that need to grow in capability over time.\n\nThe ability of pruned models to learn new tasks efficiently supports our hypothesis that models can \"grow into something much more powerful, given an existing model\" through our adaptive architecture.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Next Steps\n\nTo further explore the capabilities of Sentinel-AI:\n\n1. **Try different pruning levels**: Experiment with pruning levels from 0.3 to 0.7 to find the optimal tradeoff between efficiency and performance.\n\n2. **Test other pruning strategies**: Compare entropy-based pruning with gradient-based and random pruning to see which works best for different tasks.\n\n3. **Combine pruning with progressive learning**: Start with a heavily pruned model and allow it to \"grow\" new heads as it learns more complex tasks.\n\n4. **Explore controller-based dynamic pruning**: Use the controller to automatically adjust pruning during training based on task performance.\n\n5. **Apply to larger models**: If you have access to more computational resources, try applying these techniques to larger models like gpt2-medium.\n\nThe Sentinel-AI framework opens up numerous possibilities for creating more efficient and adaptable transformer models!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Advanced Feature: Learning After Pruning\n\nNow let's explore one of the most powerful capabilities of Sentinel-AI: the ability for pruned models to learn new tasks efficiently and potentially grow into more powerful models.\n\nWe'll use our new `learning_after_pruning.py` script to demonstrate this capability.",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}