{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d547d0",
   "metadata": {},
   "source": [
    "# Neural Plasticity in Transformer Models (v0.0.57)\n",
    "\n",
    "\n",
    "This notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics. [ID: 0062773a]\n",
    "\n",
    "### Changes in v0.0.57:\n",
    "- Completely refactored neural plasticity code for better modularity\n",
    "- Added Apple Silicon (M1/M2/M3) compatibility fixes\n",
    "- Fixed BLAS/libtorch crash issues in entropy calculation\n",
    "- Improved tensor handling with proper CPU/CUDA management\n",
    "- Added Colab environment detection for conditional optimizations\n",
    "- Enhanced numerical stability in matrix operations\n",
    "- Fixed tensor dimensions handling in pruning functions\n",
    "- Added detailed debugging metrics for troubleshooting\n",
    "\n",
    "## What is Neural Plasticity?\n",
    "\n",
    "Neural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n",
    "\n",
    "In this demo, we:\n",
    "1. Track the entropy and gradient patterns of each attention head\n",
    "2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n",
    "3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n",
    "4. Visualize the \"brain dynamics\" over time\n",
    "\n",
    "This allows models to form more efficient neural structures during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d278b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only needed in Colab\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Install system dependencies in Colab\n",
    "    !apt-get update -qq > /dev/null\n",
    "    !apt-get install -qq libopenblas-dev > /dev/null  # For better performance\n",
    "    print(\"Installed system dependencies for Colab\")\n",
    "else:\n",
    "    print(\"Running locally - skipping Colab-specific system dependencies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b276d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only needed in Colab - has no effect when running locally\n",
    "import os\n",
    "\n",
    "# Check if we're running in Colab\n",
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install required packages in Colab\n",
    "    !pip install -q torch transformers datasets matplotlib seaborn\n",
    "    \n",
    "    # Clone the repository in Colab\n",
    "    !git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n",
    "    %cd sentinel-ai\n",
    "    \n",
    "    # Add repository to path\n",
    "    import sys\n",
    "    sys.path.append('.')\n",
    "else:\n",
    "    # When running locally, we're already in the repository\n",
    "    print(\"Running locally - no need to clone repository\")\n",
    "    \n",
    "    # Make sure all required packages are installed\n",
    "    import importlib\n",
    "    \n",
    "    required_packages = ['torch', 'transformers', 'datasets', 'matplotlib', 'seaborn']\n",
    "    missing_packages = []\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(f\"Warning: The following packages are missing and should be installed: {', '.join(missing_packages)}\")\n",
    "    else:\n",
    "        print(\"All required packages are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be70e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Apple Silicon optimization support\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Detect Apple Silicon\n",
    "IS_APPLE_SILICON = platform.system() == \"Darwin\" and platform.processor() == \"arm\"\n",
    "\n",
    "# Add Apple Silicon optimizations if needed\n",
    "if IS_APPLE_SILICON:\n",
    "    try:\n",
    "        # Import the Apple Silicon optimization utilities\n",
    "        from utils.apple_silicon import apply_tensor_patches, safe_context, ensure_cpu_model\n",
    "        \n",
    "        print(\"üçé Apple Silicon detected - enabling optimizations\")\n",
    "        # Apply tensor safety patches for better stability\n",
    "        apply_tensor_patches()\n",
    "        \n",
    "        # Force PyTorch to use CPU and single-threading for stability\n",
    "        import torch\n",
    "        torch.set_num_threads(1)\n",
    "        \n",
    "        # Also set matplotlib to use Agg backend for better stability\n",
    "        import matplotlib\n",
    "        matplotlib.use('Agg')\n",
    "        print(\"üé® Switched to Agg matplotlib backend for stability\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è Apple Silicon detected but optimization utilities not available\")\n",
    "        print(\"   Some operations may be unstable. Consider installing utils.apple_silicon module.\")\n",
    "        \n",
    "    print(\"‚ÑπÔ∏è When running on Apple Silicon, model operations will be forced to CPU\")\n",
    "    print(\"   This prevents BLAS/libtorch crashes that commonly occur on M1/M2/M3 chips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e557a2d",
   "metadata": {},
   "source": [
    "# Configure the Experiment\n",
    "\n",
    "Let's set up our configuration for the neural plasticity experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf92a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "MODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\n",
    "DATASET = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 100      # Run for many epochs if needed\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 100\n",
    "WARMUP_MAX_EPOCHS = 1     # Maximum number of warmup epochs (will stop earlier if loss stabilizes)\n",
    "EVAL_INTERVAL = 50    # Evaluate every 50 steps\n",
    "VISUALIZATION_INTERVAL = 100  # Show visuals every 100 steps\n",
    "INFERENCE_INTERVAL = 500      # Run inference every 500 steps\n",
    "CHECKPOINT_INTERVAL = 500    # Save checkpoint more frequently (was 1000)\n",
    "MAX_STEPS_PER_EPOCH = None    # Set to a number to limit steps per epoch, or None for unlimited\n",
    "\n",
    "# Set to True to enable continuous training for long periods\n",
    "ENABLE_LONG_TRAINING = False  # Set to False for demo purposes to avoid memory/runtime issues\n",
    "\n",
    "# If ENABLE_LONG_TRAINING is True, run with unlimited steps per epoch\n",
    "# If ENABLE_LONG_TRAINING is False, override to a reasonable limit for demo purposes\n",
    "if not ENABLE_LONG_TRAINING:\n",
    "    MAX_STEPS_PER_EPOCH = 200 # Limit steps per epoch for demo purposes\n",
    "    NUM_EPOCHS = 3            # Limit epochs for demo purposes\n",
    "\n",
    "# Configure pruning mode\n",
    "try:\n",
    "    # First try the new modular structure\n",
    "    from utils.neural_plasticity.core import PruningStrategy\n",
    "    # Define an enum-like class for compatibility\n",
    "    class PruningMode:\n",
    "        ADAPTIVE = \"adaptive\"   # Allows recovery\n",
    "        COMPRESSED = \"compressed\"  # Prevents recovery\n",
    "    \n",
    "    PRUNING_STRATEGY_CLASS = PruningStrategy\n",
    "except ImportError:\n",
    "    try:\n",
    "        # Fall back to the original structure if in Colab\n",
    "        from sentinel.pruning.dual_mode_pruning import PruningMode\n",
    "        PRUNING_STRATEGY_CLASS = PruningMode\n",
    "    except ImportError:\n",
    "        # If all else fails, define a simple enum for the demo\n",
    "        class PruningMode:\n",
    "            ADAPTIVE = \"adaptive\"\n",
    "            COMPRESSED = \"compressed\"\n",
    "        PRUNING_STRATEGY_CLASS = PruningMode\n",
    "        print(\"WARNING: Using simplified pruning mode classes\")\n",
    "\n",
    "# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\n",
    "PRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n",
    "\n",
    "# Configure statistical-based pruning strategy\n",
    "# Instead of fixed thresholds, we'll use percentile-based thresholds\n",
    "ENTROPY_PERCENTILE = 70  # Heads with entropy above the 70th percentile are candidates for pruning\n",
    "GRADIENT_PERCENTILE = 30  # Heads with gradient below the 30th percentile are candidates for pruning\n",
    "PRUNE_PERCENT = 0.1      # Target to prune approximately 10% of heads in each step\n",
    "MIN_ZERO_EPOCHS = 1      # Minimum epochs a head should remain pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89a0d7",
   "metadata": {},
   "source": [
    "# Load Model and Dataset\n",
    "\n",
    "Now we'll load the model and prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e31299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import neural plasticity modules\n",
    "from utils.neural_plasticity.core import (\n",
    "    calculate_head_entropy,\n",
    "    calculate_head_gradients,\n",
    "    generate_pruning_mask,\n",
    "    apply_pruning_mask,\n",
    "    evaluate_model,\n",
    "    IS_APPLE_SILICON,\n",
    "    IS_COLAB,\n",
    "    safe_matmul\n",
    ")\n",
    "\n",
    "from utils.neural_plasticity.visualization import (\n",
    "    visualize_head_entropy,\n",
    "    visualize_head_gradients,\n",
    "    visualize_pruning_decisions,\n",
    "    visualize_training_metrics,\n",
    "    visualize_attention_patterns\n",
    ")\n",
    "\n",
    "# Import visualization utilities\n",
    "from utils.colab.helpers import safe_tensor_imshow\n",
    "\n",
    "# Check if we're running on Apple Silicon or in Colab\n",
    "if IS_APPLE_SILICON:\n",
    "    print(\"üçé Apple Silicon detected - using optimized tensor operations\")\n",
    "if IS_COLAB:\n",
    "    print(\"üåê Running in Google Colab environment\")\n",
    "\n",
    "# Set device - force CPU on Apple Silicon regardless of CUDA availability\n",
    "device = torch.device(\"cpu\") if IS_APPLE_SILICON else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load datasets\n",
    "print(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\n",
    "train_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\n",
    "validation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Add labels for language modeling\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.map(add_labels)\n",
    "validation_dataset = validation_dataset.map(add_labels)\n",
    "\n",
    "# Set format\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "validation_dataset = validation_dataset.with_format(\"torch\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} examples\")\n",
    "\n",
    "# Define unique ID for cache busting\n",
    "unique_id = f\"{hash(TIMESTAMP) % 10000000:08x}\"\n",
    "print(f\"Running modularized neural plasticity code [ID: {unique_id}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5949002",
   "metadata": {},
   "source": [
    "# Define Evaluation Function\n",
    "\n",
    "Let's define a function to evaluate our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b10980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, dataloader, device):\n",
    "    # Evaluate model on the provided dataloader\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Limit evaluation to 10 steps for speed\n",
    "            if total_steps >= 10:\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, device, max_length=100):\n",
    "    # Generate text from the model\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fefe5",
   "metadata": {},
   "source": [
    "## Run Model Warm-up\n",
    "\n",
    "Before measuring baseline performance and applying neural plasticity, we'll run a brief warm-up phase to get initial attention patterns and stabilize metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001756c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for warm-up\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * WARMUP_MAX_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Running warm-up until loss stabilizes (max {WARMUP_MAX_EPOCHS} epochs)...\")\n",
    "\n",
    "# Warm-up training loop\n",
    "model.train()\n",
    "warmup_losses = []\n",
    "warmup_step_losses = []\n",
    "last_loss_decrease = 0\n",
    "patience = 15      # Number of steps with no decrease to consider stabilized\n",
    "min_warmup_steps = 50  # Minimum number of warm-up steps\n",
    "max_warmup_steps = 150  # Maximum number of warm-up steps per epoch\n",
    "\n",
    "# Helper function to calculate if loss has stabilized \n",
    "def is_loss_stabilized(losses, min_steps, patience_steps, window_size=5):\n",
    "    # Check if loss has stabilized\n",
    "    # Not enough steps yet\n",
    "    if len(losses) < min_steps:\n",
    "        return False, 0\n",
    "\n",
    "    # Not enough steps since last decrease\n",
    "    steps_since_decrease = len(losses) - last_loss_decrease\n",
    "    if steps_since_decrease < patience_steps:\n",
    "        return False, steps_since_decrease\n",
    "    \n",
    "    # Check if recent trend is flat or increasing using rolling average\n",
    "    if len(losses) >= window_size * 2:\n",
    "        recent_window = sum(losses[-window_size:]) / window_size\n",
    "        previous_window = sum(losses[-(window_size*2):-window_size]) / window_size\n",
    "        # If recent average is lower than previous, we're still decreasing\n",
    "        if recent_window < previous_window * 0.99:  # Allow 1% variation\n",
    "            return False, steps_since_decrease\n",
    "            \n",
    "    return True, steps_since_decrease\n",
    "\n",
    "try:\n",
    "    for epoch in range(WARMUP_MAX_EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss\n",
    "            loss_val = loss.item()\n",
    "            epoch_loss += loss_val\n",
    "            epoch_steps += 1\n",
    "            warmup_losses.append(loss_val)\n",
    "            \n",
    "            # Check if we've met the minimum steps and loss has stabilized\n",
    "            if len(warmup_losses) > 1:\n",
    "                # Track non-increasing steps\n",
    "                if loss_val <= warmup_losses[-2]:\n",
    "                    last_loss_decrease = len(warmup_losses)\n",
    "                \n",
    "                # For visualization, track a smoothed version (rolling average of 5)\n",
    "                if len(warmup_losses) % 5 == 0:\n",
    "                    avg_loss = sum(warmup_losses[-5:]) / 5\n",
    "                    warmup_step_losses.append(avg_loss)\n",
    "            \n",
    "            # Print progress every 5 steps\n",
    "            if step % 5 == 0:\n",
    "                print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss_val:.4f}\", end='\r",
    "')\n",
    "            \n",
    "            # Check if loss has stabilized\n",
    "            is_stable, steps_without_decrease = is_loss_stabilized(\n",
    "                warmup_losses, min_warmup_steps, patience\n",
    "            )\n",
    "            \n",
    "            if is_stable:\n",
    "                print(f\"\n",
    "Warm-up loss stabilized after {len(warmup_losses)} steps\")\n",
    "                print(f\"Loss has been non-decreasing for {steps_without_decrease} steps\")\n",
    "                break\n",
    "                \n",
    "            # Stop after max_warmup_steps for faster execution in demo\n",
    "            if step >= max_warmup_steps:\n",
    "                print(f\"\n",
    "Reached maximum warm-up steps per epoch ({max_warmup_steps})\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\n",
    "Warm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n",
    "        \n",
    "        # Check if loss has stabilized across epochs\n",
    "        is_stable, steps_without_decrease = is_loss_stabilized(\n",
    "            warmup_losses, min_warmup_steps, patience\n",
    "        )\n",
    "        \n",
    "        if is_stable:\n",
    "            print(f\"Loss has stabilized with {steps_without_decrease} steps without significant decrease.\")\n",
    "            print(f\"Ending warm-up early after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    # Plot warm-up loss\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Raw loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(warmup_losses)\n",
    "    plt.title(\"Warm-up Loss (Raw)\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Smoothed loss if we have enough data\n",
    "    if len(warmup_step_losses) > 1:\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(range(0, len(warmup_step_losses)*5, 5), warmup_step_losses)\n",
    "        plt.title(\"Warm-up Loss (5-step Rolling Average)\")\n",
    "        plt.xlabel(\"Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Add trend line to smoothed plot\n",
    "        from scipy.stats import linregress\n",
    "        x = range(0, len(warmup_step_losses)*5, 5)\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, warmup_step_losses)\n",
    "        plt.plot(x, [slope*xi + intercept for xi in x], 'r--', \n",
    "                 label=f'Trend: slope={slope:.6f}, R¬≤={r_value**2:.2f}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Segment analysis - compare first third vs last third of training\n",
    "    if len(warmup_losses) > 6:\n",
    "        segment_size = len(warmup_losses) // 3\n",
    "        first_segment = warmup_losses[:segment_size]\n",
    "        last_segment = warmup_losses[-segment_size:]\n",
    "        first_avg = sum(first_segment) / len(first_segment)\n",
    "        last_avg = sum(last_segment) / len(last_segment)\n",
    "        \n",
    "        print(f\"\n",
    "Warm-up Segment Analysis:\")\n",
    "        print(f\"First {segment_size} steps average loss: {first_avg:.4f}\")\n",
    "        print(f\"Last {segment_size} steps average loss: {last_avg:.4f}\")\n",
    "        print(f\"Improvement during warm-up: {(1 - last_avg/first_avg)*100:.1f}%\")\n",
    "        \n",
    "        # Calculate if still improving significantly\n",
    "        still_improving = (first_avg - last_avg) / first_avg > 0.01  # More than 1% improvement\n",
    "        print(f\"Is model still significantly improving? {'Yes' if still_improving else 'No'}\")\n",
    "    \n",
    "    # Print warm-up summary\n",
    "    print(f\"\n",
    "Warm-up completed with {len(warmup_losses)} steps across {epoch+1} epochs\")\n",
    "    print(f\"Initial loss: {warmup_losses[0]:.4f}\")\n",
    "    print(f\"Final loss: {warmup_losses[-1]:.4f}\")\n",
    "    print(f\"Overall loss reduction: {(1 - warmup_losses[-1]/warmup_losses[0])*100:.1f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\n",
    "Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780efb1",
   "metadata": {},
   "source": [
    "# Evaluate Baseline Model\n",
    "\n",
    "Now let's measure the baseline performance after warm-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model after warm-up\n",
    "baseline_loss, baseline_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n",
    "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with baseline model\n",
    "prompt = \"Once upon a time\"\n",
    "baseline_text = generate_text(model, tokenizer, prompt, device)\n",
    "print(f\"\n",
    "Prompt: {prompt}\")\n",
    "print(f\"Generated text:\n",
    "{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c5587",
   "metadata": {},
   "source": [
    "## Analyze Attention Patterns\n",
    "\n",
    "Let's look at the attention patterns in the model to understand which heads we might want to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ac2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "batch = next(iter(validation_dataloader))\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "# Run model to get attention patterns\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "\n",
    "# Extract attention tensors\n",
    "attention_tensors = outputs.attentions\n",
    "\n",
    "# Calculate entropy for each attention head\n",
    "entropy_values = {}\n",
    "for layer_idx, layer_attention in enumerate(attention_tensors):\n",
    "    # Use our calculate_head_entropy function from utils.neural_plasticity.core\n",
    "    layer_entropy = calculate_head_entropy(layer_attention)\n",
    "    entropy_values[layer_idx] = layer_entropy\n",
    "\n",
    "# Visualize the entropy heatmap\n",
    "entropy_fig = visualize_head_entropy(\n",
    "    entropy_values=entropy_values,\n",
    "    title=\"Attention Entropy Heatmap\",\n",
    "    min_value=0.0,\n",
    "    annotate=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a878e2b",
   "metadata": {},
   "source": [
    "## Calculate Head Gradients\n",
    "\n",
    "Now we'll calculate the gradient norms for each attention head to identify which ones have the least impact on the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gradient norms for each head\n",
    "grad_norm_values = calculate_head_gradients(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    num_batches=2,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize gradient norms\n",
    "grad_fig = visualize_head_gradients(\n",
    "    grad_norm_values=grad_norm_values,\n",
    "    title=\"Head Gradient Norms\",\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a1293d",
   "metadata": {},
   "source": [
    "## Generate Pruning Mask\n",
    "\n",
    "Based on entropy and gradient values, we'll create a pruning mask to identify which heads to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pruning strategy\n",
    "PRUNING_STRATEGY = \"combined\"  # Options: \"entropy\", \"gradient\", \"random\", \"combined\"\n",
    "\n",
    "# Generate pruning mask\n",
    "pruning_mask = generate_pruning_mask(\n",
    "    grad_norm_values=grad_norm_values,\n",
    "    entropy_values=entropy_values[0],  # Use first layer's entropy as reference\n",
    "    prune_percent=PRUNE_PERCENT,\n",
    "    strategy=PRUNING_STRATEGY\n",
    ")\n",
    "\n",
    "# Visualize pruning mask\n",
    "mask_fig = visualize_pruning_decisions(\n",
    "    grad_norm_values=grad_norm_values,\n",
    "    pruning_mask=pruning_mask,\n",
    "    title=f\"Pruning Decisions ({PRUNING_STRATEGY} strategy, {PRUNE_PERCENT*100:.0f}%)\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Count pruned heads\n",
    "total_heads = pruning_mask.numel()\n",
    "pruned_count = pruning_mask.sum().item()\n",
    "print(f\"Pruning {pruned_count} out of {total_heads} heads ({pruned_count/total_heads*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7dcc66",
   "metadata": {},
   "source": [
    "## Apply Pruning to Model\n",
    "\n",
    "Now we'll apply the pruning mask to the model, zeroing out the weights of the selected heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pruning to the model\n",
    "pruned_heads = apply_pruning_mask(\n",
    "    model=model,\n",
    "    pruning_mask=pruning_mask,\n",
    "    mode=\"zero_weights\"\n",
    ")\n",
    "\n",
    "print(f\"Pruned {len(pruned_heads)} heads:\")\n",
    "for layer, head in pruned_heads[:10]:\n",
    "    print(f\"  Layer {layer}, Head {head}\")\n",
    "    \n",
    "if len(pruned_heads) > 10:\n",
    "    print(f\"  ... and {len(pruned_heads) - 10} more\")\n",
    "\n",
    "# Evaluate pruned model\n",
    "pruned_loss, pruned_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n",
    "print(f\"\n",
    "Pruned model evaluation: Loss = {pruned_loss:.4f}, Perplexity = {pruned_perplexity:.2f}\")\n",
    "print(f\"Baseline:               Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Difference:             {((pruned_loss - baseline_loss) / baseline_loss * 100):+.2f}%\")\n",
    "\n",
    "# Generate text with pruned model\n",
    "pruned_text = generate_text(model, tokenizer, prompt, device)\n",
    "print(f\"\n",
    "Prompt: {prompt}\")\n",
    "print(f\"Generated text with pruned model:\n",
    "{pruned_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9506c890",
   "metadata": {},
   "source": [
    "## Fine-tune the Pruned Model\n",
    "\n",
    "Now let's fine-tune the pruned model to adapt to the reduced structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for fine-tuning\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "global_step = 0\n",
    "training_metrics = {\n",
    "    \"train_loss\": [],\n",
    "    \"eval_loss\": [],\n",
    "    \"perplexity\": [],\n",
    "    \"steps\": []\n",
    "}\n",
    "\n",
    "# Number of fine-tuning steps (keep short for demonstration)\n",
    "fine_tuning_steps = 200\n",
    "eval_every = 40\n",
    "\n",
    "try:\n",
    "    print(f\"Fine-tuning for {fine_tuning_steps} steps...\")\n",
    "    steps_completed = 0\n",
    "    \n",
    "    while steps_completed < fine_tuning_steps:\n",
    "        for batch in train_dataloader:\n",
    "            if steps_completed >= fine_tuning_steps:\n",
    "                break\n",
    "                \n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Track training metrics\n",
    "            steps_completed += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if steps_completed % 10 == 0:\n",
    "                print(f\"Step {steps_completed}/{fine_tuning_steps}, Loss: {loss.item():.4f}\", end='\r",
    "')\n",
    "            \n",
    "            # Evaluate\n",
    "            if steps_completed % eval_every == 0:\n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                eval_loss, eval_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n",
    "                \n",
    "                # Update metrics\n",
    "                training_metrics[\"train_loss\"].append(loss.item())\n",
    "                training_metrics[\"eval_loss\"].append(eval_loss)\n",
    "                training_metrics[\"perplexity\"].append(eval_perplexity)\n",
    "                training_metrics[\"steps\"].append(steps_completed)\n",
    "                \n",
    "                print(f\"\n",
    "Step {steps_completed} - Eval loss: {eval_loss:.4f}, Perplexity: {eval_perplexity:.2f}\")\n",
    "                \n",
    "                # Back to training\n",
    "                model.train()\n",
    "    \n",
    "    print(f\"\n",
    "Fine-tuning completed: {steps_completed} steps\")\n",
    "    \n",
    "    # Plot training metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(training_metrics[\"steps\"], training_metrics[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(training_metrics[\"steps\"], training_metrics[\"eval_loss\"], label=\"Eval Loss\")\n",
    "    plt.title(\"Loss During Fine-tuning\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perplexity plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(training_metrics[\"steps\"], training_metrics[\"perplexity\"], label=\"Perplexity\", color=\"green\")\n",
    "    plt.title(\"Perplexity During Fine-tuning\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\n",
    "Error during fine-tuning: {e}\")\n",
    "\n",
    "# Evaluate the fine-tuned pruned model\n",
    "model.eval()\n",
    "final_loss, final_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n",
    "print(f\"\n",
    "Final evaluation:\n",
    "Baseline: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Pruned:   Loss = {pruned_loss:.4f}, Perplexity = {pruned_perplexity:.2f} ({((pruned_loss - baseline_loss) / baseline_loss * 100):+.2f}%)\")\n",
    "print(f\"Fine-tuned: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f} ({((final_loss - baseline_loss) / baseline_loss * 100):+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff585a",
   "metadata": {},
   "source": [
    "## Generate Text with Fine-tuned Model\n",
    "\n",
    "Let's generate text with our fine-tuned pruned model to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59c682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with various prompts\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The meaning of life is\",\n",
    "    \"In a world where AI\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    finetuned_text = generate_text(model, tokenizer, prompt, device)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated text:\n",
    "{finetuned_text}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4355b0",
   "metadata": {},
   "source": [
    "## Neural Plasticity Summary\n",
    "\n",
    "Our experiment showed how transformer models can be made more efficient through neural plasticity:\n",
    "\n",
    "1. We identified and pruned heads with low gradient impact and high entropy\n",
    "2. After pruning, there was a small initial performance drop\n",
    "3. With fine-tuning, the model adapted to its new structure, recovering most of the performance\n",
    "4. The final model is more efficient, using fewer parameters without significant quality loss\n",
    "\n",
    "This demonstrates that transformer models contain redundancy that can be eliminated, and the pruned model can adapt to function with fewer heads.\n",
    "\n",
    "Key metrics:\n",
    "- Baseline perplexity: baseline_perplexity\n",
    "- After pruning: pruned_perplexity (change%)\n",
    "- After fine-tuning: final_perplexity (change%)\n",
    "- Heads pruned: pruned_count out of total_heads (percentage%)\n",
    "\n",
    "This neural plasticity cycle mimics how biological brains optimize their neural pathways, making it an important step toward more efficient and adaptable AI systems."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
