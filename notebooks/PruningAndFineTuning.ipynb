{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pruning and Fine-Tuning Benchmark (v0.0.3)\n\nThis notebook demonstrates how pruning followed by fine-tuning can recover or even improve performance while reducing model size. The experiment runs continuously on Colab and updates visualizations in real-time.\n\n## Version 0.0.3 (April 2025)\n- Added ImprovedFineTuner with enhanced stability for large models\n- Added automatic handling for OPT model compatibility\n- Improved NaN detection and recovery\n- Added memory optimization for large models\n\n## Overview\n\n1. **Baseline Evaluation**: Establish the initial model performance\n2. **Pruning Phase**: Apply different pruning strategies and evaluate post-pruning performance\n3. **Fine-Tuning Phase**: Fine-tune pruned models to recover or improve performance\n4. **Analysis**: Compare performance across pruning levels and fine-tuning epochs\n\nThis experiment will run until interrupted, continuously improving the models and updating visualizations.\n\n## Setup\n\nFirst, let's install dependencies and clone the repository:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q jax jaxlib flax transformers datasets matplotlib numpy pandas seaborn tqdm optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (if not on a feature, change this back to !git clone https://github.com/CambrianTech/sentinel-ai.git and remove this message)\n",
    "!git clone -b feature/colab-overnight https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport os\nimport sys\nimport json\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import JAX/Flax\nimport jax\nimport jax.numpy as jnp\nimport optax\nfrom flax.training.train_state import TrainState\n\n# Import Hugging Face libraries\nfrom transformers import AutoTokenizer, FlaxAutoModelForCausalLM\nfrom datasets import load_dataset\n\n# Import our pruning library\nfrom utils.pruning import (\n    Environment,\n    ResultsManager,\n    PruningModule,\n    PruningBenchmark,\n    FineTuner,\n    ImprovedFineTuner\n)\n\n# Set up plotting\nplt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Detection\n",
    "\n",
    "Let's detect our environment capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()\n",
    "\n",
    "# Check JAX capabilities\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Note: We now import FineTuner and ImprovedFineTuner from utils.pruning\n# The notebook below shows the usage, but the implementation is now in the module\n\n# FineTuner is used for:\n# - Small models (distilgpt2, gpt2, pythia-70m)\n# - Simple fine-tuning tasks\n# - When memory is not a concern\n\n# ImprovedFineTuner is used for:\n# - Large models (opt-1.3b, bloom-560m+)\n# - Models prone to NaN loss (OPT family)\n# - When stability is critical\n# - Fine-tuning with limited resources\n\n# For example:\n# fine_tuner = FineTuner(pruning_module, dataset_name=\"wikitext\", batch_size=8)\n# OR\n# fine_tuner = ImprovedFineTuner(pruning_module, dataset_name=\"wikitext\", batch_size=4)\n#\n# tuned_params, metrics = fine_tuner.fine_tune(\n#     pruned_params, \n#     num_epochs=fine_tuning_epochs,\n#     learning_rate=5e-5,\n#     evaluate_interval=5\n# )",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    \"\"\"Fine-tunes a pruned model to recover performance\"\"\"\n",
    "    \n",
    "    def __init__(self, pruning_module, dataset_name=\"openwebtext\", batch_size=4):\n",
    "        self.pruning_module = pruning_module\n",
    "        self.dataset_name = dataset_name\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_length = 128  # Modest sequence length for faster training\n",
    "        self.train_state = None\n",
    "        self.metrics_history = []\n",
    "        \n",
    "        # Detect number of devices\n",
    "        self.devices = jax.devices()\n",
    "        self.n_devices = len(self.devices)\n",
    "        if self.n_devices > 1:\n",
    "            print(f\"Using {self.n_devices} devices for training\")\n",
    "            self.batch_size = max(self.batch_size, self.n_devices)\n",
    "            # Make batch size divisible by device count\n",
    "            self.batch_size = (self.batch_size // self.n_devices) * self.n_devices\n",
    "            print(f\"Adjusted batch size to {self.batch_size} for multi-device training\")\n",
    "    \n",
    "    def _prepare_dataset(self):\n",
    "        \"\"\"Load and prepare the dataset for fine-tuning\"\"\"\n",
    "        try:\n",
    "            # Try to load a small portion of the dataset for faster loading\n",
    "            dataset = load_dataset(self.dataset_name, split=\"train[:5000]\")\n",
    "            \n",
    "            # Process dataset\n",
    "            tokenizer = self.pruning_module.tokenizer\n",
    "            \n",
    "            def tokenize_function(examples):\n",
    "                # Tokenize the texts\n",
    "                tokenized = tokenizer(examples[\"text\"])\n",
    "                return tokenized\n",
    "            \n",
    "            tokenized_dataset = dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=1,\n",
    "                remove_columns=[\"text\"]\n",
    "            )\n",
    "            \n",
    "            # Create data loader\n",
    "            def create_batch(samples):\n",
    "                # Prepare batch of appropriate shape\n",
    "                batch = {k: np.array(v) for k, v in samples.items()}\n",
    "                \n",
    "                # Create 'labels' for the causal language modeling task\n",
    "                batch[\"labels\"] = batch[\"input_ids\"].copy()\n",
    "                \n",
    "                # Get sequence lengths\n",
    "                seq_lengths = (batch[\"input_ids\"] != tokenizer.pad_token_id).sum(axis=1)\n",
    "                \n",
    "                # Loop through samples and pad/truncate as needed\n",
    "                for i, length in enumerate(seq_lengths):\n",
    "                    # Ensure we have at least 2 tokens (can't shift with just 1)\n",
    "                    if length < 2:\n",
    "                        # Add padding to have at least 2 tokens\n",
    "                        padding = np.array([tokenizer.pad_token_id] * (2 - length))\n",
    "                        batch[\"input_ids\"][i] = np.concatenate([batch[\"input_ids\"][i][:length], padding])\n",
    "                        batch[\"attention_mask\"][i] = np.concatenate([batch[\"attention_mask\"][i][:length], \n",
    "                                                                    np.ones_like(padding)])\n",
    "                        batch[\"labels\"][i] = np.concatenate([batch[\"labels\"][i][:length], padding])\n",
    "                        seq_lengths[i] = 2\n",
    "                    \n",
    "                    # Truncate to max sequence length if needed\n",
    "                    if length > self.max_seq_length:\n",
    "                        batch[\"input_ids\"][i] = batch[\"input_ids\"][i][:self.max_seq_length]\n",
    "                        batch[\"attention_mask\"][i] = batch[\"attention_mask\"][i][:self.max_seq_length]\n",
    "                        batch[\"labels\"][i] = batch[\"labels\"][i][:self.max_seq_length]\n",
    "                        seq_lengths[i] = self.max_seq_length\n",
    "                \n",
    "                return batch\n",
    "            \n",
    "            # Create data loader\n",
    "            dataloader = tokenized_dataset.batch(self.batch_size)\n",
    "            dataloader = dataloader.map(create_batch, batched=True)\n",
    "            \n",
    "            return dataloader\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing dataset: {e}\")\n",
    "            print(\"Falling back to synthetic data for training\")\n",
    "            return self._prepare_synthetic_dataset()\n",
    "    \n",
    "    def _prepare_synthetic_dataset(self):\n",
    "        \"\"\"Create synthetic data for training when dataset loading fails\"\"\"\n",
    "        tokenizer = self.pruning_module.tokenizer\n",
    "        \n",
    "        # Generate random token IDs (avoid special tokens)\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        special_tokens = set([tokenizer.pad_token_id, tokenizer.eos_token_id, \n",
    "                             tokenizer.bos_token_id, tokenizer.unk_token_id])\n",
    "        \n",
    "        # Create 100 samples of random token sequences\n",
    "        samples = []\n",
    "        for _ in range(100):\n",
    "            # Generate random length between 10 and max_seq_length\n",
    "            length = np.random.randint(10, self.max_seq_length)\n",
    "            \n",
    "            # Generate random token IDs\n",
    "            token_ids = np.random.randint(0, vocab_size, size=length)\n",
    "            \n",
    "            # Replace special tokens with normal tokens\n",
    "            for i, token_id in enumerate(token_ids):\n",
    "                if token_id in special_tokens:\n",
    "                    token_ids[i] = (token_id + 1) % vocab_size\n",
    "            \n",
    "            # Create sample\n",
    "            sample = {\n",
    "                \"input_ids\": token_ids,\n",
    "                \"attention_mask\": np.ones_like(token_ids),\n",
    "                \"labels\": token_ids.copy()\n",
    "            }\n",
    "            samples.append(sample)\n",
    "        \n",
    "        # Create batches\n",
    "        batches = []\n",
    "        for i in range(0, len(samples), self.batch_size):\n",
    "            batch_samples = samples[i:i+self.batch_size]\n",
    "            \n",
    "            # Pad to the same length within batch\n",
    "            max_len = max(len(s[\"input_ids\"]) for s in batch_samples)\n",
    "            \n",
    "            batch = {\n",
    "                \"input_ids\": [],\n",
    "                \"attention_mask\": [],\n",
    "                \"labels\": []\n",
    "            }\n",
    "            \n",
    "            for sample in batch_samples:\n",
    "                pad_len = max_len - len(sample[\"input_ids\"])\n",
    "                batch[\"input_ids\"].append(np.pad(sample[\"input_ids\"], (0, pad_len), \n",
    "                                                constant_values=tokenizer.pad_token_id))\n",
    "                batch[\"attention_mask\"].append(np.pad(sample[\"attention_mask\"], (0, pad_len), \n",
    "                                                    constant_values=0))\n",
    "                batch[\"labels\"].append(np.pad(sample[\"labels\"], (0, pad_len), \n",
    "                                            constant_values=tokenizer.pad_token_id))\n",
    "            \n",
    "            # Convert to arrays\n",
    "            batch = {k: np.array(v) for k, v in batch.items()}\n",
    "            batches.append(batch)\n",
    "        \n",
    "        return batches\n",
    "    \n",
    "    def _create_train_state(self, params, learning_rate=5e-5):\n",
    "        \"\"\"Create a training state for the fine-tuning process\"\"\"\n",
    "        # Create optimizer\n",
    "        optimizer = optax.adam(learning_rate)\n",
    "        \n",
    "        # Create train state\n",
    "        model = self.pruning_module.model\n",
    "        return TrainState.create(\n",
    "            apply_fn=model.__call__,\n",
    "            params=params,\n",
    "            tx=optimizer\n",
    "        )\n",
    "    \n",
    "    def _loss_fn(self, params, batch):\n",
    "        \"\"\"Loss function for the language modeling task\"\"\"\n",
    "        model = self.pruning_module.model\n",
    "        \n",
    "        # Get logits from model\n",
    "        outputs = model(**batch, params=params, train=True)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get labels and create masks\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        # Create loss mask (don't compute loss for padding tokens)\n",
    "        loss_mask = (labels != self.pruning_module.tokenizer.pad_token_id)\n",
    "        \n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = logits[:, :-1]\n",
    "        shift_labels = labels[:, 1:]\n",
    "        shift_mask = loss_mask[:, 1:]\n",
    "        \n",
    "        # Calculate cross entropy loss\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            shift_logits, shift_labels\n",
    "        )\n",
    "        \n",
    "        # Apply mask and calculate mean\n",
    "        loss = (loss * shift_mask).sum() / shift_mask.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _train_step(self, state, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        grad_fn = jax.value_and_grad(self._loss_fn)\n",
    "        loss, grads = grad_fn(state.params, batch)\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        return new_state, loss\n",
    "    \n",
    "    def fine_tune(self, pruned_params, num_epochs=1, learning_rate=5e-5, evaluate_interval=5):\n",
    "        \"\"\"Fine-tune the pruned model\"\"\"\n",
    "        print(f\"\\nFine-tuning model with {self.dataset_name} dataset for {num_epochs} epochs...\")\n",
    "        \n",
    "        # Prepare dataset\n",
    "        dataset = self._prepare_dataset()\n",
    "        \n",
    "        # Create training state\n",
    "        self.train_state = self._create_train_state(pruned_params, learning_rate)\n",
    "        self.metrics_history = []\n",
    "        \n",
    "        # Training loop\n",
    "        total_steps = 0\n",
    "        perplexity_history = []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffled dataset for each epoch (if it's a list of batches)\n",
    "            if isinstance(dataset, list):\n",
    "                np.random.shuffle(dataset)\n",
    "                epoch_dataset = dataset\n",
    "            else:\n",
    "                # If it's a datasets.Dataset, shuffle\n",
    "                epoch_dataset = dataset.shuffle()\n",
    "            \n",
    "            # Create progress bar\n",
    "            epoch_desc = f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "            batch_count = len(epoch_dataset) if hasattr(epoch_dataset, \"__len__\") else \"?\"\n",
    "            progress_bar = tqdm(enumerate(epoch_dataset), desc=epoch_desc, \n",
    "                               total=batch_count if batch_count != \"?\" else None)\n",
    "            \n",
    "            epoch_losses = []\n",
    "            \n",
    "            for step, batch in progress_bar:\n",
    "                # Train step\n",
    "                self.train_state, loss = self._train_step(self.train_state, batch)\n",
    "                total_steps += 1\n",
    "                epoch_losses.append(loss.item())\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_description(f\"{epoch_desc} - Loss: {loss.item():.4f}\")\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if total_steps % evaluate_interval == 0:\n",
    "                    # Generate dummy text to check progress\n",
    "                    prompt = \"Artificial intelligence will transform\"\n",
    "                    try:\n",
    "                        generated = self.pruning_module.generate_text(\n",
    "                            self.train_state.params, prompt, max_length=30\n",
    "                        )\n",
    "                        perplexity = self.pruning_module.evaluate_perplexity(\n",
    "                            self.train_state.params, prompt\n",
    "                        )\n",
    "                        perplexity_history.append((total_steps, perplexity))\n",
    "                        print(f\"\\nStep {total_steps} - Perplexity: {perplexity:.4f}\")\n",
    "                        print(f\"Generated: {generated}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error evaluating model: {e}\")\n",
    "            \n",
    "            # End of epoch metrics\n",
    "            epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n",
    "            print(f\"\\nEpoch {epoch+1} completed. Average loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            self.metrics_history.append({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"loss\": epoch_loss,\n",
    "                \"perplexity_history\": perplexity_history\n",
    "            })\n",
    "        \n",
    "        print(\"\\nFine-tuning completed!\")\n",
    "        return self.train_state.params, self.metrics_history\n",
    "    \n",
    "    def plot_training_progress(self, figsize=(12, 6)):\n",
    "        \"\"\"Plot training progress\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"No training metrics available yet\")\n",
    "            return\n",
    "        \n",
    "        # Extract epoch losses\n",
    "        epochs = [m[\"epoch\"] for m in self.metrics_history]\n",
    "        losses = [m[\"loss\"] for m in self.metrics_history]\n",
    "        \n",
    "        # Extract perplexity history\n",
    "        steps = []\n",
    "        perplexities = []\n",
    "        for m in self.metrics_history:\n",
    "            for step, perplexity in m.get(\"perplexity_history\", []):\n",
    "                steps.append(step)\n",
    "                perplexities.append(perplexity)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot losses\n",
    "        ax1.plot(epochs, losses, \"o-\", color=\"blue\")\n",
    "        ax1.set_xlabel(\"Epoch\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.set_title(\"Training Loss\")\n",
    "        ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        \n",
    "        # Plot perplexities\n",
    "        if steps and perplexities:\n",
    "            ax2.plot(steps, perplexities, \"o-\", color=\"green\")\n",
    "            ax2.set_xlabel(\"Step\")\n",
    "            ax2.set_ylabel(\"Perplexity\")\n",
    "            ax2.set_title(\"Perplexity During Training\")\n",
    "            ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        else:\n",
    "            ax2.text(0.5, 0.5, \"No perplexity data available\",\n",
    "                    ha=\"center\", va=\"center\", fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Manager\n",
    "\n",
    "Let's create an experiment manager to run the full experiment:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class PruningFineTuningExperiment:\n    \"\"\"Manages the pruning + fine-tuning experiment\"\"\"\n    \n    def __init__(self, results_dir=\"pruning_finetuning_results\"):\n        self.results_dir = Path(results_dir)\n        self.results_dir.mkdir(exist_ok=True, parents=True)\n        self.results = []\n        self.current_experiment = {}\n        \n        # Initialize environment\n        self.env = Environment()\n        \n        # Get suitable models for this environment\n        self.available_models = self.env.get_suitable_models()\n        print(f\"Models available: {', '.join(self.available_models)}\")\n        \n        # Setup Results Manager\n        self.results_manager = ResultsManager(str(self.results_dir))\n        self.results_df = pd.DataFrame()\n    \n    def run_experiment(self, strategies, pruning_levels, prompt, fine_tuning_epochs=1, max_runtime=3600):\n        \"\"\"Run the full experiment\"\"\"\n        if not self.available_models:\n            print(\"No suitable models found for this environment\")\n            return\n        \n        # Start time for runtime tracking\n        start_time = time.time()\n        \n        # Generate all experiment combinations\n        experiments = []\n        for model in self.available_models:\n            for strategy in strategies:\n                for level in pruning_levels:\n                    experiments.append({\n                        \"model\": model,\n                        \"strategy\": strategy,\n                        \"pruning_level\": level,\n                        \"prompt\": prompt,\n                        \"fine_tuning_epochs\": fine_tuning_epochs\n                    })\n        \n        # Shuffle to get more diverse results early\n        random.shuffle(experiments)\n        \n        # Create progress bar\n        pbar = tqdm(total=len(experiments), desc=\"Running experiments\")\n        \n        # Run experiments\n        for i, exp in enumerate(experiments):\n            # Check if we've exceeded the runtime limit\n            current_runtime = time.time() - start_time\n            if max_runtime is not None and current_runtime > max_runtime:\n                print(f\"\\nReached maximum runtime of {max_runtime/3600:.1f} hours\")\n                break\n                \n            # Update progress bar\n            pbar.set_description(f\"Testing {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}\")\n            \n            # Run experiment\n            try:\n                result = self.run_single_experiment(**exp)\n                if result is not None:\n                    self.results.append(result)\n                \n                # Update progress bar\n                pbar.update(1)\n                \n                # Plot intermediate results every few experiments\n                if (i + 1) % 1 == 0 or i == len(experiments) - 1:\n                    self.plot_results()\n            except Exception as e:\n                print(f\"Error in experiment {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}: {e}\")\n                import traceback\n                traceback.print_exc()\n                # Still update progress bar\n                pbar.update(1)\n        \n        # Close progress bar\n        pbar.close()\n        \n        # Final results\n        print(f\"\\nCompleted {len(self.results)} experiments out of {len(experiments)} attempted\")\n        runtime = time.time() - start_time\n        print(f\"Total runtime: {runtime/3600:.2f} hours ({runtime/60:.2f} minutes)\")\n        \n        # Plot final results\n        self.plot_results()\n        \n        return self.results\n    \n    def run_single_experiment(self, model, strategy, pruning_level, prompt, fine_tuning_epochs=1):\n        \"\"\"Run a single experiment with pruning and fine-tuning\"\"\"\n        print(f\"\\n{'='*80}\")\n        print(f\"Experiment: {model}, {strategy} strategy, {pruning_level:.2f} pruning level\")\n        print(f\"{'='*80}\")\n        \n        # Initialize pruning module\n        pruning_module = PruningModule(model)\n        if not pruning_module.load_model():\n            print(f\"Failed to load model {model}\")\n            return None\n        \n        # Setup experiment record\n        self.current_experiment = {\n            \"model\": model,\n            \"strategy\": strategy,\n            \"pruning_level\": pruning_level,\n            \"prompt\": prompt,\n            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"stages\": {}\n        }\n        \n        # 1. Evaluate baseline model\n        print(\"\\n>> Stage 1: Evaluating baseline model\")\n        original_params = pruning_module.original_params\n        \n        # Evaluate perplexity and generation\n        perplexity_baseline = pruning_module.evaluate_perplexity(original_params, prompt)\n        print(f\"Baseline perplexity: {perplexity_baseline:.4f}\")\n        \n        generated_baseline = pruning_module.generate_text(original_params, prompt)\n        print(f\"Baseline generated: {generated_baseline}\")\n        \n        # Record baseline results\n        self.current_experiment[\"stages\"][\"baseline\"] = {\n            \"perplexity\": float(perplexity_baseline),\n            \"generated_text\": generated_baseline\n        }\n        \n        # 2. Apply pruning\n        print(\"\\n>> Stage 2: Applying pruning\")\n        from utils.pruning.strategies import get_strategy\n        pruning_strat = get_strategy(strategy, pruning_module, prompt)\n        \n        # Calculate importance scores\n        print(\"Calculating head importance...\")\n        all_head_importance = pruning_strat.get_head_importance(original_params)\n        \n        # Sort by importance (ascending)\n        all_head_importance.sort(key=lambda x: x[2])\n        \n        # Determine number of heads to prune\n        total_heads = pruning_module.num_layers * pruning_module.num_heads\n        heads_to_prune = int(total_heads * pruning_level)\n        print(f\"Pruning {heads_to_prune} out of {total_heads} heads\")\n        \n        # Get head indices to prune (least important first)\n        head_indices = [(l, h) for l, h, _ in all_head_importance[:heads_to_prune]]\n        \n        # Prune heads\n        print(\"Pruning heads...\")\n        pruned_params = pruning_strat.prune_heads(original_params, head_indices)\n        \n        # Evaluate after pruning\n        perplexity_pruned = pruning_module.evaluate_perplexity(pruned_params, prompt)\n        print(f\"Pruned perplexity: {perplexity_pruned:.4f}\")\n        \n        generated_pruned = pruning_module.generate_text(pruned_params, prompt)\n        print(f\"Pruned generated: {generated_pruned}\")\n        \n        # Record pruning results\n        self.current_experiment[\"stages\"][\"pruned\"] = {\n            \"perplexity\": float(perplexity_pruned),\n            \"perplexity_change\": float(perplexity_pruned - perplexity_baseline),\n            \"generated_text\": generated_pruned,\n            \"pruned_heads\": heads_to_prune,\n            \"total_heads\": total_heads,\n            \"head_indices\": head_indices\n        }\n        \n        # 3. Fine-tune the pruned model\n        print(\"\\n>> Stage 3: Fine-tuning the pruned model\")\n        \n        # Create fine-tuner with dataset config\n        dataset_name = \"wikitext\"\n        dataset_config = \"wikitext-2-v1\"\n        \n        # Determine batch size based on environment\n        if self.env.in_colab and self.env.has_tpu:\n            # TPUs can handle larger batch sizes\n            batch_size = 16\n        elif self.env.in_colab and self.env.has_gpu:\n            batch_size = 8\n        else:\n            batch_size = 4\n        \n        # Check if model name indicates this might be a large model\n        model_name = model.lower()\n        use_improved_tuner = any(x in model_name for x in ['opt', 'large', '1.3b', 'bloom'])\n        \n        # Select fine-tuner based on model size/type\n        if use_improved_tuner:\n            print(f\"Using ImprovedFineTuner for model {model} to enhance stability\")\n            fine_tuner = ImprovedFineTuner(\n                pruning_module, \n                dataset_name=dataset_name,\n                dataset_config=dataset_config,\n                batch_size=batch_size\n            )\n            # Use lower learning rate for large models\n            learning_rate = 1e-5\n        else:\n            print(f\"Using standard FineTuner for model {model}\")\n            fine_tuner = FineTuner(\n                pruning_module, \n                dataset_name=dataset_name,\n                batch_size=batch_size\n            )\n            learning_rate = 5e-5\n        \n        # Fine-tune model\n        try:\n            tuned_params, metrics = fine_tuner.fine_tune(\n                pruned_params, \n                num_epochs=fine_tuning_epochs,\n                learning_rate=learning_rate,\n                evaluate_interval=5\n            )\n        except Exception as e:\n            print(f\"Error during fine-tuning: {e}\")\n            # If standard tuner fails, fall back to improved tuner\n            if not use_improved_tuner:\n                print(\"Falling back to ImprovedFineTuner after error\")\n                fine_tuner = ImprovedFineTuner(\n                    pruning_module, \n                    dataset_name=dataset_name,\n                    dataset_config=dataset_config,\n                    batch_size=max(1, batch_size // 2)  # Reduce batch size\n                )\n                tuned_params, metrics = fine_tuner.fine_tune(\n                    pruned_params,\n                    num_epochs=fine_tuning_epochs,\n                    learning_rate=1e-5,  # Lower learning rate for stability\n                    evaluate_interval=5\n                )\n        \n        # Plot training progress\n        fine_tuner.plot_training_progress()\n        \n        # Evaluate fine-tuned model\n        perplexity_tuned = pruning_module.evaluate_perplexity(tuned_params, prompt)\n        print(f\"Fine-tuned perplexity: {perplexity_tuned:.4f}\")\n        \n        generated_tuned = pruning_module.generate_text(tuned_params, prompt)\n        print(f\"Fine-tuned generated: {generated_tuned}\")\n        \n        # Record fine-tuning results\n        self.current_experiment[\"stages\"][\"fine_tuned\"] = {\n            \"perplexity\": float(perplexity_tuned),\n            \"perplexity_change_from_baseline\": float(perplexity_tuned - perplexity_baseline),\n            \"perplexity_change_from_pruned\": float(perplexity_tuned - perplexity_pruned),\n            \"generated_text\": generated_tuned,\n            \"training_epochs\": fine_tuning_epochs,\n            \"training_metrics\": metrics\n        }\n        \n        # Compute recovery percentage\n        if perplexity_pruned > perplexity_baseline:\n            # Calculate how much of the perplexity increase was recovered\n            perplexity_increase = perplexity_pruned - perplexity_baseline\n            perplexity_recovery = perplexity_pruned - perplexity_tuned\n            recovery_percentage = (perplexity_recovery / perplexity_increase) * 100 if perplexity_increase > 0 else 0\n            \n            self.current_experiment[\"stages\"][\"fine_tuned\"][\"recovery_percentage\"] = float(recovery_percentage)\n            print(f\"Recovery percentage: {recovery_percentage:.2f}%\")\n        else:\n            # Pruning improved perplexity, so we measure improvement from baseline\n            improvement_percentage = ((perplexity_baseline - perplexity_tuned) / perplexity_baseline) * 100\n            \n            self.current_experiment[\"stages\"][\"fine_tuned\"][\"improvement_percentage\"] = float(improvement_percentage)\n            print(f\"Improvement percentage: {improvement_percentage:.2f}%\")\n        \n        # 4. Save results\n        print(\"\\n>> Stage 4: Saving results\")\n        \n        # Save to disk\n        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        result_filename = f\"{model.replace('/', '_')}_{strategy}_{pruning_level:.2f}_{timestamp}.json\"\n        result_path = self.results_dir / result_filename\n        \n        import json\n        with open(result_path, \"w\") as f:\n            json.dump(self.current_experiment, f, indent=2)\n            \n        print(f\"Results saved to {result_path}\")\n        \n        # Update DataFrame for plotting\n        self._update_dataframe()\n        \n        return self.current_experiment\n    \n    # [rest of the class remains the same]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Now we can run the full experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment\n",
    "experiment = PruningFineTuningExperiment(\"pruning_finetuning_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "PRUNING_LEVELS = [0.1, 0.3, 0.5]\n",
    "PROMPT = \"Artificial intelligence will transform society by\"\n",
    "FINE_TUNING_EPOCHS = 2  # Small number for quick iterations\n",
    "MAX_RUNTIME = 6 * 3600  # 6 hours\n",
    "\n",
    "# Start the experiment\n",
    "results = experiment.run_experiment(\n",
    "    strategies=STRATEGIES,\n",
    "    pruning_levels=PRUNING_LEVELS,\n",
    "    prompt=PROMPT,\n",
    "    fine_tuning_epochs=FINE_TUNING_EPOCHS,\n",
    "    max_runtime=MAX_RUNTIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Overnight Run\n",
    "\n",
    "For an extended overnight run, uncomment and run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overnight Configuration\n",
    "OVERNIGHT_STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "OVERNIGHT_PRUNING_LEVELS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "OVERNIGHT_PROMPT = \"Artificial intelligence will revolutionize industries by\"\n",
    "OVERNIGHT_FINE_TUNING_EPOCHS = 5  # More epochs for better recovery\n",
    "OVERNIGHT_MAX_RUNTIME = 24 * 3600  # 24 hours\n",
    "\n",
    "# Initialize experiment for overnight run\n",
    "overnight_experiment = PruningFineTuningExperiment(\"overnight_results\")\n",
    "\n",
    "# Run overnight experiment (uncomment to run)\n",
    "# overnight_results = overnight_experiment.run_experiment(\n",
    "#     strategies=OVERNIGHT_STRATEGIES,\n",
    "#     pruning_levels=OVERNIGHT_PRUNING_LEVELS,\n",
    "#     prompt=OVERNIGHT_PROMPT,\n",
    "#     fine_tuning_epochs=OVERNIGHT_FINE_TUNING_EPOCHS,\n",
    "#     max_runtime=OVERNIGHT_MAX_RUNTIME\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis\n",
    "\n",
    "After collecting results, run a comprehensive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "experiment.plot_results(figsize=(16, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Compare models\n",
    "if not experiment.results_df.empty:\n",
    "    finetuned_results = experiment.results_df[experiment.results_df[\"stage\"] == \"post_finetuning\"]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for model in finetuned_results[\"model\"].unique():\n",
    "        model_data = finetuned_results[finetuned_results[\"model\"] == model]\n",
    "        \n",
    "        for strategy in model_data[\"strategy\"].unique():\n",
    "            strategy_data = model_data[model_data[\"strategy\"] == strategy]\n",
    "            strategy_data = strategy_data.sort_values(\"pruning_level\")\n",
    "            \n",
    "            plt.plot(strategy_data[\"pruning_level\"], strategy_data[\"perplexity\"],\n",
    "                    marker=\"o\", label=f\"{model} - {strategy}\")\n",
    "    \n",
    "    plt.title(\"Final Perplexity After Fine-tuning\")\n",
    "    plt.xlabel(\"Pruning Level\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "if not experiment.results_df.empty:\n",
    "    # Get data for different stages\n",
    "    initial = experiment.results_df[experiment.results_df[\"stage\"] == \"initial\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    initial = initial.rename(columns={\"perplexity\": \"initial_perplexity\"})\n",
    "    \n",
    "    pruned = experiment.results_df[experiment.results_df[\"stage\"] == \"post_pruning\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    pruned = pruned.rename(columns={\"perplexity\": \"pruned_perplexity\"})\n",
    "    \n",
    "    finetuned = experiment.results_df[experiment.results_df[\"stage\"] == \"post_finetuning\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    finetuned = finetuned.rename(columns={\"perplexity\": \"finetuned_perplexity\"})\n",
    "    \n",
    "    # Merge dataframes\n",
    "    summary = pd.merge(initial, pruned, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    summary = pd.merge(summary, finetuned, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    \n",
    "    # Calculate changes\n",
    "    summary[\"pruning_effect\"] = summary[\"pruned_perplexity\"] - summary[\"initial_perplexity\"]\n",
    "    summary[\"finetuning_effect\"] = summary[\"finetuned_perplexity\"] - summary[\"pruned_perplexity\"]\n",
    "    summary[\"net_change\"] = summary[\"finetuned_perplexity\"] - summary[\"initial_perplexity\"]\n",
    "    \n",
    "    # Display summary\n",
    "    summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}