{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Adaptive Transformer Training and Evaluation Notebook\n",
        "This notebook supports adaptive training, evaluation, inference, and visualization."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets matplotlib seaborn"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from models.adaptive_transformer import AdaptiveTransformerModel\n",
        "from models.loaders.loader import load_adaptive_model, load_baseline_model\n",
        "from utils.training import compute_loss, evaluate_model, count_active_heads, count_trainable_params\n",
        "from utils.metrics_logger import MetricsLogger\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_name = 'distilgpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "baseline_model = load_baseline_model(model_name, device)\n",
        "adaptive_model = load_adaptive_model(model_name, baseline_model, device)\n",
        "\n",
        "dataset = load_dataset('tiny_shakespeare')['train']\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n",
        "dataset = dataset.map(tokenize, batched=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
        "\n",
        "optimizer = torch.optim.AdamW(adaptive_model.parameters(), lr=2e-5)\n",
        "metrics_logger = MetricsLogger()"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "adaptive_model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = adaptive_model(input_ids)\n",
        "        loss = compute_loss(logits, input_ids)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            adaptive_model.eval()\n",
        "            generated = adaptive_model.generate(input_ids[:1], max_length=50)\n",
        "            print('Epoch', epoch, 'Batch', i, 'Generated:', tokenizer.decode(generated[0], skip_special_tokens=True))\n",
        "            adaptive_model.train()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    val_loss, val_perplexity, _ = evaluate_model(adaptive_model, input_ids, input_ids)\n",
        "    active_heads = count_active_heads(adaptive_model)\n",
        "    param_count = count_trainable_params(adaptive_model)\n",
        "\n",
        "    metrics_logger.log({\n",
        "        'train_loss': avg_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'perplexity': val_perplexity,\n",
        "        'active_heads': active_heads,\n",
        "        'param_count': param_count,\n",
        "    })\n",
        "\n",
        "    print(f'Epoch {epoch}: Loss {avg_loss}, Val Loss {val_loss}, Perplexity {val_perplexity}, Heads {active_heads}, Params {param_count}')"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of metrics\n",
        "metrics = metrics_logger.get_metrics()\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axs[0, 0].plot(metrics['train_loss'], label='Train Loss')\n",
        "axs[0, 0].plot(metrics['val_loss'], label='Validation Loss')\n",
        "axs[0, 0].set_title('Loss Over Epochs')\n",
        "axs[0, 0].legend()\n",
        "\n",
        "axs[0, 1].plot(metrics['perplexity'], label='Perplexity', color='green')\n",
        "axs[0, 1].set_title('Perplexity Over Epochs')\n",
        "axs[0, 1].legend()\n",
        "\n",
        "axs[1, 0].plot(metrics['active_heads'], label='Active Heads', color='purple')\n",
        "axs[1, 0].set_title('Active Heads')\n",
        "axs[1, 0].legend()\n",
        "\n",
        "axs[1, 1].plot(metrics['param_count'], label='Parameter Count', color='orange')\n",
        "axs[1, 1].set_title('Parameter Count')\n",
        "axs[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}