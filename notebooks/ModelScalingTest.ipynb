{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Model Scaling Test\n",
          "\n",
          "Compare adaptive transformer performance across different model sizes like `distilgpt2`, `gpt2`, and `gpt2-medium`. \n",
          "This notebook benchmarks training time, perplexity, and head activity across sizes."
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "execution_count": null,
        "outputs": [],
        "source": [
          "!pip install transformers datasets torch matplotlib"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "execution_count": null,
        "outputs": [],
        "source": [
          "import torch\n",
          "import time\n",
          "import matplotlib.pyplot as plt\n",
          "from transformers import AutoTokenizer\n",
          "from models.loaders.loader import load_adaptive_model, load_baseline_model\n",
          "from datasets import load_dataset\n",
          "from utils.training import compute_loss\n",
          "\n",
          "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
          "model_names = [\"distilgpt2\", \"gpt2\"]  # Extendable to \"gpt2-medium\"\n",
          "prompt = \"The adaptive transformer architecture is\"\n",
          "results = {}"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Benchmark Loop"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "execution_count": null,
        "outputs": [],
        "source": [
          "for model_name in model_names:\n",
          "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
          "    baseline = load_baseline_model(model_name, device)\n",
          "    adaptive = load_adaptive_model(model_name, baseline, device)\n",
          "\n",
          "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
          "    labels = inputs[\"input_ids\"]\n",
          "\n",
          "    start = time.time()\n",
          "    logits = adaptive(**inputs)\n",
          "    duration = time.time() - start\n",
          "    \n",
          "    loss = compute_loss(logits, labels)\n",
          "\n",
          "    active_heads = sum(float(g > 0.1) for block in adaptive.blocks for g in block['attn'].gate)\n",
          "    total_params = sum(p.numel() for p in adaptive.parameters())\n",
          "\n",
          "    results[model_name] = {\n",
          "        \"loss\": loss.item(),\n",
          "        \"inference_time\": duration,\n",
          "        \"active_heads\": active_heads,\n",
          "        \"params\": total_params\n",
          "    }"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Results Summary"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "execution_count": null,
        "outputs": [],
        "source": [
          "import pandas as pd\n",
          "df = pd.DataFrame(results).T\n",
          "df"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Visual Comparison"
        ]
      },
      {
        "cell_type": "code",
        "metadata": {},
        "execution_count": null,
        "outputs": [],
        "source": [
          "df.plot.bar(figsize=(10,6), subplots=True, layout=(2,2), legend=False, title=\"Model Scaling Benchmark\")\n",
          "plt.tight_layout()\n",
          "plt.show()"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "name": "python3",
        "language": "python"
      },
      "language_info": {
        "name": "python",
        "version": "3.10"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 5
  }
  