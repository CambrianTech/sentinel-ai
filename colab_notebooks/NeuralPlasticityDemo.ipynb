{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Plasticity Demo: Dynamic Pruning & Regrowth (v0.0.10)\n\nThis notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics.\n\n## What is Neural Plasticity?\n\nNeural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n\nIn this demo, we:\n1. Track the entropy and gradient patterns of each attention head\n2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n4. Visualize the \"brain dynamics\" over time\n\nThis allows models to form more efficient neural structures during training.\n\n### New in v0.0.10:\n- Enhanced visualization showing gradient norms with pruning/revival status overlays\n- Visual indicators (❌,➕,⚠️) make it easy to spot pruned, revived, and vulnerable heads\n- Full unit test coverage for visualization functionality",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets matplotlib seaborn\n",
    "\n",
    "# Clone the Sentinel AI repository\n",
    "!git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Add repository to path\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Experiment\n",
    "\n",
    "Let's set up our configuration for the neural plasticity experiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure experiment\nMODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\nDATASET = \"wikitext\"\nDATASET_CONFIG = \"wikitext-2-raw-v1\"\nMAX_LENGTH = 128\nBATCH_SIZE = 4\nNUM_EPOCHS = 100\nLEARNING_RATE = 5e-5\nWARMUP_STEPS = 100\nWARMUP_EPOCHS = 1     # Number of epochs to run warmup\nEVAL_INTERVAL = 50    # Evaluate every 50 steps\n\n# Configure pruning mode\nfrom sentinel.pruning.dual_mode_pruning import PruningMode\n\n# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\nPRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n\n# Configure statistical-based pruning strategy\n# Instead of fixed thresholds, we'll use percentile-based thresholds\nENTROPY_PERCENTILE = 70  # Heads with entropy above the 70th percentile are candidates for pruning\nGRADIENT_PERCENTILE = 30  # Heads with gradient below the 30th percentile are candidates for pruning\nPRUNE_PERCENT = 0.1      # Target to prune approximately 10% of heads in each step\nMIN_ZERO_EPOCHS = 1      # Minimum epochs a head should remain pruned"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Model and Dataset\n\nNow we'll load the model and prepare the dataset for training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    default_data_collator,\n    get_linear_schedule_with_warmup\n)\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom sentinel.pruning.plasticity_controller import create_plasticity_controller\nfrom sentinel.pruning.dual_mode_pruning import prune_head_in_model, get_model_info\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load model and tokenizer\nprint(f\"Loading model: {MODEL_NAME}\")\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Set pad token if needed\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load datasets\nprint(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\ntrain_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\nvalidation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n\n# Define tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=MAX_LENGTH\n    )\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nvalidation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Add labels for language modeling\ndef add_labels(examples):\n    examples[\"labels\"] = examples[\"input_ids\"].copy()\n    return examples\n\ntrain_dataset = train_dataset.map(add_labels)\nvalidation_dataset = validation_dataset.map(add_labels)\n\n# Set format\ntrain_dataset = train_dataset.with_format(\"torch\")\nvalidation_dataset = validation_dataset.with_format(\"torch\")\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=default_data_collator\n)\n\nvalidation_dataloader = DataLoader(\n    validation_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=default_data_collator\n)\n\nprint(f\"Train dataset size: {len(train_dataset)} examples\")\nprint(f\"Validation dataset size: {len(validation_dataset)} examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "Let's define a function to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model on the provided dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Limit evaluation to 10 steps for speed\n",
    "            if total_steps >= 10:\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    \"\"\"Generate text from the model.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Warm-up\n",
    "\n",
    "Before measuring baseline performance, we'll run a brief warm-up phase to stabilize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for warm-up\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * WARMUP_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Running warm-up for {WARMUP_EPOCHS} epoch(s)...\")\n",
    "\n",
    "# Warm-up training loop\n",
    "model.train()\n",
    "warmup_losses = []\n",
    "\n",
    "for epoch in range(WARMUP_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        # Print progress every 10 steps\n",
    "        if step % 10 == 0:\n",
    "            warmup_losses.append(loss.item())\n",
    "            print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\\r\", end=\"\")\n",
    "            \n",
    "        # Stop after 50 steps for faster execution in demo\n",
    "        if step >= 50:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nWarm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n",
    "\n",
    "# Plot warm-up loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(warmup_losses)\n",
    "plt.title(\"Warm-up Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Model\n",
    "\n",
    "Now let's measure the baseline performance after warm-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model after warm-up\n",
    "baseline_loss, baseline_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with baseline model\n",
    "prompt = \"Once upon a time\"\n",
    "baseline_text = generate_text(prompt)\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated text:\\n{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Plasticity Controller\n",
    "\n",
    "Now we'll create the plasticity controller that will monitor head metrics and dynamically prune/revive heads during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create a custom statistical pruning function based only on gradients\ndef gradient_based_pruning(grad_norm_values, prune_percent=0.1):\n    \"\"\"\n    Make pruning decisions based only on gradient norms,\n    since entropy values seem to be all zeros.\n    \n    Args:\n        grad_norm_values: Tensor of gradient norm values for all heads\n        prune_percent: Target percentage of heads to prune (0-1)\n        \n    Returns:\n        pruning_mask: Boolean tensor where True indicates a head should be pruned\n    \"\"\"\n    # Flatten tensor for calculating percentiles\n    flat_grad_norm = grad_norm_values.view(-1)\n    \n    # Calculate how many heads we want to prune\n    total_heads = grad_norm_values.numel()\n    target_prune_count = int(total_heads * prune_percent)\n    \n    # Get the indices of the heads with the lowest gradient norms\n    _, indices = torch.topk(flat_grad_norm, k=len(flat_grad_norm)-target_prune_count, largest=True)\n    \n    # Create pruning mask with the lowest gradient heads marked for pruning\n    pruning_mask = torch.ones_like(grad_norm_values, dtype=torch.bool)\n    pruning_mask.view(-1)[indices] = False\n    \n    print(f\"Gradient-based pruning - target: {target_prune_count} heads\")\n    print(f\"Final pruning decision: pruning {pruning_mask.sum().item()} heads\")\n    return pruning_mask\n\n# Create plasticity controller with default thresholds\ncontroller = create_plasticity_controller(\n    model=model,\n    mode=PRUNING_MODE,\n    high_entropy_threshold=0.8,  # These will be ignored by our custom approach\n    low_entropy_threshold=0.4,   # but we need to provide values\n    grad_threshold=1e-3,\n    min_zero_epochs=MIN_ZERO_EPOCHS\n)\n\n# Display initial model stats\ninitial_stats = controller.get_summary()\nprint(f\"Model has {initial_stats['total_heads']} attention heads across {controller.total_layers} layers\")\n\n# Debug: Let's check the actual entropy values we're dealing with\nprint(\"\\nCollecting initial entropy and gradient metrics for debugging...\")\ndebug_entropy, debug_grads = controller.collect_head_metrics(\n    validation_dataloader,\n    num_batches=2\n)\n\n# Calculate statistics to help with threshold setting\nprint(\"\\nEntropy statistics:\")\nprint(f\"Mean entropy: {debug_entropy.mean().item():.4f}\")\nprint(f\"Min entropy: {debug_entropy.min().item():.4f}\")\nprint(f\"Max entropy: {debug_entropy.max().item():.4f}\")\nprint(f\"25th percentile: {torch.quantile(debug_entropy.flatten(), 0.25).item():.4f}\")\nprint(f\"50th percentile: {torch.quantile(debug_entropy.flatten(), 0.5).item():.4f}\")\nprint(f\"75th percentile: {torch.quantile(debug_entropy.flatten(), 0.75).item():.4f}\")\n\nprint(\"\\nGradient norm statistics:\")\nprint(f\"Mean grad norm: {debug_grads.mean().item():.6f}\")\nprint(f\"Min grad norm: {debug_grads.min().item():.6f}\")\nprint(f\"Max grad norm: {debug_grads.max().item():.6f}\")\nprint(f\"25th percentile: {torch.quantile(debug_grads.flatten(), 0.25).item():.6f}\")\nprint(f\"50th percentile: {torch.quantile(debug_grads.flatten(), 0.5).item():.6f}\")\nprint(f\"75th percentile: {torch.quantile(debug_grads.flatten(), 0.75).item():.6f}\")\n\n# Test our gradient-only pruning approach\npruning_mask = gradient_based_pruning(\n    debug_grads, \n    prune_percent=PRUNE_PERCENT\n)\n\n# Visualize which heads would be pruned\nplt.figure(figsize=(10, 6))\nplt.imshow(pruning_mask.detach().cpu().numpy(), cmap='Reds', aspect='auto')\nplt.colorbar(label='Prune')\nplt.title('Gradient-Based Pruning Decisions')\nplt.xlabel('Head Index')\nplt.ylabel('Layer Index')\nplt.tight_layout()\nplt.show()\n\n# Debug attention distribution collection to see why entropy is zero\nprint(\"\\nDebugging attention distributions...\")\ntry:\n    # Try to get attention directly to check if it's working\n    inputs = next(iter(validation_dataloader))\n    inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    \n    if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n        print(f\"Attention shape: {outputs.attentions[0].shape}\")\n        # Check if attention is uniform (which would give zero entropy)\n        attn = outputs.attentions[0]  # First layer's attention\n        first_head = attn[0, 0]  # First batch, first head\n        print(f\"First head attention max: {first_head.max().item():.4f}, min: {first_head.min().item():.4f}\")\n        print(f\"First head attention std: {first_head.std().item():.4f}\")\n        \n        # Check if there's any NaN or inf\n        print(f\"Contains NaN: {torch.isnan(attn).any().item()}\")\n        print(f\"Contains Inf: {torch.isinf(attn).any().item()}\")\n    else:\n        print(\"Model did not return attention outputs\")\nexcept Exception as e:\n    print(f\"Error during attention debugging: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Initial Head Metrics\n",
    "\n",
    "Let's look at the initial entropy and gradient patterns of our attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect initial head metrics\n",
    "entropy_values, grad_norm_values = controller.collect_head_metrics(\n",
    "    validation_dataloader, \n",
    "    num_batches=2\n",
    ")\n",
    "\n",
    "# Plot entropy heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Initial Head Entropy (higher = less focused attention)\")\n",
    "entropy_map = plt.imshow(entropy_values.detach().cpu().numpy(), cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(entropy_map, label=\"Entropy\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot gradient norm heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Initial Head Gradient Norms (higher = more learning)\")\n",
    "grad_map = plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n",
    "plt.colorbar(grad_map, label=\"Gradient Norm\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Neural Plasticity\n",
    "\n",
    "Now let's train the model with neural plasticity enabled, dynamically pruning and reviving attention heads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize training components\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=WARMUP_STEPS, \n    num_training_steps=total_steps\n)\n\n# Initialize metrics tracking\nmetrics_history = {\n    \"train_loss\": [],\n    \"eval_loss\": [],\n    \"pruned_heads\": [],\n    \"revived_heads\": [],\n    \"sparsity\": [],\n    \"step\": []\n}\n\n# Custom function to apply pruning based purely on gradients\ndef apply_gradient_pruning(grad_norm_values):\n    # Get pruning decisions\n    pruning_mask = gradient_based_pruning(\n        grad_norm_values, \n        prune_percent=PRUNE_PERCENT\n    )\n    \n    # Convert to list of (layer, head) tuples for pruning\n    pruned_heads = []\n    for layer in range(controller.total_layers):\n        for head in range(controller.heads_per_layer):\n            if pruning_mask[layer, head]:\n                # Check if head is already pruned\n                if not controller.stats[layer][head]['is_zeroed']:\n                    pruned_heads.append((layer, head))\n    \n    # Apply pruning\n    for layer, head in pruned_heads:\n        result = prune_head_in_model(\n            controller.model, \n            layer, \n            head, \n            mode=controller.mode, \n            verbose=True\n        )\n        if result:\n            # Update controller stats\n            controller.stats[layer][head]['is_zeroed'] = True\n            controller.stats[layer][head]['zeroed_epochs'] = 1\n    \n    # Update controller hooks\n    controller._update_pruning_hooks()\n    \n    return pruned_heads\n\n# Training loop\nglobal_step = 0\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    model.train()\n    \n    epoch_loss = 0.0\n    epoch_steps = 0\n    \n    for step, batch in enumerate(train_dataloader):\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        \n        # Forward pass\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        # Track loss\n        epoch_loss += loss.item()\n        epoch_steps += 1\n        global_step += 1\n        \n        # Periodically evaluate and apply plasticity\n        if global_step % EVAL_INTERVAL == 0:\n            # Evaluate\n            model.eval()\n            eval_loss, eval_perplexity = evaluate_model(model, validation_dataloader)\n            \n            # Collect metrics - we only need gradient norms\n            _, grad_norm_values = controller.collect_head_metrics(\n                validation_dataloader, \n                num_batches=2\n            )\n            \n            # Apply gradient-based pruning\n            pruned_heads = apply_gradient_pruning(grad_norm_values)\n            \n            # In this simplified version, we don't revive heads\n            revived_heads = []\n            \n            # Get model info\n            model_info = get_model_info(model)\n            total_pruned = controller._count_pruned_heads()\n            \n            # Update metrics\n            metrics_history[\"train_loss\"].append(epoch_loss / epoch_steps)\n            metrics_history[\"eval_loss\"].append(eval_loss)\n            metrics_history[\"pruned_heads\"].append(len(pruned_heads))\n            metrics_history[\"revived_heads\"].append(len(revived_heads))\n            metrics_history[\"sparsity\"].append(model_info[\"sparsity\"])\n            metrics_history[\"step\"].append(global_step)\n            \n            # Print status\n            print(f\"  Step {global_step} - Train loss: {epoch_loss / epoch_steps:.4f}, Eval loss: {eval_loss:.4f}\")\n            print(f\"  Pruned: {len(pruned_heads)} heads, Revived: {len(revived_heads)} heads, Total pruned: {total_pruned}\")\n            print(f\"  Sparsity: {model_info['sparsity']:.4f}\")\n            \n            # Reset for next interval\n            epoch_loss = 0.0\n            epoch_steps = 0\n            \n            # Back to training mode\n            model.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress, including loss metrics and head pruning/revival."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training metrics\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n\n# Plot losses\nax1.plot(metrics_history[\"step\"], metrics_history[\"train_loss\"], label=\"Train Loss\")\nax1.plot(metrics_history[\"step\"], metrics_history[\"eval_loss\"], label=\"Eval Loss\")\nax1.set_ylabel(\"Loss\")\nax1.set_title(\"Training and Evaluation Loss\")\nax1.legend()\nax1.grid(True)\n\n# Plot pruning metrics\nax2.bar(metrics_history[\"step\"], metrics_history[\"pruned_heads\"], alpha=0.5, label=\"Pruned Heads\", color=\"blue\")\nax2.bar(metrics_history[\"step\"], metrics_history[\"revived_heads\"], alpha=0.5, label=\"Revived Heads\", color=\"green\")\nax2.set_xlabel(\"Step\")\nax2.set_ylabel(\"Count\")\nax2.set_title(\"Head Pruning and Revival\")\nax2.legend(loc=\"upper left\")\nax2.grid(True)\n\n# Add sparsity line on secondary axis\nax3 = ax2.twinx()\nax3.plot(metrics_history[\"step\"], metrics_history[\"sparsity\"], \"r-\", label=\"Sparsity\")\nax3.set_ylabel(\"Sparsity\")\nax3.legend(loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n\n# Visualize head dynamics\ncontroller.visualize_head_dynamics(metric='entropy')\nplt.show()\n\ncontroller.visualize_head_dynamics(metric='decision')\nplt.show()\n\n# NEW: Visualize gradient norms with pruning/revival overlays\nprint(\"\\nGenerating gradient visualization with pruning/revival overlays...\")\n# Collect latest metrics\n_, latest_grad_norms = controller.collect_head_metrics(\n    validation_dataloader, \n    num_batches=2\n)\n# Generate and display the visualization\ncontroller.visualize_gradients_with_status(\n    grad_norm_values=latest_grad_norms,\n    figsize=(12, 6)\n)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Let's evaluate the final model to see how it compares to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_loss, final_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Final evaluation: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f}\")\n",
    "print(f\"Baseline:         Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Improvement:      {((baseline_loss - final_loss) / baseline_loss * 100):.2f}%\")\n",
    "\n",
    "# Get final summary\n",
    "summary = controller.get_summary()\n",
    "print(\"\\nFinal Controller Summary:\")\n",
    "print(f\"  Total heads: {summary['total_heads']}\")\n",
    "print(f\"  Pruned heads: {summary['pruned_heads']} ({summary['pruning_rate']:.2%})\")\n",
    "print(f\"  Model sparsity: {summary['sparsity']:.4f}\")\n",
    "print(f\"  Model size: {summary['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Final Model\n",
    "\n",
    "Let's generate text with the final model to see if there are any quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with final model\n",
    "final_text = generate_text(prompt)\n",
    "\n",
    "print(\"Baseline Model Output:\")\n",
    "print(baseline_text)\n",
    "print(\"\\nPlasticity-Optimized Model Output:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save the optimized model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"plasticity\", f\"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Prompts\n",
    "\n",
    "Let's try generating text with different prompts to evaluate the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"In a distant galaxy\",\n",
    "    \"The future of AI will be\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nIn this notebook, we demonstrated Sentinel AI's neural plasticity system, which enables transformer models to dynamically prune and revive attention heads during training based on their utility.\n\nKey findings:\n1. The plasticity system successfully pruned high-entropy, low-gradient heads\n2. Some heads were revived when they showed potential for useful learning\n3. The final model achieved comparable quality with fewer active heads\n4. The brain dynamics visualization shows how attention heads evolve over time\n\nThis approach mimics biological neural plasticity, where brains form efficient neural pathways by pruning unused connections and strengthening useful ones.\n\n## Version History\n\n- v0.0.10: Added unit tests for visualization functions to ensure reliability and correctness\n- v0.0.9: Added new gradient visualization with pruning overlays (red X's, green plus, yellow warning)\n- v0.0.8: Fixed entropy calculation issue by implementing gradient-only based pruning\n- v0.0.7: Replaced fixed magic number thresholds with statistical approach using percentile-based pruning\n- v0.0.6: Fixed bug in debug code (removed invalid 'verbose' parameter from collect_head_metrics call)\n- v0.0.5: Significantly more aggressive pruning thresholds (HIGH_ENTROPY_THRESHOLD: 0.6→0.4, LOW_ENTROPY_THRESHOLD: 0.3→0.2, GRAD_THRESHOLD: 5e-5→1e-3)\n- v0.0.4: Adjusted pruning thresholds for more aggressive pruning behavior (HIGH_ENTROPY_THRESHOLD: 0.8→0.6, LOW_ENTROPY_THRESHOLD: 0.4→0.3, GRAD_THRESHOLD: 1e-4→5e-5)\n- v0.0.3: Removed hard-coded 200-step limit to allow full NUM_EPOCHS training\n- v0.0.2: Added warmup phase to get more accurate baseline measurements, improved visualization of head metrics, fixed perplexity calculation issues\n- v0.0.1: Initial implementation of neural plasticity demo",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}