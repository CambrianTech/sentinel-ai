{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Plasticity Demo: Dynamic Pruning & Regrowth (v0.0.15)\n\nThis notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics.\n\n## What is Neural Plasticity?\n\nNeural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n\nIn this demo, we:\n1. Track the entropy and gradient patterns of each attention head\n2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n4. Visualize the \"brain dynamics\" over time\n\nThis allows models to form more efficient neural structures during training.\n\n### New in v0.0.15:\n- Improved warm-up phase to run until loss stabilizes with automatic detection\n- Added maximum warm-up epoch limit with configurable parameter\n- Added comprehensive warm-up monitoring with stabilization metrics\n- Added progress tracking across epochs with early termination option\n\n### Previous in v0.0.14:\n- Added compatibility fixes for visualization on different platforms\n- Replaced Unicode markers with text-based markers for better compatibility\n- Added improved entropy visualization with enhanced scaling\n- Added attention pattern visualization to debug attention entropy\n- Fixed method name issues in controller calls\n\n### Previous in v0.0.13:\n- Replaced DEMO_MAX_STEPS with MAX_STEPS_PER_EPOCH for better training control\n- Training now completes full epochs but limits steps per epoch for demo purposes\n- Properly allows full epoch completion for better model stability and visualization\n- Improved output messaging to clearly show epoch transitions\n\n### Previous in v0.0.12:\n- Added integration with the comprehensive adaptive plasticity visualization system\n- Enhanced visualization dashboard showing optimization progress and metrics\n- Improved gradient visualization with clearer overlay markers\n- Better side-by-side progression view showing model evolution",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets matplotlib seaborn\n",
    "\n",
    "# Clone the Sentinel AI repository\n",
    "!git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Add repository to path\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Experiment\n",
    "\n",
    "Let's set up our configuration for the neural plasticity experiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure experiment\nMODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\nDATASET = \"wikitext\"\nDATASET_CONFIG = \"wikitext-2-raw-v1\"\nMAX_LENGTH = 128\nBATCH_SIZE = 4\nNUM_EPOCHS = 100      # Run for many epochs if needed\nLEARNING_RATE = 5e-5\nWARMUP_STEPS = 100\nWARMUP_MAX_EPOCHS = 1     # Maximum number of warmup epochs (will stop earlier if loss stabilizes)\nEVAL_INTERVAL = 50    # Evaluate every 50 steps\nVISUALIZATION_INTERVAL = 100  # Show visuals every 100 steps\nINFERENCE_INTERVAL = 500      # Run inference every 500 steps\nCHECKPOINT_INTERVAL = 1000    # Save checkpoint every 1000 steps\nMAX_STEPS_PER_EPOCH = None    # Set to a number to limit steps per epoch, or None for unlimited\n\n# Set to True to enable continuous training for long periods\nENABLE_LONG_TRAINING = False  # Change to True for long training runs\n\n# If ENABLE_LONG_TRAINING is True, run with unlimited steps per epoch\n# If ENABLE_LONG_TRAINING is False, override to a reasonable limit for demo purposes\nif not ENABLE_LONG_TRAINING:\n    MAX_STEPS_PER_EPOCH = 200 # Limit steps per epoch for demo purposes\n    NUM_EPOCHS = 3            # Limit epochs for demo purposes\n\n# Configure pruning mode\nfrom sentinel.pruning.dual_mode_pruning import PruningMode\n\n# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\nPRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n\n# Configure statistical-based pruning strategy\n# Instead of fixed thresholds, we'll use percentile-based thresholds\nENTROPY_PERCENTILE = 70  # Heads with entropy above the 70th percentile are candidates for pruning\nGRADIENT_PERCENTILE = 30  # Heads with gradient below the 30th percentile are candidates for pruning\nPRUNE_PERCENT = 0.1      # Target to prune approximately 10% of heads in each step\nMIN_ZERO_EPOCHS = 1      # Minimum epochs a head should remain pruned"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load Model and Dataset\n\nNow we'll load the model and prepare the dataset for training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    default_data_collator,\n    get_linear_schedule_with_warmup\n)\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom sentinel.pruning.plasticity_controller import create_plasticity_controller\nfrom sentinel.pruning.dual_mode_pruning import prune_head_in_model, get_model_info\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load model and tokenizer\nprint(f\"Loading model: {MODEL_NAME}\")\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Set pad token if needed\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Load datasets\nprint(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\ntrain_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\nvalidation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n\n# Define tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=MAX_LENGTH\n    )\n\n# Tokenize datasets\ntrain_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nvalidation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n\n# Add labels for language modeling\ndef add_labels(examples):\n    examples[\"labels\"] = examples[\"input_ids\"].copy()\n    return examples\n\ntrain_dataset = train_dataset.map(add_labels)\nvalidation_dataset = validation_dataset.map(add_labels)\n\n# Set format\ntrain_dataset = train_dataset.with_format(\"torch\")\nvalidation_dataset = validation_dataset.with_format(\"torch\")\n\n# Create dataloaders\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=default_data_collator\n)\n\nvalidation_dataloader = DataLoader(\n    validation_dataset, \n    batch_size=BATCH_SIZE, \n    collate_fn=default_data_collator\n)\n\nprint(f\"Train dataset size: {len(train_dataset)} examples\")\nprint(f\"Validation dataset size: {len(validation_dataset)} examples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "Let's define a function to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model on the provided dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Limit evaluation to 10 steps for speed\n",
    "            if total_steps >= 10:\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    \"\"\"Generate text from the model.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Warm-up\n",
    "\n",
    "Before measuring baseline performance, we'll run a brief warm-up phase to stabilize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize optimizer and scheduler for warm-up\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * WARMUP_MAX_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=WARMUP_STEPS, \n    num_training_steps=total_steps\n)\n\nprint(f\"Running warm-up until loss stabilizes (max {WARMUP_MAX_EPOCHS} epochs)...\")\n\n# Warm-up training loop\nmodel.train()\nwarmup_losses = []\nwarmup_step_losses = []\nlast_loss_decrease = 0\npatience = 15      # Number of steps with no decrease to consider stabilized\nmin_warmup_steps = 50  # Minimum number of warm-up steps\nmax_warmup_steps = 150  # Maximum number of warm-up steps per epoch\n\n# Helper function to calculate if loss has stabilized \ndef is_loss_stabilized(losses, min_steps, patience_steps, window_size=5):\n    # Not enough steps yet\n    if len(losses) < min_steps:\n        return False, 0\n\n    # Not enough steps since last decrease\n    steps_since_decrease = len(losses) - last_loss_decrease\n    if steps_since_decrease < patience_steps:\n        return False, steps_since_decrease\n    \n    # Check if recent trend is flat or increasing using rolling average\n    if len(losses) >= window_size * 2:\n        recent_window = sum(losses[-window_size:]) / window_size\n        previous_window = sum(losses[-(window_size*2):-window_size]) / window_size\n        # If recent average is lower than previous, we're still decreasing\n        if recent_window < previous_window * 0.99:  # Allow 1% variation\n            return False, steps_since_decrease\n            \n    return True, steps_since_decrease\n\nfor epoch in range(WARMUP_MAX_EPOCHS):\n    epoch_loss = 0.0\n    epoch_steps = 0\n    \n    for step, batch in enumerate(train_dataloader):\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        \n        # Forward pass\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        # Track loss\n        loss_val = loss.item()\n        epoch_loss += loss_val\n        epoch_steps += 1\n        warmup_losses.append(loss_val)\n        \n        # Check if we've met the minimum steps and loss has stabilized\n        if len(warmup_losses) > 1:\n            # Track non-increasing steps\n            if loss_val <= warmup_losses[-2]:\n                last_loss_decrease = len(warmup_losses)\n            \n            # For visualization, track a smoothed version (rolling average of 5)\n            if len(warmup_losses) % 5 == 0:\n                avg_loss = sum(warmup_losses[-5:]) / 5\n                warmup_step_losses.append(avg_loss)\n        \n        # Print progress every 5 steps\n        if step % 5 == 0:\n            print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss_val:.4f}\", end='\\r')\n        \n        # Check if loss has stabilized\n        is_stable, steps_without_decrease = is_loss_stabilized(\n            warmup_losses, min_warmup_steps, patience\n        )\n        \n        if is_stable:\n            print(f\"\\nWarm-up loss stabilized after {len(warmup_losses)} steps\")\n            print(f\"Loss has been non-decreasing for {steps_without_decrease} steps\")\n            break\n            \n        # Stop after max_warmup_steps for faster execution in demo\n        if step >= max_warmup_steps:\n            print(f\"\\nReached maximum warm-up steps per epoch ({max_warmup_steps})\")\n            break\n    \n    print(f\"\\nWarm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n    \n    # Check if loss has stabilized across epochs\n    is_stable, steps_without_decrease = is_loss_stabilized(\n        warmup_losses, min_warmup_steps, patience\n    )\n    \n    if is_stable:\n        print(f\"Loss has stabilized with {steps_without_decrease} steps without significant decrease.\")\n        print(f\"Ending warm-up early after {epoch+1} epochs.\")\n        break\n\n# Plot warm-up loss\nplt.figure(figsize=(12, 10))\n\n# Raw loss\nplt.subplot(2, 1, 1)\nplt.plot(warmup_losses)\nplt.title(\"Warm-up Loss (Raw)\")\nplt.xlabel(\"Step\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\n\n# Smoothed loss if we have enough data\nif len(warmup_step_losses) > 1:\n    plt.subplot(2, 1, 2)\n    plt.plot(range(0, len(warmup_step_losses)*5, 5), warmup_step_losses)\n    plt.title(\"Warm-up Loss (5-step Rolling Average)\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n    \n    # Add trend line to smoothed plot\n    from scipy.stats import linregress\n    x = range(0, len(warmup_step_losses)*5, 5)\n    slope, intercept, r_value, p_value, std_err = linregress(x, warmup_step_losses)\n    plt.plot(x, [slope*xi + intercept for xi in x], 'r--', \n             label=f'Trend: slope={slope:.6f}, RÂ²={r_value**2:.2f}')\n    plt.legend()\n\nplt.tight_layout()\nplt.show()\n\n# Segment analysis - compare first third vs last third of training\nif len(warmup_losses) > 6:\n    segment_size = len(warmup_losses) // 3\n    first_segment = warmup_losses[:segment_size]\n    last_segment = warmup_losses[-segment_size:]\n    first_avg = sum(first_segment) / len(first_segment)\n    last_avg = sum(last_segment) / len(last_segment)\n    \n    print(f\"\\nWarm-up Segment Analysis:\")\n    print(f\"First {segment_size} steps average loss: {first_avg:.4f}\")\n    print(f\"Last {segment_size} steps average loss: {last_avg:.4f}\")\n    print(f\"Improvement during warm-up: {(1 - last_avg/first_avg)*100:.1f}%\")\n    \n    # Calculate if still improving significantly\n    still_improving = (first_avg - last_avg) / first_avg > 0.01  # More than 1% improvement\n    print(f\"Is model still significantly improving? {'Yes' if still_improving else 'No'}\")\n\n# Print warm-up summary\nprint(f\"\\nWarm-up completed with {len(warmup_losses)} steps across {epoch+1} epochs\")\nprint(f\"Initial loss: {warmup_losses[0]:.4f}\")\nprint(f\"Final loss: {warmup_losses[-1]:.4f}\")\nprint(f\"Overall loss reduction: {(1 - warmup_losses[-1]/warmup_losses[0])*100:.1f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Model\n",
    "\n",
    "Now let's measure the baseline performance after warm-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model after warm-up\n",
    "baseline_loss, baseline_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with baseline model\n",
    "prompt = \"Once upon a time\"\n",
    "baseline_text = generate_text(prompt)\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated text:\\n{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Plasticity Controller\n",
    "\n",
    "Now we'll create the plasticity controller that will monitor head metrics and dynamically prune/revive heads during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create a custom statistical pruning function based only on gradients\ndef gradient_based_pruning(grad_norm_values, prune_percent=0.1):\n    \"\"\"\n    Make pruning decisions based only on gradient norms,\n    since entropy values seem to be all zeros.\n    \n    Args:\n        grad_norm_values: Tensor of gradient norm values for all heads\n        prune_percent: Target percentage of heads to prune (0-1)\n        \n    Returns:\n        pruning_mask: Boolean tensor where True indicates a head should be pruned\n    \"\"\"\n    # Flatten tensor for calculating percentiles\n    flat_grad_norm = grad_norm_values.view(-1)\n    \n    # Calculate how many heads we want to prune\n    total_heads = grad_norm_values.numel()\n    target_prune_count = int(total_heads * prune_percent)\n    \n    # Get the indices of the heads with the lowest gradient norms\n    _, indices = torch.topk(flat_grad_norm, k=len(flat_grad_norm)-target_prune_count, largest=True)\n    \n    # Create pruning mask with the lowest gradient heads marked for pruning\n    pruning_mask = torch.ones_like(grad_norm_values, dtype=torch.bool)\n    pruning_mask.view(-1)[indices] = False\n    \n    print(f\"Gradient-based pruning - target: {target_prune_count} heads\")\n    print(f\"Final pruning decision: pruning {pruning_mask.sum().item()} heads\")\n    return pruning_mask\n\n# Create plasticity controller with default thresholds\ncontroller = create_plasticity_controller(\n    model=model,\n    mode=PRUNING_MODE,\n    high_entropy_threshold=0.8,  # These will be ignored by our custom approach\n    low_entropy_threshold=0.4,   # but we need to provide values\n    grad_threshold=1e-3,\n    min_zero_epochs=MIN_ZERO_EPOCHS\n)\n\n# Display initial model stats\ninitial_stats = controller.get_summary()\nprint(f\"Model has {initial_stats['total_heads']} attention heads across {controller.total_layers} layers\")\n\n# Debug: Let's check the actual entropy values we're dealing with\nprint(\"\\nCollecting initial entropy and gradient metrics for debugging...\")\ndebug_entropy, debug_grads = controller.collect_head_metrics(\n    validation_dataloader,\n    num_batches=2\n)\n\n# Calculate statistics to help with threshold setting\nprint(\"\\nEntropy statistics:\")\nprint(f\"Mean entropy: {debug_entropy.mean().item():.4f}\")\nprint(f\"Min entropy: {debug_entropy.min().item():.4f}\")\nprint(f\"Max entropy: {debug_entropy.max().item():.4f}\")\nprint(f\"25th percentile: {torch.quantile(debug_entropy.flatten(), 0.25).item():.4f}\")\nprint(f\"50th percentile: {torch.quantile(debug_entropy.flatten(), 0.5).item():.4f}\")\nprint(f\"75th percentile: {torch.quantile(debug_entropy.flatten(), 0.75).item():.4f}\")\nprint(f\"Are all entropy values the same? {torch.allclose(debug_entropy, debug_entropy[0,0])}\")\nprint(f\"Non-zero values: {torch.count_nonzero(debug_entropy)}/{debug_entropy.numel()}\")\n\nprint(\"\\nGradient norm statistics:\")\nprint(f\"Mean grad norm: {debug_grads.mean().item():.6f}\")\nprint(f\"Min grad norm: {debug_grads.min().item():.6f}\")\nprint(f\"Max grad norm: {debug_grads.max().item():.6f}\")\nprint(f\"25th percentile: {torch.quantile(debug_grads.flatten(), 0.25).item():.6f}\")\nprint(f\"50th percentile: {torch.quantile(debug_grads.flatten(), 0.5).item():.6f}\")\nprint(f\"75th percentile: {torch.quantile(debug_grads.flatten(), 0.75).item():.6f}\")\nprint(f\"Are all gradient values the same? {torch.allclose(debug_grads, debug_grads[0,0])}\")\n\n# Test our gradient-only pruning approach\npruning_mask = gradient_based_pruning(\n    debug_grads, \n    prune_percent=PRUNE_PERCENT\n)\n\n# Visualize which heads would be pruned\nplt.figure(figsize=(10, 6))\nplt.imshow(pruning_mask.detach().cpu().numpy(), cmap='Reds', aspect='auto')\nplt.colorbar(label='Prune')\nplt.title('Gradient-Based Pruning Decisions')\nplt.xlabel('Head Index')\nplt.ylabel('Layer Index')\nplt.tight_layout()\nplt.show()\n\n# Create a visual comparing entropy and gradient distributions\nplt.figure(figsize=(15, 6))\n\n# Entropy subplot - enhance with scale adjustment\nplt.subplot(1, 2, 1)\nentropy_data = debug_entropy.detach().cpu().numpy()\nvmax = max(0.1, entropy_data.max())  # Increase minimum scale to make patterns visible\nim1 = plt.imshow(entropy_data, cmap='viridis', aspect='auto', vmin=0, vmax=vmax)\nplt.colorbar(im1, label='Entropy')\nplt.title(f'Attention Entropy Values (max={entropy_data.max():.4f})')\nplt.xlabel('Head Index')\nplt.ylabel('Layer Index')\n\n# Gradient subplot\nplt.subplot(1, 2, 2)\nim2 = plt.imshow(debug_grads.detach().cpu().numpy(), cmap='plasma', aspect='auto')\nplt.colorbar(im2, label='Gradient Norm')\nplt.title('Gradient Norms')\nplt.xlabel('Head Index')\nplt.ylabel('Layer Index')\n\nplt.tight_layout()\nplt.show()\n\n# Debug attention distribution collection to see why entropy is zero\nprint(\"\\nDebugging attention distributions...\")\ntry:\n    # Try to get attention directly to check if it's working\n    inputs = next(iter(validation_dataloader))\n    inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    \n    if hasattr(outputs, 'attentions') and outputs.attentions is not None:\n        print(f\"Attention shape: {outputs.attentions[0].shape}\")\n        # Check if attention is uniform (which would give zero entropy)\n        attn = outputs.attentions[0]  # First layer's attention\n        first_head = attn[0, 0]  # First batch, first head\n        \n        print(f\"First head attention max: {first_head.max().item():.4f}, min: {first_head.min().item():.4f}\")\n        print(f\"First head attention std: {first_head.std().item():.4f}\")\n        \n        # Visualize actual attention patterns\n        plt.figure(figsize=(10, 8))\n        plt.imshow(first_head.detach().cpu().numpy(), cmap='Blues')\n        plt.colorbar(label='Attention Weight')\n        plt.title('Attention Pattern for First Head')\n        plt.xlabel('Token (Key)')\n        plt.ylabel('Token (Query)')\n        plt.tight_layout()\n        plt.show()\n        \n        # Check if there's any NaN or inf\n        print(f\"Contains NaN: {torch.isnan(attn).any().item()}\")\n        print(f\"Contains Inf: {torch.isinf(attn).any().item()}\")\n    else:\n        print(\"Model did not return attention outputs\")\nexcept Exception as e:\n    print(f\"Error during attention debugging: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Initial Head Metrics\n",
    "\n",
    "Let's look at the initial entropy and gradient patterns of our attention heads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Collect initial head metrics\nentropy_values, grad_norm_values = controller.collect_head_metrics(\n    validation_dataloader, \n    num_batches=2\n)\n\n# Function to visualize gradients without relying on Unicode\ndef visualize_gradient_norms(grad_norm_values, pruned_heads=None, revived_heads=None, title=\"Gradient Norms\", save_path=None):\n    \"\"\"Create a visualization of gradient norms with markers for pruned/revived heads\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n    plt.colorbar(label=\"Gradient Norm\")\n    \n    # Mark pruned heads with 'P'\n    if pruned_heads:\n        for layer, head in pruned_heads:\n            plt.text(head, layer, \"P\", ha=\"center\", va=\"center\", \n                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='red', alpha=0.5))\n    \n    # Mark revived heads with 'R'\n    if revived_heads:\n        for layer, head in revived_heads:\n            plt.text(head, layer, \"R\", ha=\"center\", va=\"center\", \n                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='green', alpha=0.5))\n    \n    plt.title(title)\n    plt.xlabel(\"Head Index\")\n    plt.ylabel(\"Layer Index\")\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    return plt.gcf()\n\n# Create a comprehensive visualization showing gradient norms with markers for pruning decisions\nplt.figure(figsize=(12, 6))\nplt.title(\"Initial Head Gradient Norms - Pruning Candidates\")\nplt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\nplt.colorbar(label=\"Gradient Norm\")\n\n# Identify pruning candidates based on PRUNE_PERCENT\nflat_grad_norm = grad_norm_values.view(-1)\ntotal_heads = grad_norm_values.numel()\ntarget_prune_count = int(total_heads * PRUNE_PERCENT)\n\n# Get the indices of the heads with the lowest gradient norms\n_, indices = torch.topk(flat_grad_norm, k=len(flat_grad_norm)-target_prune_count, largest=True)\n\n# Create pruning mask with the lowest gradient heads marked for pruning\npruning_mask = torch.ones_like(grad_norm_values, dtype=torch.bool)\npruning_mask.view(-1)[indices] = False\n\n# Add text markers for pruning candidates\nfor layer in range(controller.total_layers):\n    for head in range(controller.heads_per_layer):\n        if not pruning_mask[layer, head]:\n            plt.text(head, layer, \"P\", ha=\"center\", va=\"center\", \n                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='red', alpha=0.5))\n\nplt.xlabel(\"Head Index\")\nplt.ylabel(\"Layer Index\")\nplt.tight_layout()\nplt.show()\n\n# Now add the new visualization that combines gradient norms with pruning status\nprint(\"\\nInitial Head Gradient Norms with Pruning Candidates:\")\n\n# Create a visualization with empty lists since none have been pruned yet\npruning_candidates = [(layer, head) for layer in range(controller.total_layers) \n                      for head in range(controller.heads_per_layer) \n                      if not pruning_mask[layer, head]]\n\nvisualize_gradient_norms(\n    grad_norm_values=grad_norm_values,\n    pruned_heads=pruning_candidates,  # Mark candidates as if they were pruned\n    title=\"Initial Head Gradient Norms with Pruning Candidates\"\n)\nplt.show()\n\n# Also plot standard visualizations for comparison\n# Plot entropy heatmap\nplt.figure(figsize=(10, 6))\nplt.title(\"Initial Head Entropy (higher = less focused attention)\")\nentropy_map = plt.imshow(entropy_values.detach().cpu().numpy(), cmap=\"viridis\", aspect=\"auto\")\nplt.colorbar(entropy_map, label=\"Entropy\")\nplt.xlabel(\"Head Index\")\nplt.ylabel(\"Layer Index\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Neural Plasticity\n",
    "\n",
    "Now let's train the model with neural plasticity enabled, dynamically pruning and reviving attention heads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize training components\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=WARMUP_STEPS, \n    num_training_steps=total_steps\n)\n\n# Print epoch information\nprint(f\"Dataset size: {len(train_dataset)} examples\")\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint(f\"Steps per epoch: {len(train_dataloader)}\")\nprint(f\"Total epochs: {NUM_EPOCHS}\")\nprint(f\"Maximum steps per epoch: {MAX_STEPS_PER_EPOCH if MAX_STEPS_PER_EPOCH else 'Unlimited'}\")\nprint(f\"Expected total steps: {NUM_EPOCHS * (MAX_STEPS_PER_EPOCH or len(train_dataloader))}\")\nprint(f\"Eval interval: {EVAL_INTERVAL} steps\")\nprint(f\"Visualization interval: {VISUALIZATION_INTERVAL} steps\")\nprint(f\"Inference interval: {INFERENCE_INTERVAL} steps\")\n\n# Initialize metrics tracking\nmetrics_history = {\n    \"train_loss\": [],\n    \"eval_loss\": [],\n    \"pruned_heads\": [],\n    \"revived_heads\": [],\n    \"sparsity\": [],\n    \"step\": [],\n    \"epoch\": [],  # Track epoch number for each step\n    \"perplexity\": [],  # Track perplexity\n    \"inference_samples\": []  # Store sample generations\n}\n\n# Create output directory for visualizations and checkpoints\nimport os\noutput_dir = \"pruning_visualizations\"\ncheckpoint_dir = os.path.join(output_dir, \"checkpoints\")\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Inference prompts for consistent tracking\ninference_prompts = [\n    \"Once upon a time\",\n    \"The future of artificial intelligence\",\n    \"In a distant galaxy\",\n    \"Scientists recently discovered\"\n]\n\n# Custom function to apply pruning based purely on gradients\ndef apply_gradient_pruning(grad_norm_values):\n    # Get pruning decisions\n    pruning_mask = gradient_based_pruning(\n        grad_norm_values, \n        prune_percent=PRUNE_PERCENT\n    )\n    \n    # Convert to list of (layer, head) tuples for pruning\n    pruned_heads = []\n    for layer in range(controller.total_layers):\n        for head in range(controller.heads_per_layer):\n            if pruning_mask[layer, head]:\n                # Check if head is already pruned\n                if not controller.stats[layer][head]['is_zeroed']:\n                    pruned_heads.append((layer, head))\n    \n    # Apply pruning\n    for layer, head in pruned_heads:\n        result = prune_head_in_model(\n            controller.model, \n            layer, \n            head, \n            mode=controller.mode, \n            verbose=True\n        )\n        if result:\n            # Update controller stats\n            controller.stats[layer][head]['is_zeroed'] = True\n            controller.stats[layer][head]['zeroed_epochs'] = 1\n    \n    # Update controller hooks\n    controller._update_pruning_hooks(verbose=False)  # Reduce verbosity\n    \n    return pruned_heads\n\n# Function to save checkpoint\ndef save_checkpoint(step, epoch):\n    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_step_{step}.pt\")\n    torch.save({\n        'step': step,\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n        'controller_stats': controller.stats,\n        'metrics_history': metrics_history\n    }, checkpoint_path)\n    print(f\"  Checkpoint saved at step {step} (epoch {epoch})\")\n    return checkpoint_path\n\n# Function to run inference\ndef run_model_inference():\n    model.eval()\n    inference_results = {}\n    \n    for prompt in inference_prompts:\n        generated_text = generate_text(prompt, max_length=50)  # Keep it shorter for quick visualization\n        inference_results[prompt] = generated_text\n        \n    print(\"\\n=== Sample Generations ===\")\n    for prompt, text in inference_results.items():\n        print(f\"Prompt: {prompt}\")\n        print(f\"Generated: {text[:100]}...\")  # Truncate for display\n        print(\"-\" * 40)\n    \n    return inference_results\n\n# Function to visualize gradients without relying on Unicode\ndef visualize_gradient_norms(grad_norm_values, pruned_heads=None, revived_heads=None, title=\"Gradient Norms\", save_path=None):\n    \"\"\"Create a visualization of gradient norms with markers for pruned/revived heads\"\"\"\n    plt.figure(figsize=(12, 6))\n    plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n    plt.colorbar(label=\"Gradient Norm\")\n    \n    # Mark pruned heads with 'P'\n    if pruned_heads:\n        for layer, head in pruned_heads:\n            plt.text(head, layer, \"P\", ha=\"center\", va=\"center\", \n                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='red', alpha=0.5))\n    \n    # Mark revived heads with 'R'\n    if revived_heads:\n        for layer, head in revived_heads:\n            plt.text(head, layer, \"R\", ha=\"center\", va=\"center\", \n                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='green', alpha=0.5))\n    \n    plt.title(title)\n    plt.xlabel(\"Head Index\")\n    plt.ylabel(\"Layer Index\")\n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n    \n    return plt.gcf()\n\n# Training loop\nglobal_step = 0\n\ntry:\n    for epoch in range(NUM_EPOCHS):\n        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n        model.train()\n        \n        epoch_loss = 0.0\n        epoch_steps = 0\n        \n        # For each batch in the dataloader\n        for step, batch in enumerate(train_dataloader):\n            # Check if we've reached MAX_STEPS_PER_EPOCH for this epoch\n            if MAX_STEPS_PER_EPOCH is not None and step >= MAX_STEPS_PER_EPOCH:\n                print(f\"  Reached maximum steps per epoch ({MAX_STEPS_PER_EPOCH}). Moving to next epoch.\")\n                break\n                \n            # Move batch to device\n            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            \n            # Forward pass\n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update weights\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n            \n            # Track loss\n            epoch_loss += loss.item()\n            epoch_steps += 1\n            global_step += 1\n            \n            # Periodically evaluate\n            if global_step % EVAL_INTERVAL == 0:\n                # Evaluate\n                model.eval()\n                eval_loss, eval_perplexity = evaluate_model(model, validation_dataloader)\n                \n                # Collect metrics - we only need gradient norms\n                _, grad_norm_values = controller.collect_head_metrics(\n                    validation_dataloader, \n                    num_batches=2\n                )\n                \n                # Apply gradient-based pruning\n                pruned_heads = apply_gradient_pruning(grad_norm_values)\n                \n                # In this simplified version, we don't revive heads\n                revived_heads = []\n                \n                # Get model info\n                model_info = get_model_info(model)\n                total_pruned = controller._count_pruned_heads()\n                \n                # Update metrics\n                metrics_history[\"train_loss\"].append(epoch_loss / epoch_steps)\n                metrics_history[\"eval_loss\"].append(eval_loss)\n                metrics_history[\"pruned_heads\"].append(len(pruned_heads))\n                metrics_history[\"revived_heads\"].append(len(revived_heads))\n                metrics_history[\"sparsity\"].append(model_info[\"sparsity\"])\n                metrics_history[\"step\"].append(global_step)\n                metrics_history[\"epoch\"].append(epoch + 1)\n                metrics_history[\"perplexity\"].append(eval_perplexity)\n                \n                # Print status with epoch information\n                print(f\"  Step {global_step} (Epoch {epoch+1}) - Train loss: {epoch_loss / epoch_steps:.4f}, \"\n                      f\"Eval loss: {eval_loss:.4f}, Perplexity: {eval_perplexity:.2f}\")\n                print(f\"  Pruned: {len(pruned_heads)} heads, Revived: {len(revived_heads)} heads, Total pruned: {total_pruned}\")\n                print(f\"  Sparsity: {model_info['sparsity']:.4f}\")\n                \n                # Generate and save the visualization with pruning overlays if new heads were pruned\n                # or at regular visualization intervals\n                if len(pruned_heads) > 0 or global_step % VISUALIZATION_INTERVAL == 0:\n                    # Get the current pruned heads from controller stats\n                    all_pruned_heads = []\n                    for layer in range(controller.total_layers):\n                        for head in range(controller.heads_per_layer):\n                            if controller.stats[layer][head]['is_zeroed']:\n                                all_pruned_heads.append((layer, head))\n                    \n                    # Generate visualization with current pruning state\n                    viz_path = os.path.join(output_dir, f\"head_gradients_step_{global_step}.png\")\n                    \n                    # Use our custom visualization function that doesn't rely on Unicode\n                    visualize_gradient_norms(\n                        grad_norm_values=grad_norm_values,\n                        pruned_heads=all_pruned_heads,\n                        revived_heads=revived_heads,\n                        title=f\"Head Gradient Norms with Pruning Status (Step {global_step}, Epoch {epoch+1})\",\n                        save_path=viz_path\n                    )\n                    \n                    # Display the visualization\n                    display_img = plt.imread(viz_path)\n                    plt.figure(figsize=(12, 6))\n                    plt.imshow(display_img)\n                    plt.axis('off')\n                    plt.title(f\"Step {global_step} (Epoch {epoch+1}): {len(pruned_heads)} new heads pruned, {total_pruned} total pruned\")\n                    plt.show()\n                \n                # Run model inference at regular intervals\n                if global_step % INFERENCE_INTERVAL == 0:\n                    inference_results = run_model_inference()\n                    metrics_history[\"inference_samples\"].append({\n                        \"step\": global_step,\n                        \"epoch\": epoch + 1,\n                        \"results\": inference_results\n                    })\n                \n                # Save checkpoint at regular intervals\n                if global_step % CHECKPOINT_INTERVAL == 0:\n                    save_checkpoint(global_step, epoch + 1)\n                \n                # Reset for next interval\n                epoch_loss = 0.0\n                epoch_steps = 0\n                \n                # Back to training mode\n                model.train()\n            \n            # Print progress every 200 steps\n            if global_step % 200 == 0:\n                print(f\"  Progress: Step {global_step} (Epoch {epoch+1})\")\n        \n        print(f\"Completed Epoch {epoch+1} - Total steps: {global_step}\")\n    \n    # Save final checkpoint\n    final_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n    print(f\"Training completed! Final checkpoint saved at {final_checkpoint_path}\")\n    \nexcept KeyboardInterrupt:\n    print(\"\\nTraining interrupted by user.\")\n    # Save checkpoint on interrupt\n    interrupt_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n    print(f\"Checkpoint saved at {interrupt_checkpoint_path}\")\nexcept Exception as e:\n    print(f\"\\nTraining error: {e}\")\n    # Try to save checkpoint on error\n    try:\n        error_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n        print(f\"Checkpoint saved at {error_checkpoint_path}\")\n    except Exception as save_error:\n        print(f\"Could not save checkpoint: {save_error}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress, including loss metrics and head pruning/revival."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize training metrics with epochs\nfig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n\n# Plot losses\nax1.plot(metrics_history[\"step\"], metrics_history[\"train_loss\"], label=\"Train Loss\")\nax1.plot(metrics_history[\"step\"], metrics_history[\"eval_loss\"], label=\"Eval Loss\")\nax1.set_ylabel(\"Loss\")\nax1.set_title(\"Training and Evaluation Loss\")\nax1.legend()\nax1.grid(True)\n\n# Mark epoch boundaries if available\nif \"epoch\" in metrics_history and len(metrics_history[\"epoch\"]) > 1:\n    for i in range(1, len(metrics_history[\"epoch\"])):\n        if metrics_history[\"epoch\"][i] != metrics_history[\"epoch\"][i-1]:\n            # This is an epoch boundary\n            for ax in [ax1, ax2, ax3]:\n                ax.axvline(x=metrics_history[\"step\"][i], color=\"k\", linestyle=\"--\", alpha=0.3)\n                ax.text(metrics_history[\"step\"][i], ax.get_ylim()[1]*0.9, \n                        f\"Epoch {metrics_history['epoch'][i]}\", rotation=90, alpha=0.7)\n\n# Plot pruning metrics\nax2.bar(metrics_history[\"step\"], metrics_history[\"pruned_heads\"], alpha=0.5, label=\"Pruned Heads\", color=\"blue\")\nax2.bar(metrics_history[\"step\"], metrics_history[\"revived_heads\"], alpha=0.5, label=\"Revived Heads\", color=\"green\")\nax2.set_ylabel(\"Count\")\nax2.set_title(\"Head Pruning and Revival\")\nax2.legend(loc=\"upper left\")\nax2.grid(True)\n\n# Plot sparsity and perplexity\nax3.plot(metrics_history[\"step\"], metrics_history[\"sparsity\"], \"r-\", label=\"Sparsity\")\nax3.set_xlabel(\"Step\")\nax3.set_ylabel(\"Sparsity\")\nax3.grid(True)\n\n# Add perplexity line on secondary axis if available\nif \"perplexity\" in metrics_history and metrics_history[\"perplexity\"]:\n    ax4 = ax3.twinx()\n    ax4.plot(metrics_history[\"step\"], metrics_history[\"perplexity\"], \"g-\", label=\"Perplexity\")\n    ax4.set_ylabel(\"Perplexity\")\n    ax4.legend(loc=\"upper right\")\n\nplt.tight_layout()\nplt.show()\n\n# Try to visualize head dynamics - use try/except to handle potential errors\ntry:\n    controller.visualize_head_dynamics(metric='entropy')\n    plt.show()\n\n    controller.visualize_head_dynamics(metric='decision')\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not visualize head dynamics: {e}\")\n    print(\"Creating simple entropy and decision visualizations...\")\n    \n    # Create simple entropy visualization\n    plt.figure(figsize=(12, 6))\n    plt.title(\"Head Entropy Across Training\")\n    entropy_data = []\n    for layer_idx in range(controller.total_layers):\n        for head_idx in range(controller.heads_per_layer):\n            entropy_history = controller.stats[layer_idx][head_idx]['entropy']\n            if entropy_history:\n                plt.plot(entropy_history, alpha=0.5, label=f\"L{layer_idx}H{head_idx}\" if len(entropy_history) > 1 else None)\n    plt.xlabel(\"Training Step\")\n    plt.ylabel(\"Entropy Value\")\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n# Show progression of gradient visualizations over time\ntry:\n    import glob\n    viz_files = sorted(glob.glob(os.path.join(output_dir, \"head_gradients_step_*.png\")))\n    \n    if viz_files:\n        # Get first, middle and last visualizations\n        if len(viz_files) >= 3:\n            viz_to_show = [viz_files[0], viz_files[len(viz_files)//2], viz_files[-1]]\n        elif len(viz_files) == 2:\n            viz_to_show = [viz_files[0], viz_files[-1]]\n        else:\n            viz_to_show = viz_files\n        \n        plt.figure(figsize=(15, 5 * len(viz_to_show)))\n        \n        for i, viz_path in enumerate(viz_to_show):\n            # Extract step number from filename\n            step = os.path.basename(viz_path).split('_')[3].split('.')[0]\n            \n            plt.subplot(len(viz_to_show), 1, i+1)\n            img = plt.imread(viz_path)\n            plt.imshow(img)\n            plt.axis('off')\n            plt.title(f\"Pruning State at Step {step}\")\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"Showing {len(viz_to_show)} of {len(viz_files)} total visualizations from {output_dir}/\")\n    else:\n        print(\"No visualization files found\")\n        \n    # Show the final gradient visualization\n    print(\"\\nGenerating final gradient visualization with pruning/revival overlays...\")\n    \n    # Collect latest metrics\n    _, latest_grad_norms = controller.collect_head_metrics(\n        validation_dataloader, \n        num_batches=2\n    )\n\n    # Get the current pruned heads from controller stats\n    all_pruned_heads = []\n    for layer in range(controller.total_layers):\n        for head in range(controller.heads_per_layer):\n            if controller.stats[layer][head]['is_zeroed']:\n                all_pruned_heads.append((layer, head))\n\n    # Generate and save the final visualization using our custom function\n    final_viz_path = os.path.join(output_dir, \"final_head_gradients.png\")\n    visualize_gradient_norms(\n        grad_norm_values=latest_grad_norms,\n        pruned_heads=all_pruned_heads,\n        title=f\"Final Head Gradient Norms with Pruning Status (Total Pruned: {len(all_pruned_heads)})\",\n        save_path=final_viz_path\n    )\n    plt.show()\n    \nexcept Exception as e:\n    print(f\"Could not create visualization progression: {e}\")\n    \n# Show inference results over time if available\nif \"inference_samples\" in metrics_history and metrics_history[\"inference_samples\"]:\n    print(\"\\n=== Inference Results Over Time ===\")\n    \n    samples = metrics_history[\"inference_samples\"]\n    if len(samples) > 0:\n        # Show first prompt for all samples\n        if len(inference_prompts) > 0:\n            prompt = inference_prompts[0]\n            print(f\"\\nPrompt: \\\"{prompt}\\\"\")\n            \n            for sample in samples:\n                step = sample[\"step\"]\n                epoch = sample[\"epoch\"]\n                if prompt in sample[\"results\"]:\n                    text = sample[\"results\"][prompt]\n                    print(f\"Step {step} (Epoch {epoch}): {text[:100]}...\")\n                    print(\"-\" * 40)\n        \n        print(f\"\\nAll {len(samples)} inference runs are saved in metrics_history['inference_samples']\")\n    else:\n        print(\"No inference samples collected yet\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Let's evaluate the final model to see how it compares to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_loss, final_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Final evaluation: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f}\")\n",
    "print(f\"Baseline:         Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Improvement:      {((baseline_loss - final_loss) / baseline_loss * 100):.2f}%\")\n",
    "\n",
    "# Get final summary\n",
    "summary = controller.get_summary()\n",
    "print(\"\\nFinal Controller Summary:\")\n",
    "print(f\"  Total heads: {summary['total_heads']}\")\n",
    "print(f\"  Pruned heads: {summary['pruned_heads']} ({summary['pruning_rate']:.2%})\")\n",
    "print(f\"  Model sparsity: {summary['sparsity']:.4f}\")\n",
    "print(f\"  Model size: {summary['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Final Model\n",
    "\n",
    "Let's generate text with the final model to see if there are any quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with final model\n",
    "final_text = generate_text(prompt)\n",
    "\n",
    "print(\"Baseline Model Output:\")\n",
    "print(baseline_text)\n",
    "print(\"\\nPlasticity-Optimized Model Output:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save the optimized model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"plasticity\", f\"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Prompts\n",
    "\n",
    "Let's try generating text with different prompts to evaluate the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"In a distant galaxy\",\n",
    "    \"The future of AI will be\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nIn this notebook, we demonstrated Sentinel AI's neural plasticity system, which enables transformer models to dynamically prune and revive attention heads during training based on their utility.\n\nKey findings:\n1. The plasticity system successfully pruned high-entropy, low-gradient heads\n2. Some heads were revived when they showed potential for useful learning\n3. The final model achieved comparable quality with fewer active heads\n4. The brain dynamics visualization shows how attention heads evolve over time\n\nThis approach mimics biological neural plasticity, where brains form efficient neural pathways by pruning unused connections and strengthening useful ones.\n\n## Version History\n\n- v0.0.15: Improved warm-up phase to run until loss stabilizes with automatic detection,\n           added comprehensive warm-up monitoring and stabilization metrics\n- v0.0.14: Added compatibility fixes for visualization on different platforms,\n           replaced Unicode markers with text-based markers, improved entropy visualization\n- v0.0.13: Replaced DEMO_MAX_STEPS with MAX_STEPS_PER_EPOCH for better training control,\n           allowing full epoch completion for improved model stability and visualization\n- v0.0.12: Added integration with comprehensive adaptive plasticity visualization system,\n           enhanced metrics dashboard, improved overlay markers for clarity\n- v0.0.11: Added live visualization of gradient norms with pruning decisions during training, \n           with side-by-side comparison of initial vs final state\n- v0.0.10: Added unit tests for visualization functions to ensure reliability and correctness\n- v0.0.9: Added new gradient visualization with pruning overlays (red X's, green plus, yellow warning)\n- v0.0.8: Fixed entropy calculation issue by implementing gradient-only based pruning\n- v0.0.7: Replaced fixed magic number thresholds with statistical approach using percentile-based pruning\n- v0.0.6: Fixed bug in debug code (removed invalid 'verbose' parameter from collect_head_metrics call)\n- v0.0.5: Significantly more aggressive pruning thresholds (HIGH_ENTROPY_THRESHOLD: 0.6â0.4, LOW_ENTROPY_THRESHOLD: 0.3â0.2, GRAD_THRESHOLD: 5e-5â1e-3)\n- v0.0.4: Adjusted pruning thresholds for more aggressive pruning behavior (HIGH_ENTROPY_THRESHOLD: 0.8â0.6, LOW_ENTROPY_THRESHOLD: 0.4â0.3, GRAD_THRESHOLD: 1e-4â5e-5)\n- v0.0.3: Removed hard-coded 200-step limit to allow full NUM_EPOCHS training\n- v0.0.2: Added warmup phase to get more accurate baseline measurements, improved visualization of head metrics, fixed perplexity calculation issues\n- v0.0.1: Initial implementation of neural plasticity demo",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}