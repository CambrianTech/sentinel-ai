{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Plasticity Demo: Dynamic Pruning & Regrowth (v0.0.25)\n",
    "\n",
    "This notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics.\n",
    "\n",
    "## What is Neural Plasticity?\n",
    "\n",
    "Neural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n",
    "\n",
    "In this demo, we:\n",
    "1. Track the entropy and gradient patterns of each attention head\n",
    "2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n",
    "3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n",
    "4. Visualize the \"brain dynamics\" over time\n",
    "\n",
    "This allows models to form more efficient neural structures during training.\n",
    "### New in v0.0.25:\n",
    "- Fixed layout issues\n",
    "\n",
    "### New in v0.0.23:\n",
    "- Fixed visualization issues causing excessively large images\n",
    "- Reduced figure sizes and DPI settings\n",
    "- Fixed cell splitting in controller section\n",
    "\n",
    "### New in v0.0.22:\n",
    "- Fixed intro and conclusion section formatting\n",
    "- Fixed cell character encoding issues\n",
    "- Split large cells into focused, manageable sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets matplotlib seaborn\n",
    "\n",
    "# Clone the Sentinel AI repository\n",
    "!git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Add repository to path\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the ExperimentLet's set up our configuration for the neural plasticity experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "MODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\n",
    "DATASET = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 100      # Run for many epochs if needed\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_STEPS = 100\n",
    "WARMUP_MAX_EPOCHS = 1     # Maximum number of warmup epochs (will stop earlier if loss stabilizes)\n",
    "EVAL_INTERVAL = 50    # Evaluate every 50 steps\n",
    "VISUALIZATION_INTERVAL = 100  # Show visuals every 100 steps\n",
    "INFERENCE_INTERVAL = 500      # Run inference every 500 steps\n",
    "CHECKPOINT_INTERVAL = 1000    # Save checkpoint every 1000 steps\n",
    "MAX_STEPS_PER_EPOCH = None    # Set to a number to limit steps per epoch, or None for unlimited\n",
    "\n",
    "# Set to True to enable continuous training for long periods\n",
    "ENABLE_LONG_TRAINING = False  # Change to True for long training runs\n",
    "\n",
    "# If ENABLE_LONG_TRAINING is True, run with unlimited steps per epoch\n",
    "# If ENABLE_LONG_TRAINING is False, override to a reasonable limit for demo purposes\n",
    "if not ENABLE_LONG_TRAINING:\n",
    "    MAX_STEPS_PER_EPOCH = 200 # Limit steps per epoch for demo purposes\n",
    "    NUM_EPOCHS = 3            # Limit epochs for demo purposes\n",
    "\n",
    "# Configure pruning mode\n",
    "from sentinel.pruning.dual_mode_pruning import PruningMode\n",
    "\n",
    "# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\n",
    "PRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n",
    "\n",
    "# Configure statistical-based pruning strategy\n",
    "# Instead of fixed thresholds, we'll use percentile-based thresholds\n",
    "ENTROPY_PERCENTILE = 70  # Heads with entropy above the 70th percentile are candidates for pruning\n",
    "GRADIENT_PERCENTILE = 30  # Heads with gradient below the 30th percentile are candidates for pruning\n",
    "PRUNE_PERCENT = 0.1      # Target to prune approximately 10% of heads in each step\n",
    "MIN_ZERO_EPOCHS = 1      # Minimum epochs a head should remain pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and DatasetNow we'll load the model and prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentinel.pruning.plasticity_controller import create_plasticity_controller\n",
    "from sentinel.pruning.dual_mode_pruning import prune_head_in_model, get_model_info\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load datasets\n",
    "print(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\n",
    "train_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\n",
    "validation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Add labels for language modeling\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.map(add_labels)\n",
    "validation_dataset = validation_dataset.map(add_labels)\n",
    "\n",
    "# Set format\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "validation_dataset = validation_dataset.with_format(\"torch\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Evaluation FunctionLet's define a function to evaluate our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model on the provided dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Limit evaluation to 10 steps for speed\n",
    "            if total_steps >= 10:\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    \"\"\"Generate text from the model.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Warm-up\n",
    "\n",
    "Before measuring baseline performance and applying neural plasticity, we'll run a brief warm-up phase to get initial attention patterns and stabilize metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for warm-up\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * WARMUP_MAX_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Running warm-up until loss stabilizes (max {WARMUP_MAX_EPOCHS} epochs)...\")\n",
    "\n",
    "# Warm-up training loop\n",
    "model.train()\n",
    "warmup_losses = []\n",
    "warmup_step_losses = []\n",
    "last_loss_decrease = 0\n",
    "patience = 15      # Number of steps with no decrease to consider stabilized\n",
    "min_warmup_steps = 50  # Minimum number of warm-up steps\n",
    "max_warmup_steps = 150  # Maximum number of warm-up steps per epoch\n",
    "\n",
    "# Helper function to calculate if loss has stabilized \n",
    "def is_loss_stabilized(losses, min_steps, patience_steps, window_size=5):\n",
    "    # Not enough steps yet\n",
    "    if len(losses) < min_steps:\n",
    "        return False, 0\n",
    "\n",
    "    # Not enough steps since last decrease\n",
    "    steps_since_decrease = len(losses) - last_loss_decrease\n",
    "    if steps_since_decrease < patience_steps:\n",
    "        return False, steps_since_decrease\n",
    "    \n",
    "    # Check if recent trend is flat or increasing using rolling average\n",
    "    if len(losses) >= window_size * 2:\n",
    "        recent_window = sum(losses[-window_size:]) / window_size\n",
    "        previous_window = sum(losses[-(window_size*2):-window_size]) / window_size\n",
    "        # If recent average is lower than previous, we're still decreasing\n",
    "        if recent_window < previous_window * 0.99:  # Allow 1% variation\n",
    "            return False, steps_since_decrease\n",
    "            \n",
    "    return True, steps_since_decrease\n",
    "\n",
    "for epoch in range(WARMUP_MAX_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss\n",
    "        loss_val = loss.item()\n",
    "        epoch_loss += loss_val\n",
    "        epoch_steps += 1\n",
    "        warmup_losses.append(loss_val)\n",
    "        \n",
    "        # Check if we've met the minimum steps and loss has stabilized\n",
    "        if len(warmup_losses) > 1:\n",
    "            # Track non-increasing steps\n",
    "            if loss_val <= warmup_losses[-2]:\n",
    "                last_loss_decrease = len(warmup_losses)\n",
    "            \n",
    "            # For visualization, track a smoothed version (rolling average of 5)\n",
    "            if len(warmup_losses) % 5 == 0:\n",
    "                avg_loss = sum(warmup_losses[-5:]) / 5\n",
    "                warmup_step_losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress every 5 steps\n",
    "        if step % 5 == 0:\n",
    "            print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss_val:.4f}\", end='\\r')\n",
    "        \n",
    "        # Check if loss has stabilized\n",
    "        is_stable, steps_without_decrease = is_loss_stabilized(\n",
    "            warmup_losses, min_warmup_steps, patience\n",
    "        )\n",
    "        \n",
    "        if is_stable:\n",
    "            print(f\"\\nWarm-up loss stabilized after {len(warmup_losses)} steps\")\n",
    "            print(f\"Loss has been non-decreasing for {steps_without_decrease} steps\")\n",
    "            break\n",
    "            \n",
    "        # Stop after max_warmup_steps for faster execution in demo\n",
    "        if step >= max_warmup_steps:\n",
    "            print(f\"\\nReached maximum warm-up steps per epoch ({max_warmup_steps})\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nWarm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n",
    "    \n",
    "    # Check if loss has stabilized across epochs\n",
    "    is_stable, steps_without_decrease = is_loss_stabilized(\n",
    "        warmup_losses, min_warmup_steps, patience\n",
    "    )\n",
    "    \n",
    "    if is_stable:\n",
    "        print(f\"Loss has stabilized with {steps_without_decrease} steps without significant decrease.\")\n",
    "        print(f\"Ending warm-up early after {epoch+1} epochs.\")\n",
    "        break\n",
    "\n",
    "# Plot warm-up loss\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Raw loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(warmup_losses)\n",
    "plt.title(\"Warm-up Loss (Raw)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Smoothed loss if we have enough data\n",
    "if len(warmup_step_losses) > 1:\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(0, len(warmup_step_losses)*5, 5), warmup_step_losses)\n",
    "    plt.title(\"Warm-up Loss (5-step Rolling Average)\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add trend line to smoothed plot\n",
    "    from scipy.stats import linregress\n",
    "    x = range(0, len(warmup_step_losses)*5, 5)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x, warmup_step_losses)\n",
    "    plt.plot(x, [slope*xi + intercept for xi in x], 'r--', \n",
    "             label=f'Trend: slope={slope:.6f}, R²={r_value**2:.2f}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Segment analysis - compare first third vs last third of training\n",
    "if len(warmup_losses) > 6:\n",
    "    segment_size = len(warmup_losses) // 3\n",
    "    first_segment = warmup_losses[:segment_size]\n",
    "    last_segment = warmup_losses[-segment_size:]\n",
    "    first_avg = sum(first_segment) / len(first_segment)\n",
    "    last_avg = sum(last_segment) / len(last_segment)\n",
    "    \n",
    "    print(f\"\\nWarm-up Segment Analysis:\")\n",
    "    print(f\"First {segment_size} steps average loss: {first_avg:.4f}\")\n",
    "    print(f\"Last {segment_size} steps average loss: {last_avg:.4f}\")\n",
    "    print(f\"Improvement during warm-up: {(1 - last_avg/first_avg)*100:.1f}%\")\n",
    "    \n",
    "    # Calculate if still improving significantly\n",
    "    still_improving = (first_avg - last_avg) / first_avg > 0.01  # More than 1% improvement\n",
    "    print(f\"Is model still significantly improving? {'Yes' if still_improving else 'No'}\")\n",
    "\n",
    "# Print warm-up summary\n",
    "print(f\"\\nWarm-up completed with {len(warmup_losses)} steps across {epoch+1} epochs\")\n",
    "print(f\"Initial loss: {warmup_losses[0]:.4f}\")\n",
    "print(f\"Final loss: {warmup_losses[-1]:.4f}\")\n",
    "print(f\"Overall loss reduction: {(1 - warmup_losses[-1]/warmup_losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Evaluate Baseline ModelNow let's measure the baseline performance after warm-u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model after warm-up\n",
    "baseline_loss, baseline_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with baseline model\n",
    "prompt = \"Once upon a time\"\n",
    "baseline_text = generate_text(prompt)\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated text:\\n{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Plasticity Controller\n",
    "\n",
    "Now we'll create our neural plasticity controller that will monitor attention heads and make pruning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom statistical pruning function based only on gradients\n",
    "def gradient_based_pruning(grad_norm_values, prune_percent=0.1):\n",
    "    \"\"\"\n",
    "    Make pruning decisions based only on gradient norms.\n",
    "    We want to prune heads with LOWEST gradient norms, as they're\n",
    "    learning the least.\n",
    "    \n",
    "    Args:\n",
    "        grad_norm_values: Tensor of gradient norm values for all heads\n",
    "        prune_percent: Target percentage of heads to prune (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        pruning_mask: Boolean tensor where True indicates a head should be pruned\n",
    "    \"\"\"\n",
    "    # Flatten tensor for calculating percentiles\n",
    "    flat_grad_norm = grad_norm_values.view(-1)\n",
    "    \n",
    "    # Calculate how many heads we want to prune\n",
    "    total_heads = grad_norm_values.numel()\n",
    "    target_prune_count = int(total_heads * prune_percent)\n",
    "    \n",
    "    # Get the indices of the heads with the LOWEST gradient norms\n",
    "    # Here's the fix: we use largest=False to get the lowest values\n",
    "    _, indices = torch.topk(flat_grad_norm, k=target_prune_count, largest=False)\n",
    "    \n",
    "    # Create pruning mask where True = head should be pruned (low gradient norm)\n",
    "    pruning_mask = torch.zeros_like(grad_norm_values, dtype=torch.bool)\n",
    "    pruning_mask.view(-1)[indices] = True\n",
    "    \n",
    "    print(f\"Gradient-based pruning - target: {target_prune_count} heads\")\n",
    "    print(f\"Final pruning decision: pruning {pruning_mask.sum().item()} heads\")\n",
    "    print(f\"Average grad norm of pruned heads: {grad_norm_values[pruning_mask].mean().item():.6f}\")\n",
    "    print(f\"Average grad norm of kept heads: {grad_norm_values[~pruning_mask].mean().item():.6f}\")\n",
    "    return pruning_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Create plasticity controller with default thresholds\n",
    "controller = create_plasticity_controller(\n",
    "    model=model,\n",
    "    mode=PRUNING_MODE,\n",
    "    high_entropy_threshold=0.8,  # These will be ignored by our custom approach\n",
    "    low_entropy_threshold=0.4,   # but we need to provide values\n",
    "    grad_threshold=1e-3,\n",
    "    min_zero_epochs=MIN_ZERO_EPOCHS\n",
    ")\n",
    "\n",
    "# Display initial model stats\n",
    "initial_stats = controller.get_summary()\n",
    "print(f\"Model has {initial_stats['total_heads']} attention heads across {controller.total_layers} layers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller defined in the previous cell\n",
    "# Debug: Let's check the actual entropy values we're dealing with\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "print(\"\\nCollecting initial entropy and gradient metrics for debugging...\")\n",
    "debug_entropy, debug_grads = controller.collect_head_metrics(\n",
    "    validation_dataloader,\n",
    "    num_batches=2\n",
    ")\n",
    "\n",
    "# Calculate statistics to help with threshold setting\n",
    "print(\"\\nEntropy statistics:\")\n",
    "print(f\"Mean entropy: {debug_entropy.mean().item():.4f}\")\n",
    "print(f\"Min entropy: {debug_entropy.min().item():.4f}\")\n",
    "print(f\"Max entropy: {debug_entropy.max().item():.4f}\")\n",
    "print(f\"25th percentile: {torch.quantile(debug_entropy.flatten(), 0.25).item():.4f}\")\n",
    "print(f\"50th percentile: {torch.quantile(debug_entropy.flatten(), 0.5).item():.4f}\")\n",
    "print(f\"75th percentile: {torch.quantile(debug_entropy.flatten(), 0.75).item():.4f}\")\n",
    "print(f\"Are all entropy values the same? {torch.allclose(debug_entropy, debug_entropy[0,0])}\")\n",
    "print(f\"Non-zero values: {torch.count_nonzero(debug_entropy)}/{debug_entropy.numel()}\")\n",
    "\n",
    "print(\"\\nGradient norm statistics:\")\n",
    "print(f\"Mean grad norm: {debug_grads.mean().item():.6f}\")\n",
    "print(f\"Min grad norm: {debug_grads.min().item():.6f}\")\n",
    "print(f\"Max grad norm: {debug_grads.max().item():.6f}\")\n",
    "print(f\"25th percentile: {torch.quantile(debug_grads.flatten(), 0.25).item():.6f}\")\n",
    "print(f\"50th percentile: {torch.quantile(debug_grads.flatten(), 0.5).item():.6f}\")\n",
    "print(f\"75th percentile: {torch.quantile(debug_grads.flatten(), 0.75).item():.6f}\")\n",
    "print(f\"Are all gradient values the same? {torch.allclose(debug_grads, debug_grads[0,0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our gradient-only pruning approach\n",
    "pruning_mask = gradient_based_pruning(\n",
    "    debug_grads, \n",
    "    prune_percent=PRUNE_PERCENT\n",
    ")\n",
    "\n",
    "# Visualize which heads would be pruned\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(pruning_mask.detach().cpu().numpy(), cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Prune')\n",
    "plt.title('Gradient-Based Pruning Decisions')\n",
    "plt.xlabel('Head Index')\n",
    "plt.ylabel('Layer Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual comparing entropy and gradient distributions\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Entropy subplot - enhance with scale adjustment\n",
    "plt.subplot(1, 2, 1)\n",
    "entropy_data = debug_entropy.detach().cpu().numpy()\n",
    "vmax = max(0.1, entropy_data.max())  # Increase minimum scale to make patterns visible\n",
    "im1 = plt.imshow(entropy_data, cmap='viridis', aspect='auto', vmin=0, vmax=vmax)\n",
    "plt.colorbar(im1, label='Entropy')\n",
    "plt.title(f'Attention Entropy Values (max={entropy_data.max():.4f})')\n",
    "plt.xlabel('Head Index')\n",
    "plt.ylabel('Layer Index')\n",
    "\n",
    "# Gradient subplot\n",
    "plt.subplot(1, 2, 2)\n",
    "im2 = plt.imshow(debug_grads.detach().cpu().numpy(), cmap='plasma', aspect='auto')\n",
    "plt.colorbar(im2, label='Gradient Norm')\n",
    "plt.title('Gradient Norms')\n",
    "plt.xlabel('Head Index')\n",
    "plt.ylabel('Layer Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Initial Head Metrics\n",
    "\n",
    "Let's look at the initial head metrics to establish our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Collect initial head metrics\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "entropy_values, grad_norm_values = controller.collect_head_metrics(\n",
    "    validation_dataloader, \n",
    "    num_batches=2\n",
    ")\n",
    "\n",
    "# Function to visualize gradients without relying on Unicode\n",
    "def visualize_gradient_norms(grad_norm_values, pruned_heads=None, revived_heads=None, title=\"Gradient Norms\", save_path=None):\n",
    "    \"\"\"Create a visualization of gradient norms with markers for pruned/revived heads\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Gradient Norm\")\n",
    "    \n",
    "    # Mark pruned heads with 'P'\n",
    "    if pruned_heads:\n",
    "        for layer, head in pruned_heads:\n",
    "            plt.text(head, layer, \"P\", ha=\"center\", va=\"center\", \n",
    "                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "    \n",
    "    # Mark revived heads with 'R'\n",
    "    if revived_heads:\n",
    "        for layer, head in revived_heads:\n",
    "            plt.text(head, layer, \"R\", ha=\"center\", va=\"center\", \n",
    "                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='green', alpha=0.5))\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Head Index\")\n",
    "    plt.ylabel(\"Layer Index\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Create a better pruning mask for visualization\n",
    "# Get the indices of the heads with the LOWEST gradient norms\n",
    "flat_grad_norm = grad_norm_values.view(-1)\n",
    "total_heads = grad_norm_values.numel()\n",
    "target_prune_count = int(total_heads * PRUNE_PERCENT)\n",
    "_, indices = torch.topk(flat_grad_norm, k=target_prune_count, largest=False)\n",
    "pruning_mask = torch.zeros_like(grad_norm_values, dtype=torch.bool)\n",
    "pruning_mask.view(-1)[indices] = True\n",
    "\n",
    "# Create a comprehensive visualization showing gradient norms with markers for pruning decisions\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Initial Head Gradient Norms - Pruning Candidates\")\n",
    "plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Gradient Norm\")\n",
    "\n",
    "# Add text markers for the heads with LOWEST gradient norms (candidates for pruning)\n",
    "for layer in range(controller.total_layers):\n",
    "    for head in range(controller.heads_per_layer):\n",
    "        if pruning_mask[layer, head]:  # True means prune this head (lowest gradients)\n",
    "            plt.text(head, layer, \"P\", ha=\"center\", va=\"center\", \n",
    "                     color=\"white\", weight=\"bold\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now add the new visualization that combines gradient norms with pruning status\n",
    "print(\"\\nInitial Head Gradient Norms with Pruning Candidates:\")\n",
    "\n",
    "# Create a list of (layer, head) tuples for heads marked for pruning\n",
    "pruning_candidates = [(layer, head) for layer in range(controller.total_layers) \n",
    "                      for head in range(controller.heads_per_layer) \n",
    "                      if pruning_mask[layer, head]]  # True means prune (low gradient)\n",
    "\n",
    "visualize_gradient_norms(\n",
    "    grad_norm_values=grad_norm_values,\n",
    "    pruned_heads=pruning_candidates,  # Mark candidates as if they were pruned\n",
    "    title=\"Initial Head Gradient Norms with Pruning Candidates\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Also plot standard visualizations for comparison\n",
    "# Plot entropy heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Initial Head Entropy (higher = less focused attention)\")\n",
    "entropy_map = plt.imshow(entropy_values.detach().cpu().numpy(), cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(entropy_map, label=\"Entropy\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a visualization highlighting the relationship between gradient norms and pruning decisions\n",
    "plt.figure(figsize=(12, 8))\n",
    "grad_data = grad_norm_values.detach().cpu().numpy()\n",
    "mask_data = pruning_mask.detach().cpu().numpy()\n",
    "\n",
    "# Create a masked array where pruned heads are highlighted\n",
    "masked_grads = np.ma.array(grad_data, mask=~mask_data)\n",
    "\n",
    "# Base plot with all gradient values\n",
    "plt.imshow(grad_data, cmap='Blues', aspect='auto')\n",
    "# Overlay plot with pruned heads highlighted\n",
    "plt.imshow(masked_grads, cmap='Reds', aspect='auto')\n",
    "plt.colorbar(label='Gradient Norm')\n",
    "plt.title('Gradient Norms with Low-Gradient Heads Highlighted for Pruning')\n",
    "plt.xlabel('Head Index')\n",
    "plt.ylabel('Layer Index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Neural Plasticity\n",
    "\n",
    "Now let's train the model with neural plasticity enabled, allowing it to adaptively prune and restore attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training components\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Print epoch information\n",
    "print(f\"Dataset size: {len(train_dataset)} examples\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {len(train_dataloader)}\")\n",
    "print(f\"Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Maximum steps per epoch: {MAX_STEPS_PER_EPOCH if MAX_STEPS_PER_EPOCH else 'Unlimited'}\")\n",
    "print(f\"Expected total steps: {NUM_EPOCHS * (MAX_STEPS_PER_EPOCH or len(train_dataloader))}\")\n",
    "print(f\"Eval interval: {EVAL_INTERVAL} steps\")\n",
    "print(f\"Visualization interval: {VISUALIZATION_INTERVAL} steps\")\n",
    "print(f\"Inference interval: {INFERENCE_INTERVAL} steps\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metrics tracking\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"eval_loss\": [],\n",
    "    \"pruned_heads\": [],\n",
    "    \"revived_heads\": [],\n",
    "    \"sparsity\": [],\n",
    "    \"step\": [],\n",
    "    \"epoch\": [],  # Track epoch number for each step\n",
    "    \"perplexity\": [],  # Track perplexity\n",
    "    \"inference_samples\": []  # Store sample generations\n",
    "}\n",
    "\n",
    "# Create output directory for visualizations and checkpoints\n",
    "import os\n",
    "output_dir = \"pruning_visualizations\"\n",
    "checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Inference prompts for consistent tracking\n",
    "inference_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"In a distant galaxy\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Custom function to apply pruning based purely on gradients - CORRECTED VERSION\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "def apply_gradient_pruning(grad_norm_values):\n",
    "    \"\"\"Apply gradient-based pruning targeting heads with lowest gradient norms.\"\"\"\n",
    "    # Get pruning decisions\n",
    "    # Get the indices of the heads with the LOWEST gradient norms\n",
    "    flat_grad_norm = grad_norm_values.view(-1)\n",
    "    total_heads = grad_norm_values.numel()\n",
    "    target_prune_count = int(total_heads * PRUNE_PERCENT)\n",
    "    \n",
    "    # Key fix: use largest=False to get lowest gradient heads\n",
    "    _, indices = torch.topk(flat_grad_norm, k=target_prune_count, largest=False)\n",
    "    \n",
    "    # Create pruning mask - True means \"prune this head\"\n",
    "    pruning_mask = torch.zeros_like(grad_norm_values, dtype=torch.bool)\n",
    "    pruning_mask.view(-1)[indices] = True\n",
    "    \n",
    "    # Convert to list of (layer, head) tuples for pruning\n",
    "    pruned_heads = []\n",
    "    for layer in range(controller.total_layers):\n",
    "        for head in range(controller.heads_per_layer):\n",
    "            if pruning_mask[layer, head]:  # True means prune (low gradient)\n",
    "                # Check if head is already pruned\n",
    "                if not controller.stats[layer][head]['is_zeroed']:\n",
    "                    pruned_heads.append((layer, head))\n",
    "    \n",
    "    # Apply pruning\n",
    "    for layer, head in pruned_heads:\n",
    "        result = prune_head_in_model(\n",
    "            controller.model, \n",
    "            layer, \n",
    "            head, \n",
    "            mode=controller.mode, \n",
    "            verbose=True\n",
    "        )\n",
    "        if result:\n",
    "            # Update controller stats\n",
    "            controller.stats[layer][head]['is_zeroed'] = True\n",
    "            controller.stats[layer][head]['zeroed_epochs'] = 1\n",
    "    \n",
    "    # Update controller hooks\n",
    "    controller._update_pruning_hooks(verbose=False)  # Reduce verbosity\n",
    "    \n",
    "    # Print stats about the pruned and kept heads\n",
    "    if pruned_heads:\n",
    "        print(f\"Pruned {len(pruned_heads)} heads with lowest gradient norms\")\n",
    "        avg_pruned = grad_norm_values[pruning_mask].mean().item()\n",
    "        avg_kept = grad_norm_values[~pruning_mask].mean().item()\n",
    "        print(f\"Average gradient of pruned heads: {avg_pruned:.6f}\")\n",
    "        print(f\"Average gradient of kept heads: {avg_kept:.6f}\")\n",
    "        print(f\"Ratio (kept/pruned): {avg_kept/avg_pruned:.2f}x\")\n",
    "    \n",
    "    return pruned_heads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stats dict to regular dict for serialization\n",
    "def convert_stats_for_checkpoint(stats_dict):\n",
    "    \"\"\"Convert defaultdict to regular dict for pickle serialization.\"\"\"\n",
    "    regular_dict = {}\n",
    "    for layer, heads in stats_dict.items():\n",
    "        regular_dict[layer] = {}\n",
    "        for head, values in heads.items():\n",
    "            regular_dict[layer][head] = dict(values)\n",
    "    return regular_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Function to save checkpoint\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "def save_checkpoint(step, epoch):\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_step_{step}.pt\")\n",
    "    # Convert stats_dict to regular dict to avoid pickle issues\n",
    "    stats_dict = convert_stats_for_checkpoint(controller.stats)\n",
    "    torch.save({\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'controller_stats': stats_dict,\n",
    "        'metrics_history': metrics_history\n",
    "    }, checkpoint_path)\n",
    "    print(f\"  Checkpoint saved at step {step} (epoch {epoch})\")\n",
    "    return checkpoint_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run inference\n",
    "def run_model_inference():\n",
    "    model.eval()\n",
    "    inference_results = {}\n",
    "    \n",
    "    for prompt in inference_prompts:\n",
    "        generated_text = generate_text(prompt, max_length=50)  # Keep it shorter for quick visualization\n",
    "        inference_results[prompt] = generated_text\n",
    "        \n",
    "    print(\"\\n=== Sample Generations ===\")\n",
    "    for prompt, text in inference_results.items():\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {text[:100]}...\")  # Truncate for display\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return inference_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "global_step = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "try:\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        \n",
    "        # For each batch in the dataloader\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Check if we've reached MAX_STEPS_PER_EPOCH for this epoch\n",
    "            if MAX_STEPS_PER_EPOCH is not None and step >= MAX_STEPS_PER_EPOCH:\n",
    "                print(f\"  Reached maximum steps per epoch ({MAX_STEPS_PER_EPOCH}). Moving to next epoch.\")\n",
    "                break\n",
    "                \n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_steps += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            # Periodically evaluate\n",
    "            if global_step % EVAL_INTERVAL == 0:\n",
    "                # Evaluate\n",
    "                model.eval()\n",
    "                eval_loss, eval_perplexity = evaluate_model(model, validation_dataloader)\n",
    "                \n",
    "                # Collect metrics - we only need gradient norms\n",
    "                _, grad_norm_values = controller.collect_head_metrics(\n",
    "                    validation_dataloader, \n",
    "                    num_batches=2\n",
    "                )\n",
    "                \n",
    "                # Apply gradient-based pruning\n",
    "                pruned_heads = apply_gradient_pruning(grad_norm_values)\n",
    "                \n",
    "                # In this simplified version, we don't revive heads\n",
    "                revived_heads = []\n",
    "                \n",
    "                # Get model info\n",
    "                model_info = get_model_info(model)\n",
    "                total_pruned = controller._count_pruned_heads()\n",
    "                \n",
    "                # Update metrics\n",
    "                metrics_history[\"train_loss\"].append(epoch_loss / epoch_steps)\n",
    "                metrics_history[\"eval_loss\"].append(eval_loss)\n",
    "                metrics_history[\"pruned_heads\"].append(len(pruned_heads))\n",
    "                metrics_history[\"revived_heads\"].append(len(revived_heads))\n",
    "                metrics_history[\"sparsity\"].append(model_info[\"sparsity\"])\n",
    "                metrics_history[\"step\"].append(global_step)\n",
    "                metrics_history[\"epoch\"].append(epoch + 1)\n",
    "                metrics_history[\"perplexity\"].append(eval_perplexity)\n",
    "                \n",
    "                # Print status with epoch information\n",
    "                print(f\"  Step {global_step} (Epoch {epoch+1}) - Train loss: {epoch_loss / epoch_steps:.4f}, \"\n",
    "                      f\"Eval loss: {eval_loss:.4f}, Perplexity: {eval_perplexity:.2f}\")\n",
    "                print(f\"  Pruned: {len(pruned_heads)} heads, Revived: {len(revived_heads)} heads, Total pruned: {total_pruned}\")\n",
    "                print(f\"  Sparsity: {model_info['sparsity']:.4f}\")\n",
    "                \n",
    "                # Generate and save the visualization with pruning overlays if new heads were pruned\n",
    "                # or at regular visualization intervals\n",
    "                if len(pruned_heads) > 0 or global_step % VISUALIZATION_INTERVAL == 0:\n",
    "                    # Get the current pruned heads from controller stats\n",
    "                    all_pruned_heads = []\n",
    "                    for layer in range(controller.total_layers):\n",
    "                        for head in range(controller.heads_per_layer):\n",
    "                            if controller.stats[layer][head]['is_zeroed']:\n",
    "                                all_pruned_heads.append((layer, head))\n",
    "                    \n",
    "                    # Generate visualization with current pruning state\n",
    "                    viz_path = os.path.join(output_dir, f\"head_gradients_step_{global_step}.png\")\n",
    "                    \n",
    "                    # Use our custom visualization function that doesn't rely on Unicode\n",
    "                    visualize_gradient_norms(\n",
    "                        grad_norm_values=grad_norm_values,\n",
    "                        pruned_heads=all_pruned_heads,\n",
    "                        revived_heads=revived_heads,\n",
    "                        title=f\"Head Gradient Norms with Pruning Status (Step {global_step}, Epoch {epoch+1})\",\n",
    "                        save_path=viz_path\n",
    "                    )\n",
    "                    \n",
    "                    # Display the visualization\n",
    "                    display_img = plt.imread(viz_path)\n",
    "                    plt.figure(figsize=(12, 6))\n",
    "                    plt.imshow(display_img)\n",
    "                    plt.axis('off')\n",
    "                    plt.title(f\"Step {global_step} (Epoch {epoch+1}): {len(pruned_heads)} new heads pruned, {total_pruned} total pruned\")\n",
    "                    plt.show()\n",
    "                \n",
    "                # Run model inference at regular intervals\n",
    "                if global_step % INFERENCE_INTERVAL == 0:\n",
    "                    inference_results = run_model_inference()\n",
    "                    metrics_history[\"inference_samples\"].append({\n",
    "                        \"step\": global_step,\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"results\": inference_results\n",
    "                    })\n",
    "                \n",
    "                # Save checkpoint at regular intervals\n",
    "                if global_step % CHECKPOINT_INTERVAL == 0:\n",
    "                    save_checkpoint(global_step, epoch + 1)\n",
    "                \n",
    "                # Reset for next interval\n",
    "                epoch_loss = 0.0\n",
    "                epoch_steps = 0\n",
    "                \n",
    "                # Back to training mode\n",
    "                model.train()\n",
    "            \n",
    "            # Print progress every 200 steps\n",
    "            if global_step % 200 == 0:\n",
    "                print(f\"  Progress: Step {global_step} (Epoch {epoch+1})\")\n",
    "        \n",
    "        print(f\"Completed Epoch {epoch+1} - Total steps: {global_step}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n",
    "    print(f\"Training completed! Final checkpoint saved at {final_checkpoint_path}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user.\")\n",
    "    # Save checkpoint on interrupt\n",
    "    interrupt_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n",
    "    print(f\"Checkpoint saved at {interrupt_checkpoint_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining error: {e}\")\n",
    "    # Try to save checkpoint on error\n",
    "    try:\n",
    "        error_checkpoint_path = save_checkpoint(global_step, epoch + 1)\n",
    "        print(f\"Checkpoint saved at {error_checkpoint_path}\")\n",
    "    except Exception as save_error:\n",
    "        print(f\"Could not save checkpoint: {save_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training history to see how neural plasticity affected the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training metrics with epochs\n",
    "# Create a more reasonably sized figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 10), dpi=80, sharex=True)\n",
    "\n",
    "# Set maximum display limit to prevent excessively large plots\n",
    "max_display_points = 100\n",
    "display_steps = metrics_history[\"step\"]\n",
    "if len(display_steps) > max_display_points:\n",
    "    # Downsample by selecting evenly spaced points\n",
    "    indices = np.linspace(0, len(display_steps) - 1, max_display_points).astype(int)\n",
    "    display_steps = [metrics_history[\"step\"][i] for i in indices]\n",
    "    display_train_loss = [metrics_history[\"train_loss\"][i] for i in indices]\n",
    "    display_eval_loss = [metrics_history[\"eval_loss\"][i] for i in indices]\n",
    "    display_pruned_heads = [metrics_history[\"pruned_heads\"][i] for i in indices]\n",
    "    display_revived_heads = [metrics_history[\"revived_heads\"][i] for i in indices]\n",
    "    display_sparsity = [metrics_history[\"sparsity\"][i] for i in indices]\n",
    "    display_epoch = [metrics_history[\"epoch\"][i] for i in indices]\n",
    "    display_perplexity = [metrics_history[\"perplexity\"][i] for i in indices] if \"perplexity\" in metrics_history and metrics_history[\"perplexity\"] else []\n",
    "else:\n",
    "    display_train_loss = metrics_history[\"train_loss\"]\n",
    "    display_eval_loss = metrics_history[\"eval_loss\"]\n",
    "    display_pruned_heads = metrics_history[\"pruned_heads\"]\n",
    "    display_revived_heads = metrics_history[\"revived_heads\"]\n",
    "    display_sparsity = metrics_history[\"sparsity\"]\n",
    "    display_epoch = metrics_history[\"epoch\"]\n",
    "    display_perplexity = metrics_history[\"perplexity\"] if \"perplexity\" in metrics_history else []\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(display_steps, display_train_loss, label=\"Train Loss\")\n",
    "ax1.plot(display_steps, display_eval_loss, label=\"Eval Loss\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Evaluation Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Mark epoch boundaries if available\n",
    "if \"epoch\" in metrics_history and len(display_epoch) > 1:\n",
    "    for i in range(1, len(display_epoch)):\n",
    "        if display_epoch[i] != display_epoch[i-1]:\n",
    "            # This is an epoch boundary\n",
    "            for ax in [ax1, ax2, ax3]:\n",
    "                ax.axvline(x=display_steps[i], color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "                ax.text(display_steps[i], ax.get_ylim()[1]*0.9, \n",
    "                        f\"Epoch {display_epoch[i]}\", rotation=90, alpha=0.7)\n",
    "\n",
    "# Plot pruning metrics\n",
    "ax2.bar(display_steps, display_pruned_heads, alpha=0.5, label=\"Pruned Heads\", color=\"blue\")\n",
    "ax2.bar(display_steps, display_revived_heads, alpha=0.5, label=\"Revived Heads\", color=\"green\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Head Pruning and Revival\")\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot sparsity and perplexity\n",
    "ax3.plot(display_steps, display_sparsity, \"r-\", label=\"Sparsity\")\n",
    "ax3.set_xlabel(\"Step\")\n",
    "ax3.set_ylabel(\"Sparsity\")\n",
    "ax3.grid(True)\n",
    "\n",
    "# Add perplexity line on secondary axis if available\n",
    "if \"perplexity\" in metrics_history and metrics_history[\"perplexity\"]:\n",
    "    ax4 = ax3.twinx()\n",
    "    ax4.plot(display_steps, display_perplexity, \"g-\", label=\"Perplexity\")\n",
    "    ax4.set_ylabel(\"Perplexity\")\n",
    "    ax4.legend(loc=\"upper right\")\n",
    "\n",
    "# Ensure figure has reasonable dimensions\n",
    "plt.gcf().set_dpi(100)\n",
    "# Set explicit figure size limits before layout\n",
    "plt.gcf().set_size_inches(10, 10, forward=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "# Final EvaluationLet's evaluate the final model to see how it compares to the baselin\n",
    "\n",
    "e\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This cell requires the controller to be defined\n",
    "# Final evaluation\n",
    "# Check if controller exists\n",
    "try:\n",
    "    controller\n",
    "except NameError:\n",
    "    print(\"ERROR: The controller variable is not defined. Please run the cell that creates the plasticity controller first.\")\n",
    "    # Create a simple dummy controller to avoid breaking the notebook flow\n",
    "    from types import SimpleNamespace\n",
    "    controller = SimpleNamespace()\n",
    "    controller.collect_head_metrics = lambda *args, **kwargs: (None, None)\n",
    "    controller.display_stats = lambda *args, **kwargs: None\n",
    "    controller.stats = {}\n",
    "    controller.total_layers = 0\n",
    "    controller.heads_per_layer = 0\n",
    "\n",
    "\n",
    "final_loss, final_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Final evaluation: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f}\")\n",
    "print(f\"Baseline:         Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Improvement:      {((baseline_loss - final_loss) / baseline_loss * 100):.2f}%\")\n",
    "\n",
    "# Get final summary\n",
    "summary = controller.get_summary()\n",
    "print(\"\\nFinal Controller Summary:\")\n",
    "print(f\"  Total heads: {summary['total_heads']}\")\n",
    "print(f\"  Pruned heads: {summary['pruned_heads']} ({summary['pruning_rate']:.2%})\")\n",
    "print(f\"  Model sparsity: {summary['sparsity']:.4f}\")\n",
    "print(f\"  Model size: {summary['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Final Model\n",
    "\n",
    "Let's generate text with our plasticity-enhanced model to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with final model\n",
    "final_text = generate_text(prompt)\n",
    "\n",
    "print(\"Baseline Model Output:\")\n",
    "print(baseline_text)\n",
    "print(\"\\nPlasticity-Optimized Model Output:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the ModelLet's save the optimized model for later us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"plasticity\", f\"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Prompts\n",
    "\n",
    "Let's try generating text with different prompts to see how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"In a distant galaxy\",\n",
    "    \"The future of AI will be\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we demonstrated Sentinel AI's neural plasticity system, which enables transformer models to dynamically prune and revive attention heads during training based on their utility.Key findings:1. The plasticity system successfully pruned high-entropy, low-gradient heads2. Some heads were revived when they showed potential for useful learning3. The final model achieved comparable quality with fewer active heads4. The brain dynamics visualization shows how attention heads evolve over timeThis approach mimics biological neural plasticity, where brains form efficient neural pathways by pruning unused connections and strengthening useful ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
