{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Neural Plasticity Demo: Dynamic Pruning & Regrowth (v0.0.5)\n\nThis notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics.\n\n## What is Neural Plasticity?\n\nNeural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n\nIn this demo, we:\n1. Track the entropy and gradient patterns of each attention head\n2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n4. Visualize the \"brain dynamics\" over time\n\nThis allows models to form more efficient neural structures during training.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets matplotlib seaborn\n",
    "\n",
    "# Clone the Sentinel AI repository\n",
    "!git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Add repository to path\n",
    "import sys\n",
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Experiment\n",
    "\n",
    "Let's set up our configuration for the neural plasticity experiment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure experiment\nMODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\nDATASET = \"wikitext\"\nDATASET_CONFIG = \"wikitext-2-raw-v1\"\nMAX_LENGTH = 128\nBATCH_SIZE = 4\nNUM_EPOCHS = 100\nLEARNING_RATE = 5e-5\nWARMUP_STEPS = 100\nWARMUP_EPOCHS = 1     # Number of epochs to run warmup\nEVAL_INTERVAL = 50    # Evaluate every 50 steps\n\n# Configure pruning mode\nfrom sentinel.pruning.dual_mode_pruning import PruningMode\n\n# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\nPRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n\n# Configure plasticity thresholds (even more aggressive pruning)\nHIGH_ENTROPY_THRESHOLD = 0.4  # Drastically lowered from 0.6 - Heads with entropy above this are candidates for pruning\nLOW_ENTROPY_THRESHOLD = 0.2   # Lowered from 0.3 - Pruned heads with entropy below this are candidates for revival\nGRAD_THRESHOLD = 1e-3         # Increased from 5e-5 - Gradient threshold for pruning decisions\nMIN_ZERO_EPOCHS = 1           # Minimum epochs a head should remain pruned"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Dataset\n",
    "\n",
    "Now we'll load the model and prepare the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentinel.pruning.plasticity_controller import create_plasticity_controller\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Set pad token if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load datasets\n",
    "print(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\n",
    "train_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\n",
    "validation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Add labels for language modeling\n",
    "def add_labels(examples):\n",
    "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n",
    "    return examples\n",
    "\n",
    "train_dataset = train_dataset.map(add_labels)\n",
    "validation_dataset = validation_dataset.map(add_labels)\n",
    "\n",
    "# Set format\n",
    "train_dataset = train_dataset.with_format(\"torch\")\n",
    "validation_dataset = validation_dataset.with_format(\"torch\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation dataset size: {len(validation_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function\n",
    "\n",
    "Let's define a function to evaluate our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model on the provided dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Limit evaluation to 10 steps for speed\n",
    "            if total_steps >= 10:\n",
    "                break\n",
    "    \n",
    "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    \"\"\"Generate text from the model.\"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode and return text\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Warm-up\n",
    "\n",
    "Before measuring baseline performance, we'll run a brief warm-up phase to stabilize the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler for warm-up\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "total_steps = len(train_dataloader) * WARMUP_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=WARMUP_STEPS, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Running warm-up for {WARMUP_EPOCHS} epoch(s)...\")\n",
    "\n",
    "# Warm-up training loop\n",
    "model.train()\n",
    "warmup_losses = []\n",
    "\n",
    "for epoch in range(WARMUP_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Track loss\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        \n",
    "        # Print progress every 10 steps\n",
    "        if step % 10 == 0:\n",
    "            warmup_losses.append(loss.item())\n",
    "            print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss.item():.4f}\\r\", end=\"\")\n",
    "            \n",
    "        # Stop after 50 steps for faster execution in demo\n",
    "        if step >= 50:\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nWarm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n",
    "\n",
    "# Plot warm-up loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(warmup_losses)\n",
    "plt.title(\"Warm-up Loss\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baseline Model\n",
    "\n",
    "Now let's measure the baseline performance after warm-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline model after warm-up\n",
    "baseline_loss, baseline_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "\n",
    "# Generate text with baseline model\n",
    "prompt = \"Once upon a time\"\n",
    "baseline_text = generate_text(prompt)\n",
    "print(f\"\\nPrompt: {prompt}\")\n",
    "print(f\"Generated text:\\n{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Neural Plasticity Controller\n",
    "\n",
    "Now we'll create the plasticity controller that will monitor head metrics and dynamically prune/revive heads during training."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Create plasticity controller\ncontroller = create_plasticity_controller(\n    model=model,\n    mode=PRUNING_MODE,\n    high_entropy_threshold=HIGH_ENTROPY_THRESHOLD,\n    low_entropy_threshold=LOW_ENTROPY_THRESHOLD,\n    grad_threshold=GRAD_THRESHOLD,\n    min_zero_epochs=MIN_ZERO_EPOCHS\n)\n\n# Display initial model stats\ninitial_stats = controller.get_summary()\nprint(f\"Model has {initial_stats['total_heads']} attention heads across {controller.total_layers} layers\")\n\n# Debug: Let's check the actual entropy values we're dealing with\nprint(\"\\nCollecting initial entropy and gradient metrics for debugging...\")\ndebug_entropy, debug_grads = controller.collect_head_metrics(\n    validation_dataloader,\n    num_batches=2,\n    verbose=True\n)\n\n# Calculate statistics to help with threshold setting\nprint(\"\\nEntropy statistics:\")\nprint(f\"Mean entropy: {debug_entropy.mean().item():.4f}\")\nprint(f\"Min entropy: {debug_entropy.min().item():.4f}\")\nprint(f\"Max entropy: {debug_entropy.max().item():.4f}\")\nprint(f\"25th percentile: {torch.quantile(debug_entropy.flatten(), 0.25).item():.4f}\")\nprint(f\"50th percentile: {torch.quantile(debug_entropy.flatten(), 0.5).item():.4f}\")\nprint(f\"75th percentile: {torch.quantile(debug_entropy.flatten(), 0.75).item():.4f}\")\n\nprint(\"\\nGradient norm statistics:\")\nprint(f\"Mean grad norm: {debug_grads.mean().item():.6f}\")\nprint(f\"Min grad norm: {debug_grads.min().item():.6f}\")\nprint(f\"Max grad norm: {debug_grads.max().item():.6f}\")\nprint(f\"25th percentile: {torch.quantile(debug_grads.flatten(), 0.25).item():.6f}\")\nprint(f\"50th percentile: {torch.quantile(debug_grads.flatten(), 0.5).item():.6f}\")\nprint(f\"75th percentile: {torch.quantile(debug_grads.flatten(), 0.75).item():.6f}\")\n\n# Print current thresholds\nprint(f\"\\nCurrent thresholds:\")\nprint(f\"HIGH_ENTROPY_THRESHOLD: {HIGH_ENTROPY_THRESHOLD:.4f}\")\nprint(f\"LOW_ENTROPY_THRESHOLD: {LOW_ENTROPY_THRESHOLD:.4f}\")\nprint(f\"GRAD_THRESHOLD: {GRAD_THRESHOLD:.6f}\")\n\n# Count how many heads would be pruned with current thresholds\nwould_prune = (debug_entropy > HIGH_ENTROPY_THRESHOLD) & (debug_grads < GRAD_THRESHOLD)\nprint(f\"\\nWith current thresholds, {would_prune.sum().item()} heads would be eligible for pruning.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Initial Head Metrics\n",
    "\n",
    "Let's look at the initial entropy and gradient patterns of our attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect initial head metrics\n",
    "entropy_values, grad_norm_values = controller.collect_head_metrics(\n",
    "    validation_dataloader, \n",
    "    num_batches=2\n",
    ")\n",
    "\n",
    "# Plot entropy heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Initial Head Entropy (higher = less focused attention)\")\n",
    "entropy_map = plt.imshow(entropy_values.detach().cpu().numpy(), cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(entropy_map, label=\"Entropy\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot gradient norm heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Initial Head Gradient Norms (higher = more learning)\")\n",
    "grad_map = plt.imshow(grad_norm_values.detach().cpu().numpy(), cmap=\"plasma\", aspect=\"auto\")\n",
    "plt.colorbar(grad_map, label=\"Gradient Norm\")\n",
    "plt.xlabel(\"Head Index\")\n",
    "plt.ylabel(\"Layer Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Neural Plasticity\n",
    "\n",
    "Now let's train the model with neural plasticity enabled, dynamically pruning and reviving attention heads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize training components\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\ntotal_steps = len(train_dataloader) * NUM_EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=WARMUP_STEPS, \n    num_training_steps=total_steps\n)\n\n# Initialize metrics tracking\nmetrics_history = {\n    \"train_loss\": [],\n    \"eval_loss\": [],\n    \"pruned_heads\": [],\n    \"revived_heads\": [],\n    \"sparsity\": [],\n    \"step\": []\n}\n\n# Training loop\nglobal_step = 0\n\nfor epoch in range(NUM_EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n    model.train()\n    \n    epoch_loss = 0.0\n    epoch_steps = 0\n    \n    for step, batch in enumerate(train_dataloader):\n        # Move batch to device\n        batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        \n        # Forward pass\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        # Backward pass\n        loss.backward()\n        \n        # Update weights\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n        # Track loss\n        epoch_loss += loss.item()\n        epoch_steps += 1\n        global_step += 1\n        \n        # Periodically evaluate and apply plasticity\n        if global_step % EVAL_INTERVAL == 0:\n            # Evaluate\n            model.eval()\n            eval_loss, eval_perplexity = evaluate_model(model, validation_dataloader)\n            \n            # Apply plasticity\n            pruned, revived, plasticity_metrics = controller.step(\n                validation_dataloader, \n                num_batches=2,\n                verbose=True\n            )\n            \n            # Update metrics\n            metrics_history[\"train_loss\"].append(epoch_loss / epoch_steps)\n            metrics_history[\"eval_loss\"].append(eval_loss)\n            metrics_history[\"pruned_heads\"].append(len(pruned))\n            metrics_history[\"revived_heads\"].append(len(revived))\n            metrics_history[\"sparsity\"].append(plasticity_metrics[\"sparsity\"])\n            metrics_history[\"step\"].append(global_step)\n            \n            # Print status\n            print(f\"  Step {global_step} - Train loss: {epoch_loss / epoch_steps:.4f}, Eval loss: {eval_loss:.4f}\")\n            print(f\"  Pruned: {len(pruned)} heads, Revived: {len(revived)} heads, Total pruned: {plasticity_metrics['total_pruned']}\")\n            print(f\"  Sparsity: {plasticity_metrics['sparsity']:.4f}\")\n            \n            # Reset for next interval\n            epoch_loss = 0.0\n            epoch_steps = 0\n            \n            # Back to training mode\n            model.train()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Let's visualize the training progress, including loss metrics and head pruning/revival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training metrics\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
    "\n",
    "# Plot losses\n",
    "ax1.plot(metrics_history[\"step\"], metrics_history[\"train_loss\"], label=\"Train Loss\")\n",
    "ax1.plot(metrics_history[\"step\"], metrics_history[\"eval_loss\"], label=\"Eval Loss\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Evaluation Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot pruning metrics\n",
    "ax2.bar(metrics_history[\"step\"], metrics_history[\"pruned_heads\"], alpha=0.5, label=\"Pruned Heads\", color=\"blue\")\n",
    "ax2.bar(metrics_history[\"step\"], metrics_history[\"revived_heads\"], alpha=0.5, label=\"Revived Heads\", color=\"green\")\n",
    "ax2.set_xlabel(\"Step\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.set_title(\"Head Pruning and Revival\")\n",
    "ax2.legend(loc=\"upper left\")\n",
    "ax2.grid(True)\n",
    "\n",
    "# Add sparsity line on secondary axis\n",
    "ax3 = ax2.twinx()\n",
    "ax3.plot(metrics_history[\"step\"], metrics_history[\"sparsity\"], \"r-\", label=\"Sparsity\")\n",
    "ax3.set_ylabel(\"Sparsity\")\n",
    "ax3.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize head dynamics\n",
    "controller.visualize_head_dynamics(metric='entropy')\n",
    "plt.show()\n",
    "\n",
    "controller.visualize_head_dynamics(metric='decision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Let's evaluate the final model to see how it compares to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "final_loss, final_perplexity = evaluate_model(model, validation_dataloader)\n",
    "print(f\"Final evaluation: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f}\")\n",
    "print(f\"Baseline:         Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n",
    "print(f\"Improvement:      {((baseline_loss - final_loss) / baseline_loss * 100):.2f}%\")\n",
    "\n",
    "# Get final summary\n",
    "summary = controller.get_summary()\n",
    "print(\"\\nFinal Controller Summary:\")\n",
    "print(f\"  Total heads: {summary['total_heads']}\")\n",
    "print(f\"  Pruned heads: {summary['pruned_heads']} ({summary['pruning_rate']:.2%})\")\n",
    "print(f\"  Model sparsity: {summary['sparsity']:.4f}\")\n",
    "print(f\"  Model size: {summary['model_size_mb']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text with Final Model\n",
    "\n",
    "Let's generate text with the final model to see if there are any quality differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with final model\n",
    "final_text = generate_text(prompt)\n",
    "\n",
    "print(\"Baseline Model Output:\")\n",
    "print(baseline_text)\n",
    "print(\"\\nPlasticity-Optimized Model Output:\")\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Let's save the optimized model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = os.path.join(\"output\", \"plasticity\", f\"run_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Different Prompts\n",
    "\n",
    "Let's try generating text with different prompts to evaluate the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"The meaning of life is\",\n",
    "    \"In a distant galaxy\",\n",
    "    \"The future of AI will be\",\n",
    "    \"Scientists recently discovered\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Generated: {generated}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Conclusion\n\nIn this notebook, we demonstrated Sentinel AI's neural plasticity system, which enables transformer models to dynamically prune and revive attention heads during training based on their utility.\n\nKey findings:\n1. The plasticity system successfully pruned high-entropy, low-gradient heads\n2. Some heads were revived when they showed potential for useful learning\n3. The final model achieved comparable quality with fewer active heads\n4. The brain dynamics visualization shows how attention heads evolve over time\n\nThis approach mimics biological neural plasticity, where brains form efficient neural pathways by pruning unused connections and strengthening useful ones.\n\n## Version History\n\n- v0.0.5: Significantly more aggressive pruning thresholds (HIGH_ENTROPY_THRESHOLD: 0.6→0.4, LOW_ENTROPY_THRESHOLD: 0.3→0.2, GRAD_THRESHOLD: 5e-5→1e-3)\n- v0.0.4: Adjusted pruning thresholds for more aggressive pruning behavior (HIGH_ENTROPY_THRESHOLD: 0.8→0.6, LOW_ENTROPY_THRESHOLD: 0.4→0.3, GRAD_THRESHOLD: 1e-4→5e-5)\n- v0.0.3: Removed hard-coded 200-step limit to allow full NUM_EPOCHS training\n- v0.0.2: Added warmup phase to get more accurate baseline measurements, improved visualization of head metrics, fixed perplexity calculation issues\n- v0.0.1: Initial implementation of neural plasticity demo",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}