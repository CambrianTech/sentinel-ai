{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrayedd Continuous Optimization\n",
    "\n",
    "This notebook demonstrates the continuous optimization mode of Upgrayedd, which runs until manually interrupted and provides regular checkpoints and sample outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "!pip install torch transformers datasets tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clone the repository if running on Colab\n",
    "import os\n",
    "if not os.path.exists('sentinel-ai'):\n",
    "    !git clone https://github.com/yourusername/sentinel-ai.git\n",
    "    %cd sentinel-ai\n",
    "else:\n",
    "    %cd sentinel-ai\n",
    "    !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Add the repository to the path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"üöÄ {gpu_name} GPU detected!\")\n",
    "    \n",
    "    # Recommended models based on GPU\n",
    "    if \"T4\" in gpu_name or \"K80\" in gpu_name or \"P4\" in gpu_name:\n",
    "        print(\"Recommended models:\")\n",
    "        print(\"- distilgpt2 (82M parameters)\")\n",
    "        print(\"- facebook/opt-350m (350M parameters)\")\n",
    "        print(\"- EleutherAI/pythia-410m (410M parameters)\")\n",
    "    elif \"V100\" in gpu_name or \"A100\" in gpu_name or \"P100\" in gpu_name:\n",
    "        print(\"Recommended models:\")\n",
    "        print(\"- gpt2 (124M parameters)\")\n",
    "        print(\"- facebook/opt-1.3b (1.3B parameters)\")\n",
    "        print(\"- EleutherAI/pythia-1.4b (1.4B parameters)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Using CPU.\")\n",
    "    print(\"Recommended models:\")\n",
    "    print(\"- distilgpt2 (82M parameters)\")\n",
    "    print(\"- distilbert-base-uncased (66M parameters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    # Model selection\n",
    "    \"model_name\": \"distilgpt2\",  # Change this to your preferred model\n",
    "    \n",
    "    # Dataset selection\n",
    "    \"dataset\": \"wikitext\",  # Options: wikitext, tiny_shakespeare, gutenberg\n",
    "    \n",
    "    # Optimization parameters\n",
    "    \"pruning_ratio\": 0.3,      # Fraction of heads to prune\n",
    "    \"growth_ratio\": 0.1,       # Fraction of pruned heads to regrow\n",
    "    \"learning_rate\": 5e-5,     # Learning rate for fine-tuning\n",
    "    \"batch_size\": 4,           # Training batch size\n",
    "    \"gradient_accumulation\": 2,  # Gradient accumulation steps\n",
    "    \n",
    "    # Training parameters\n",
    "    \"epochs_per_cycle\": 1,     # Training epochs per cycle\n",
    "    \n",
    "    # Checkpointing\n",
    "    \"save_frequency\": 1,       # Save checkpoints every N cycles\n",
    "    \"eval_frequency\": 1,       # Generate samples every N cycles\n",
    "    \"compress_model\": True,    # Apply compression after training\n",
    "    \"compression_type\": \"mask\",  # Compression type: mask, remove, distill\n",
    "    \n",
    "    # Controller-plasticity integration\n",
    "    \"controller_type\": \"ann\",  # Type of controller (ann, rule)\n",
    "    \n",
    "    # Visualization\n",
    "    \"visualize\": True,         # Create visualizations\n",
    "    \"plot_frequency\": 1,       # Update plots every N cycles\n",
    "    \n",
    "    # Advanced\n",
    "    \"use_differential_lr\": True  # Use different learning rates for different heads\n",
    "}\n",
    "\n",
    "# Display configuration\n",
    "print(\"Upgrayedd Configuration:\")\n",
    "print(f\"- Model: {config['model_name']}\")\n",
    "print(f\"- Dataset: {config['dataset']}\")\n",
    "print(f\"- Pruning level: {config['pruning_ratio']}\")\n",
    "print(f\"- Growth ratio: {config['growth_ratio']}\")\n",
    "print(f\"- Controller type: {config['controller_type']}\")\n",
    "print(f\"- Learning rate: {config['learning_rate']}\")\n",
    "print(f\"- Batch size: {config['batch_size']} (gradient accumulation: {config['gradient_accumulation']})\")\n",
    "print(f\"- Compression: {config['compress_model']} (type: {config['compression_type']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create output directory\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_short_name = config[\"model_name\"].split('/')[-1]\n",
    "output_dir = f\"./upgrayedd_output/{model_short_name}_{timestamp}\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Output will be saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import and run Upgrayedd\n",
    "from upgrayedd.core import transform_model\n",
    "\n",
    "# Optional: Specify a checkpoint to resume from\n",
    "resume_checkpoint = None  # Set to a checkpoint path to resume\n",
    "\n",
    "# Run the transformation in continuous mode\n",
    "print(\"Starting continuous optimization (run this cell and press Stop when you want to interrupt)\")\n",
    "print(\"The process will save checkpoints regularly and can be resumed later\")\n",
    "\n",
    "results = transform_model(\n",
    "    model_name=config[\"model_name\"],\n",
    "    output_dir=output_dir,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    mode=\"continuous\",  # This is the key setting for continuous mode\n",
    "    resume_checkpoint=resume_checkpoint,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize results (only needed if interrupted before visualization)\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load results from the latest checkpoint if available\n",
    "checkpoint_dirs = sorted(glob.glob(f\"{output_dir}/checkpoint-*\"))\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = checkpoint_dirs[-1]\n",
    "    results_path = os.path.join(latest_checkpoint, \"results.json\")\n",
    "    \n",
    "    if os.path.exists(results_path):\n",
    "        with open(results_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    \n",
    "# Plot perplexity over cycles\n",
    "if results and \"cycles\" in results:\n",
    "    cycles = list(range(1, len(results[\"cycles\"]) + 1))\n",
    "    \n",
    "    # Extract perplexities\n",
    "    perplexities = [cycle.get(\"perplexity\", 0) for cycle in results[\"cycles\"]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cycles, perplexities, marker='o')\n",
    "    plt.axhline(y=results.get(\"baseline_perplexity\", 0), color='r', linestyle='--', label='Baseline')\n",
    "    plt.title(\"Perplexity over Optimization Cycles\")\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_dir, \"perplexity_plot.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot active heads\n",
    "    if \"pruned_heads_percent\" in results[\"cycles\"][0]:\n",
    "        pruned_percents = [cycle.get(\"pruned_heads_percent\", 0) * 100 for cycle in results[\"cycles\"]]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(cycles, pruned_percents, marker='o', color='g')\n",
    "        plt.title(\"Pruned Heads Percentage over Optimization Cycles\")\n",
    "        plt.xlabel(\"Cycle\")\n",
    "        plt.ylabel(\"Pruned Heads (%)\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(output_dir, \"pruning_plot.png\"))\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No cycle results available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate text with the optimized model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the final model if available, otherwise use the latest checkpoint\n",
    "final_model_path = os.path.join(output_dir, \"final_checkpoint\")\n",
    "if not os.path.exists(final_model_path) and checkpoint_dirs:\n",
    "    final_model_path = checkpoint_dirs[-1]\n",
    "\n",
    "if os.path.exists(final_model_path):\n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "    \n",
    "    # Define prompt\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence is\",\n",
    "        \"In recent scientific discoveries,\",\n",
    "        \"The most important thing to remember about learning is\"\n",
    "    ]\n",
    "    \n",
    "    # Generate text\n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Generated: {generated_text}\")\n",
    "else:\n",
    "    print(\"No optimized model available for text generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}