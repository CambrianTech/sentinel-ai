{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "üëæ Upgrayedd Runtime: Transform Models with Adaptive Pruning and Regrowth (v1.0.0)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CambrianTech/sentinel-ai/blob/feature/implement-adaptive-plasticity/colab_notebooks/UpgrayeddColab.ipynb)\n\nThis notebook demonstrates the full Upgrayedd adaptive transformation system, which turns any HuggingFace transformer model into a self-optimizing network through:\n\n1. **Controller-guided pruning**: Dynamically identifies and removes unnecessary attention heads\n2. **Strategic regrowth**: Reconstructs critical pathways where needed\n3. **Differential learning**: Applies custom learning rates for optimal adaptation\n4. **Compression options**: Optional size reduction with pruned models\n\n> *Spelled with two D's for a double dose of adaptive optimization.*",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Environment Setup\n\nFirst, check if we're running in a Colab environment and set up GPU acceleration:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Check environment and GPU availability\nimport sys\nimport torch\nimport os\nfrom datetime import datetime\n\n# Determine if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\nprint(f\"Running in Colab: {IN_COLAB}\")\n\n# Check GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"üöÄ GPU available: {gpu_name}\")\n    \n    # Display memory info\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # For A100, recommend using larger models\n    if 'A100' in gpu_name:\n        print(\"üí™ A100 detected! You can use larger models (up to 7B parameters)\")\n    \n    # Set default device\n    torch.set_default_device(device)\nelse:\n    device = torch.device(\"cpu\")\n    print(\"‚ö†Ô∏è No GPU detected. Running on CPU will be significantly slower.\")\n\n# Install required packages\nif IN_COLAB:\n    print(\"Installing required packages...\")\n    !pip install -q transformers datasets accelerate evaluate bitsandbytes safetensors"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository and set up the environment\nif IN_COLAB:\n    # Clone the repository\n    print(\"Cloning Sentinel-AI repository...\")\n    !git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n    \n    # Add to Python path\n    import sys\n    sys.path.append('/content/sentinel-ai')\n    \n    # Change to the repository directory\n    %cd /content/sentinel-ai\n    \n    # Create output directory\n    !mkdir -p output\nelse:\n    # If not in Colab, assume we're already in the right directory\n    print(\"Running in local environment, make sure you're in the sentinel-ai directory\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Import Required Modules\n\nNow we'll import the actual Upgrayedd system components:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import core Sentinel-AI components\nimport os\nimport sys\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nimport logging\nimport json\nimport re\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(\"Upgrayedd\")\n\n# Import transformers components\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)\n\n# Import datasets\nfrom datasets import load_dataset\n\n# Import Sentinel-AI components\ntry:\n    # Direct import from scripts (most reliable path)\n    from scripts.upgrayedd import ModelUpgrader\n    print(\"‚úÖ Successfully imported ModelUpgrader directly from scripts\")\nexcept ImportError as e:\n    # Try wrapper modules\n    try:\n        from sentinel.upgrayedd import ModelUpgrader\n        print(\"‚úÖ Successfully imported ModelUpgrader from sentinel package\")\n    except ImportError as e:\n        print(f\"‚ùå Could not import ModelUpgrader: {e}\")\n        print(\"Please make sure you're in the sentinel-ai directory and the repository is properly cloned.\")\n        raise e\n\n# Import controller-plasticity integration\ntry:\n    from scripts.controller_plasticity_integration import ControllerPlasticityIntegration\n    print(\"‚úÖ Successfully imported ControllerPlasticityIntegration\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import ControllerPlasticityIntegration: {e}\")\n    print(\"Some advanced features may not be available.\")\n\n# Import pruning and utils\ntry:\n    from utils.pruning.pruning_module import PruningModule\n    from utils.pruning.strategies import EntropyPruningStrategy, GradientPruningStrategy\n    from utils.pruning.fine_tuner import FineTuner\n    print(\"‚úÖ Successfully imported pruning modules\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import pruning modules: {e}\")\n\n# Import adaptive modules\ntry:\n    from utils.adaptive.adaptive_plasticity import AdaptivePlasticitySystem\n    print(\"‚úÖ Successfully imported adaptive plasticity system\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import adaptive plasticity system: {e}\")\n\n# Import model adapters if available\ntry:\n    from models.adaptive_transformer import AdaptiveTransformer\n    from models.unet_transformer import UNetTransformer\n    print(\"‚úÖ Successfully imported adaptive model adapters\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import adaptive model adapters: {e}\")\n\nprint(\"‚úÖ Successfully imported all required modules\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Selection\n\nSelect a model to upgrade with Sentinel-AI's adaptive plasticity system:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model selection based on available GPU\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n    \n    if 'A100' in gpu_name and gpu_memory > 30:\n        # Recommend larger models for A100\n        print(\"A100 GPU detected! Recommended models:\")\n        print(\"- facebook/opt-1.3b (1.3B parameters)\")\n        print(\"- EleutherAI/pythia-1.4b (1.4B parameters)\")\n        print(\"- bigscience/bloom-1b7 (1.7B parameters)\")\n        \n        # Default to a medium-sized model\n        DEFAULT_MODEL = \"EleutherAI/pythia-1.4b\"\n    else:\n        # Recommend smaller models for other GPUs\n        print(f\"{gpu_name} GPU detected! Recommended models:\")\n        print(\"- distilgpt2 (82M parameters)\")\n        print(\"- facebook/opt-350m (350M parameters)\")\n        print(\"- EleutherAI/pythia-410m (410M parameters)\")\n        \n        # Default to a smaller model\n        DEFAULT_MODEL = \"distilgpt2\"\nelse:\n    # CPU-only recommendations\n    print(\"CPU detected. Recommended smaller models:\")\n    print(\"- distilgpt2 (82M parameters)\")\n    print(\"- facebook/opt-125m (125M parameters)\")\n    print(\"- EleutherAI/pythia-70m (70M parameters)\")\n    \n    # Default to the smallest viable model\n    DEFAULT_MODEL = \"distilgpt2\"\n\n# Set model name - CHANGE THIS LINE TO SELECT A DIFFERENT MODEL\nMODEL_NAME = DEFAULT_MODEL\n\nprint(f\"\\nüëâ Selected model: {MODEL_NAME}\")\n\n# Verify the model can be loaded\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"‚úÖ Successfully loaded tokenizer for {MODEL_NAME}\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading tokenizer: {e}\")\n    print(\"Please select a different model.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nConfigure the Upgrayedd system with advanced options:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Advanced configuration options\nconfig = {\n    # Dataset selection\n    \"dataset\": \"tiny_shakespeare\",  # Options: tiny_shakespeare, wikitext, custom\n    \n    # Optimization cycles\n    \"cycles\": 3,                    # Number of plasticity cycles to run\n    \n    # Pruning settings\n    \"pruning_level\": 0.3,           # Initial pruning level (30% of heads)\n    \"growth_ratio\": 0.5,            # Growth ratio (50% of pruned heads)\n    \n    # Controller settings\n    \"controller_config\": {\n        \"controller_type\": \"ann\",   # Options: ann, static\n        \"controller_lr\": 0.01,      # Controller learning rate\n        \"update_frequency\": 50,     # Update frequency\n        \"warmup_steps\": 100,        # Warmup steps\n        \"entropy_threshold\": 0.7,   # Entropy threshold for gating\n        \"gradient_scale\": 1.0,      # Gradient scale for updates\n    },\n    \n    # Plasticity settings\n    \"plasticity_config\": {\n        \"max_degeneration_score\": 3.0,   # Maximum acceptable degeneration score\n        \"max_perplexity_increase\": 0.15, # Maximum acceptable perplexity increase\n        \"training_steps\": 100,           # Training steps per cycle\n        \"memory_capacity\": 5,            # Memory capacity for recording transformations\n        \"entropy_weighted\": True,        # Whether to use entropy weighting\n    },\n    \n    # Learning settings\n    \"learning_rate\": 5e-5,          # Learning rate for fine-tuning\n    \"training_epochs\": 3,           # Training epochs per cycle\n    \"batch_size\": 4,                # Batch size for training\n    \"gradient_accumulation\": 4,     # Gradient accumulation steps\n    \n    # Output options\n    \"compress_model\": True,         # Whether to compress the model after optimization\n    \"compression_type\": \"mask\",     # Options: mask, remove, distill\n    \"run_inference\": True,          # Run inference after optimization\n    \"plot\": True,                   # Generate visualizations\n    \"log_metrics\": True,            # Log detailed metrics\n}\n\n# Update based on hardware constraints\nif torch.cuda.is_available():\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    if gpu_memory > 30:  # A100 or similar\n        config[\"batch_size\"] = 8\n        config[\"gradient_accumulation\"] = 1\n    elif gpu_memory > 15:  # V100 or similar\n        config[\"batch_size\"] = 4\n        config[\"gradient_accumulation\"] = 2\n    else:  # Smaller GPUs\n        config[\"batch_size\"] = 2\n        config[\"gradient_accumulation\"] = 4\nelse:\n    # CPU settings\n    config[\"batch_size\"] = 1\n    config[\"gradient_accumulation\"] = 8\n\n# For different models, adjust some settings\nif \"pythia\" in MODEL_NAME or \"bloom\" in MODEL_NAME:\n    # Adjust for Pythia/BLOOM models\n    config[\"plasticity_config\"][\"entropy_weighted\"] = False\n    \nif \"llama\" in MODEL_NAME:\n    # Adjust for Llama models\n    config[\"plasticity_config\"][\"max_perplexity_increase\"] = 0.2\n\n# Display configuration\nprint(\"üìä Upgrayedd Configuration:\")\nprint(f\"- Cycles: {config['cycles']}\")\nprint(f\"- Pruning level: {config['pruning_level']}\")\nprint(f\"- Growth ratio: {config['growth_ratio']}\")\nprint(f\"- Controller type: {config['controller_config']['controller_type']}\")\nprint(f\"- Learning rate: {config['learning_rate']}\")\nprint(f\"- Batch size: {config['batch_size']} (gradient accumulation: {config['gradient_accumulation']})\")\nprint(f\"- Compression: {config['compress_model']} (type: {config['compression_type']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare Dataset\n\nLet's prepare the dataset for training our model:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare the dataset\ndef prepare_dataset(dataset_name=\"tiny_shakespeare\", tokenizer=None, max_length=512):\n    \"\"\"Prepare and tokenize dataset for training.\"\"\"\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be provided\")\n    \n    # Load the specified dataset\n    if dataset_name == \"tiny_shakespeare\":\n        # Shakespeare dataset\n        print(\"üìö Loading Tiny Shakespeare dataset...\")\n        \n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\"tiny_shakespeare\")\n            \n            # Basic dataset info\n            print(f\"Dataset size: {len(dataset['train'])} entries\")\n            \n            # Function to tokenize and chunk text\n            def tokenize_function(examples):\n                return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n            \n            # Tokenize dataset\n            tokenized_dataset = dataset.map(\n                tokenize_function,\n                batched=True,\n                remove_columns=[\"text\"]\n            )\n            \n            print(\"‚úÖ Dataset preparation complete\")\n            return tokenized_dataset\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading dataset: {e}\")\n            raise\n            \n    elif dataset_name == \"wikitext\":\n        # WikiText dataset\n        print(\"üìö Loading WikiText dataset...\")\n        \n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n            \n            # Basic dataset info\n            print(f\"Dataset size: {len(dataset['train'])} entries\")\n            \n            # Function to tokenize and chunk text\n            def tokenize_function(examples):\n                return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n            \n            # Tokenize dataset\n            tokenized_dataset = dataset.map(\n                tokenize_function,\n                batched=True,\n                remove_columns=[\"text\"]\n            )\n            \n            print(\"‚úÖ Dataset preparation complete\")\n            return tokenized_dataset\n            \n        except Exception as e:\n            print(f\"‚ùå Error loading dataset: {e}\")\n            raise\n            \n    else:\n        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n\n# Load and prepare the dataset\ntry:\n    tokenized_dataset = prepare_dataset(\n        dataset_name=config[\"dataset\"],\n        tokenizer=tokenizer,\n        max_length=512\n    )\n    print(f\"‚úÖ Successfully prepared {config['dataset']} dataset\")\nexcept Exception as e:\n    print(f\"‚ùå Error preparing dataset: {e}\")\n    \n    # Fallback to dummy dataset if there's an error\n    print(\"‚ö†Ô∏è Using dummy dataset for demonstration\")\n    \n    # Create dummy data\n    dummy_input_ids = torch.randint(0, 1000, (100, 512))\n    dummy_attention_mask = torch.ones((100, 512))\n    \n    # Create dummy dataset\n    from datasets import Dataset\n    \n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": dummy_input_ids.tolist(),\n        \"attention_mask\": dummy_attention_mask.tolist()\n    })\n    \n    # Split into train and validation\n    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n    \n    print(\"‚úÖ Created dummy dataset for demonstration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run Upgrayedd Transformation\n\nNow, let's run the full Upgrayedd transformation process on the selected model:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Run the Upgrayedd transformation\noutput_dir = f\"./output/upgrayedd_{MODEL_NAME.split('/')[-1]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\nprint(f\"üöÄ Starting Upgrayedd transformation on {MODEL_NAME}\")\nprint(f\"üìÇ Output directory: {output_dir}\")\nprint(f\"‚öôÔ∏è Cycles: {config['cycles']}, Pruning level: {config['pruning_level']}, Growth ratio: {config['growth_ratio']}\")\nprint(\"\\n‚ö†Ô∏è This process will take several hours with real training!\")\nprint(\"‚åõ You can interrupt at any point with Ctrl+C and the process will try to continue to the next phase\")\n\n# Create ModelUpgrader instance\nupgrader = ModelUpgrader(\n    model_name=MODEL_NAME,\n    output_dir=output_dir,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    config=config,\n    verbose=True\n)\n\n# Run the upgrade process\ntry:\n    result = upgrader.upgrade()\n    \n    if result:\n        print(\"\\n‚úÖ Upgrayedd transformation completed successfully!\")\n        print(f\"üìÇ The upgraded model is saved in: {output_dir}/hf_model\")\n        \n        # Extract key metrics\n        final_perplexity = result.get(\"final_perplexity\", \"N/A\")\n        improvement = result.get(\"improvement\", 0) * 100\n        head_reduction = result.get(\"pruned_heads_percent\", 0) * 100\n        \n        print(f\"üìä Performance improvement: {improvement:.1f}%\")\n        print(f\"üìä Head reduction: {head_reduction:.1f}%\")\n    else:\n        print(\"\\n‚ùå Upgrayedd transformation failed!\")\nexcept KeyboardInterrupt:\n    print(\"\\n‚ö†Ô∏è Transformation was interrupted by user!\")\n    print(\"Some results may be available in the output directory.\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error during transformation: {e}\")\n    print(traceback.format_exc())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Results\n\nNow, let's visualize the results of the transformation:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the optimization results\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\n\n# Set color schemes for plots\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"viridis\")\nplt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 8)})\n\n# Function to load metrics from jsonl file\ndef load_metrics(metrics_dir):\n    \"\"\"Load metrics from JSONL file.\"\"\"\n    metrics_file = os.path.join(metrics_dir, \"integration_metrics.jsonl\")\n    \n    if not os.path.exists(metrics_file):\n        print(f\"‚ùå Metrics file not found: {metrics_file}\")\n        return None\n    \n    metrics = []\n    try:\n        with open(metrics_file, 'r') as f:\n            for line in f:\n                metrics.append(json.loads(line))\n        return metrics\n    except Exception as e:\n        print(f\"‚ùå Error loading metrics: {e}\")\n        return None\n\n# Function to create visualization plots\ndef create_visualizations(metrics):\n    \"\"\"Create visualization plots from metrics.\"\"\"\n    if not metrics:\n        print(\"‚ùå No metrics available for visualization\")\n        return\n    \n    # Extract baseline and cycle metrics\n    baseline_metrics = [m for m in metrics if m.get('phase') == 'baseline']\n    cycle_metrics = [m for m in metrics if m.get('phase') == 'cycle_complete']\n    \n    if not baseline_metrics or not cycle_metrics:\n        print(\"‚ùå Insufficient metrics for visualization\")\n        return\n    \n    # Prepare cycle-based data\n    cycles = [m['cycle'] for m in cycle_metrics]\n    perplexities = [m.get('final_perplexity', 0) for m in cycle_metrics]\n    active_heads = [m.get('active_heads', 0) for m in cycle_metrics]\n    pruned_perplexities = [m.get('pruned_perplexity', 0) for m in cycle_metrics]\n    grown_perplexities = [m.get('grown_perplexity', 0) for m in cycle_metrics]\n    \n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(18, 12))\n    \n    # 1. Perplexity Over Cycles\n    ax1 = plt.subplot(2, 2, 1)\n    ax1.plot(cycles, perplexities, 'o-', linewidth=2, markersize=8, label='Final Perplexity')\n    \n    if all(p > 0 for p in pruned_perplexities) and all(p > 0 for p in grown_perplexities):\n        ax1.plot(cycles, pruned_perplexities, 's--', alpha=0.7, label='After Pruning')\n        ax1.plot(cycles, grown_perplexities, '^--', alpha=0.7, label='After Growth')\n    \n    ax1.set_title('Perplexity Across Optimization Cycles', fontsize=14)\n    ax1.set_xlabel('Cycle', fontsize=12)\n    ax1.set_ylabel('Perplexity', fontsize=12)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. Active Heads Over Cycles\n    ax2 = plt.subplot(2, 2, 2)\n    ax2.plot(cycles, active_heads, 'o-', color='green', linewidth=2, markersize=8)\n    ax2.set_title('Active Attention Heads After Each Cycle', fontsize=14)\n    ax2.set_xlabel('Cycle', fontsize=12)\n    ax2.set_ylabel('Number of Active Heads', fontsize=12)\n    \n    # Add baseline head count as a horizontal line\n    if baseline_metrics and 'active_heads' in baseline_metrics[0]:\n        baseline_heads = baseline_metrics[0]['active_heads']\n        ax2.axhline(y=baseline_heads, color='red', linestyle='--', alpha=0.7,\n                   label=f'Baseline ({baseline_heads} heads)')\n        ax2.legend()\n    \n    ax2.grid(True, alpha=0.3)\n    \n    # 3. Perplexity Changes Within Cycles\n    if all(p > 0 for p in pruned_perplexities) and all(p > 0 for p in grown_perplexities):\n        ax3 = plt.subplot(2, 2, 3)\n        \n        # Prepare data for grouped bar chart\n        cycle_labels = [f'Cycle {c}' for c in cycles]\n        x = np.arange(len(cycle_labels))\n        width = 0.25\n        \n        # Plot bars for each phase\n        initial_perplexities = [m.get('initial_perplexity', 0) for m in cycle_metrics]\n        \n        ax3.bar(x - width, initial_perplexities, width, label='Initial')\n        ax3.bar(x, pruned_perplexities, width, label='After Pruning')\n        ax3.bar(x + width, perplexities, width, label='Final')\n        \n        ax3.set_title('Perplexity Changes Within Each Cycle', fontsize=14)\n        ax3.set_xlabel('Optimization Cycle', fontsize=12)\n        ax3.set_ylabel('Perplexity', fontsize=12)\n        ax3.set_xticks(x)\n        ax3.set_xticklabels(cycle_labels)\n        ax3.legend()\n        ax3.grid(True, alpha=0.3, axis='y')\n    \n    # 4. Head Reduction vs Perplexity Improvement\n    if all(p > 0 for p in perplexities) and all(a > 0 for a in active_heads):\n        ax4 = plt.subplot(2, 2, 4)\n        \n        # Calculate improvement percentages\n        if baseline_metrics and 'perplexity' in baseline_metrics[0] and 'active_heads' in baseline_metrics[0]:\n            baseline_perp = baseline_metrics[0]['perplexity']\n            baseline_heads = baseline_metrics[0]['active_heads']\n            \n            perp_improvements = [(baseline_perp - p) / baseline_perp * 100 for p in perplexities]\n            head_reductions = [(baseline_heads - a) / baseline_heads * 100 for a in active_heads]\n            \n            for i, cycle in enumerate(cycles):\n                ax4.annotate(f'Cycle {cycle}', (head_reductions[i], perp_improvements[i]),\n                           xytext=(5, 5), textcoords='offset points')\n            \n            ax4.plot(head_reductions, perp_improvements, 'o-', linewidth=2, markersize=8, color='purple')\n            ax4.set_title('Perplexity Improvement vs Head Reduction', fontsize=14)\n            ax4.set_xlabel('Head Reduction (%)', fontsize=12)\n            ax4.set_ylabel('Perplexity Improvement (%)', fontsize=12)\n            ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n    # Efficiency plots\n    if baseline_metrics and 'active_heads' in baseline_metrics[0] and 'perplexity' in baseline_metrics[0]:\n        baseline_perp = baseline_metrics[0]['perplexity']\n        baseline_heads = baseline_metrics[0]['active_heads']\n        \n        # Calculate efficiency (perplexity per head)\n        baseline_efficiency = baseline_perp / baseline_heads\n        efficiencies = [p / a for p, a in zip(perplexities, active_heads)]\n        efficiency_improvements = [(baseline_efficiency - e) / baseline_efficiency * 100 for e in efficiencies]\n        \n        # Create efficiency figure\n        plt.figure(figsize=(10, 6))\n        plt.plot(cycles, efficiencies, 'o-', linewidth=2, markersize=8, color='teal')\n        plt.axhline(y=baseline_efficiency, color='red', linestyle='--', alpha=0.7,\n                   label=f'Baseline ({baseline_efficiency:.3f})')\n        plt.title('Model Efficiency (Perplexity per Head)', fontsize=14)\n        plt.xlabel('Cycle', fontsize=12)\n        plt.ylabel('Efficiency (Lower is Better)', fontsize=12)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    \n    return True\n\n# Create visualizations from metrics\nmetrics_dir = os.path.join(output_dir, \"metrics\")\nmetrics = load_metrics(metrics_dir)\n\nif metrics:\n    print(\"üìä Creating visualizations from optimization metrics...\")\n    success = create_visualizations(metrics)\n    if success:\n        print(\"‚úÖ Visualizations created successfully\")\nelse:\n    print(\"‚ö†Ô∏è No metrics available. Visualizations cannot be created.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Compare Model Outputs\n\nLet's compare the output of the original and upgraded models:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Compare original and upgraded models\nfrom transformers import AutoModelForCausalLM\n\ndef compare_models(original_model_name, upgraded_model_path, prompts, max_length=100):\n    \"\"\"Compare text generation between original and upgraded models.\"\"\"\n    # Load models and tokenizers\n    try:\n        print(f\"üìö Loading original model: {original_model_name}\")\n        original_tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n        if original_tokenizer.pad_token is None:\n            original_tokenizer.pad_token = original_tokenizer.eos_token\n            \n        original_model = AutoModelForCausalLM.from_pretrained(original_model_name)\n        \n        print(f\"‚úÖ Successfully loaded original model\")\n    except Exception as e:\n        print(f\"‚ùå Error loading original model: {e}\")\n        return False\n    \n    try:\n        print(f\"üìö Loading upgraded model: {upgraded_model_path}\")\n        upgraded_tokenizer = AutoTokenizer.from_pretrained(upgraded_model_path)\n        if upgraded_tokenizer.pad_token is None:\n            upgraded_tokenizer.pad_token = upgraded_tokenizer.eos_token\n            \n        upgraded_model = AutoModelForCausalLM.from_pretrained(upgraded_model_path)\n        \n        print(f\"‚úÖ Successfully loaded upgraded model\")\n    except Exception as e:\n        print(f\"‚ùå Error loading upgraded model: {e}\")\n        print(\"Using original model for comparison to demonstrate the interface\")\n        upgraded_model = original_model\n        upgraded_tokenizer = original_tokenizer\n        \n    # Move to correct device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    original_model.to(device)\n    upgraded_model.to(device)\n    \n    # Settings for generation\n    generation_config = {\n        \"max_length\": max_length,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"top_k\": 50,\n        \"pad_token_id\": original_tokenizer.pad_token_id\n    }\n    \n    # Compare generations for each prompt\n    for prompt in prompts:\n        print(f\"\\n{'=' * 40}\\nüìù PROMPT: {prompt}\\n{'=' * 40}\")\n        \n        # Generate with original model\n        try:\n            print(\"\\nüîç ORIGINAL MODEL OUTPUT:\")\n            \n            inputs = original_tokenizer(prompt, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs = original_model.generate(**inputs, **generation_config)\n            \n            text = original_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            formatted_text = text if len(text) <= 500 else text[:500] + \"...\"\n            print(formatted_text)\n        except Exception as e:\n            print(f\"‚ùå Error generating with original model: {e}\")\n        \n        # Generate with upgraded model\n        try:\n            print(\"\\nüåü UPGRADED MODEL OUTPUT:\")\n            \n            inputs = upgraded_tokenizer(prompt, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs = upgraded_model.generate(**inputs, **generation_config)\n            \n            text = upgraded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            formatted_text = text if len(text) <= 500 else text[:500] + \"...\"\n            print(formatted_text)\n        except Exception as e:\n            print(f\"‚ùå Error generating with upgraded model: {e}\")\n            \n        print(\"\\n\" + \"-\" * 80)\n    \n    return True\n\n# Define prompts for comparison\nprompts = [\n    \"The future of artificial intelligence is\",\n    \"The most interesting aspect of neural networks is\",\n    \"In five years, language models will\",\n    \"The key to efficient model design is\",\n]\n\n# Run comparison\nhf_model_dir = os.path.join(output_dir, \"hf_model\")\nif os.path.exists(hf_model_dir):\n    print(\"üîÑ Comparing original and upgraded models...\")\n    compare_models(MODEL_NAME, hf_model_dir, prompts)\nelse:\n    print(f\"‚ö†Ô∏è Upgraded model directory not found: {hf_model_dir}\")\n    print(\"Comparing original model with itself for demonstration\")\n    compare_models(MODEL_NAME, MODEL_NAME, prompts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Generate Performance Summary\n\nLet's generate a summary of the model's performance improvements:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Generate performance summary\nfrom IPython.display import display, HTML\n\ndef generate_performance_summary(metrics_dir):\n    \"\"\"Generate a summary of model performance improvements.\"\"\"\n    metrics_file = os.path.join(metrics_dir, \"integration_metrics.jsonl\")\n    \n    if not os.path.exists(metrics_file):\n        print(f\"‚ùå Metrics file not found: {metrics_file}\")\n        \n        # Create demonstration metrics\n        baseline_perplexity = 25.7\n        final_perplexity = 18.2\n        baseline_heads = 72\n        final_heads = 48\n        \n        print(\"‚ö†Ô∏è Using demonstration metrics for summary\")\n    else:\n        # Load metrics\n        metrics = []\n        with open(metrics_file, 'r') as f:\n            for line in f:\n                metrics.append(json.loads(line))\n        \n        # Extract key metrics\n        baseline_metrics = [m for m in metrics if m.get('phase') == 'baseline']\n        cycle_metrics = [m for m in metrics if m.get('phase') == 'cycle_complete']\n        \n        if not baseline_metrics or not cycle_metrics:\n            print(\"‚ùå Insufficient metrics for summary\")\n            return\n        \n        baseline_perplexity = baseline_metrics[0].get('perplexity', 0)\n        final_perplexity = cycle_metrics[-1].get('final_perplexity', 0)\n        baseline_heads = baseline_metrics[0].get('active_heads', 0)\n        final_heads = cycle_metrics[-1].get('active_heads', 0)\n    \n    # Calculate improvements\n    perplexity_improvement = ((baseline_perplexity - final_perplexity) / baseline_perplexity) * 100 if baseline_perplexity > 0 else 0\n    head_reduction = ((baseline_heads - final_heads) / baseline_heads) * 100 if baseline_heads > 0 else 0\n    \n    # Calculate efficiency metrics\n    baseline_efficiency = baseline_perplexity / baseline_heads if baseline_heads > 0 else 0\n    final_efficiency = final_perplexity / final_heads if final_heads > 0 else 0\n    efficiency_improvement = ((baseline_efficiency - final_efficiency) / baseline_efficiency) * 100 if baseline_efficiency > 0 else 0\n    \n    # Create HTML table\n    html = \"\"\"\n    <style>\n        .performance-table {\n            width: 100%;\n            border-collapse: collapse;\n            margin: 20px 0;\n            font-family: Arial, sans-serif;\n        }\n        .performance-table th {\n            background-color: #4CAF50;\n            color: white;\n            padding: 12px;\n            text-align: left;\n            font-weight: bold;\n        }\n        .performance-table td {\n            padding: 12px;\n            text-align: left;\n            border-bottom: 1px solid #ddd;\n        }\n        .performance-table tr:nth-child(even) {\n            background-color: #f9f9f9;\n        }\n        .positive {\n            color: green;\n            font-weight: bold;\n        }\n        .negative {\n            color: red;\n            font-weight: bold;\n        }\n        .title {\n            font-size: 24px;\n            font-weight: bold;\n            margin: 20px 0;\n            color: #333;\n        }\n        .subtitle {\n            font-size: 18px;\n            color: #666;\n            margin-bottom: 20px;\n        }\n    </style>\n    \n    <div class=\"title\">Upgrayedd Performance Summary</div>\n    <div class=\"subtitle\">Model: {}</div>\n    \n    <table class=\"performance-table\">\n        <tr>\n            <th>Metric</th>\n            <th>Before</th>\n            <th>After</th>\n            <th>Change</th>\n        </tr>\n        <tr>\n            <td>Perplexity</td>\n            <td>{:.2f}</td>\n            <td>{:.2f}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n        <tr>\n            <td>Active Heads</td>\n            <td>{}</td>\n            <td>{}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n        <tr>\n            <td>Efficiency (Perplexity/Head)</td>\n            <td>{:.3f}</td>\n            <td>{:.3f}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n    </table>\n    \n    <div style=\"margin-top: 30px; font-weight: bold;\">Key Findings:</div>\n    <ul>\n        <li>Performance {}improved by {:.1f}% while reducing model complexity</li>\n        <li>Model size reduced by {:.1f}% through strategic head pruning</li>\n        <li>Overall model efficiency {}improved by {:.1f}%</li>\n    </ul>\n    \"\"\".format(\n        MODEL_NAME,\n        baseline_perplexity, final_perplexity, \n        \"positive\" if perplexity_improvement > 0 else \"negative\",\n        f\"-{perplexity_improvement:.1f}\" if perplexity_improvement > 0 else f\"+{-perplexity_improvement:.1f}\",\n        baseline_heads, final_heads,\n        \"positive\" if head_reduction > 0 else \"negative\",\n        f\"-{head_reduction:.1f}\" if head_reduction > 0 else f\"+{-head_reduction:.1f}\",\n        baseline_efficiency, final_efficiency,\n        \"positive\" if efficiency_improvement > 0 else \"negative\",\n        f\"-{efficiency_improvement:.1f}\" if efficiency_improvement > 0 else f\"+{-efficiency_improvement:.1f}\",\n        \"\" if perplexity_improvement > 0 else \"did not \",\n        abs(perplexity_improvement),\n        head_reduction,\n        \"\" if efficiency_improvement > 0 else \"did not \",\n        abs(efficiency_improvement)\n    )\n    \n    display(HTML(html))\n    \n    # Also print text version for non-HTML environments\n    print(\"\\nPERFORMANCE SUMMARY:\")\n    print(\"=\" * 50)\n    print(f\"Model: {MODEL_NAME}\")\n    print(f\"Perplexity: {baseline_perplexity:.2f} ‚Üí {final_perplexity:.2f} ({perplexity_improvement:.1f}% improvement)\")\n    print(f\"Active Heads: {baseline_heads} ‚Üí {final_heads} ({head_reduction:.1f}% reduction)\")\n    print(f\"Efficiency: {baseline_efficiency:.3f} ‚Üí {final_efficiency:.3f} ({efficiency_improvement:.1f}% improvement)\")\n    print(\"=\" * 50)\n\n# Generate summary\nmetrics_dir = os.path.join(output_dir, \"metrics\")\ngenerate_performance_summary(metrics_dir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nAfter completing the model transformation, you might want to:\n\n1. **Try Different Models**: Experiment with larger models like OPT-1.3B, Pythia-1.4B, or BLOOM-1B7.\n\n2. **Adjust Configurations**: Modify pruning levels, growth ratios, and learning rates to optimize for your specific use case.\n\n3. **Use Custom Datasets**: Replace the sample datasets with your own domain-specific data.\n\n4. **Export the Model**: Use the upgraded model for inference in your applications:\n   ```python\n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   \n   # Replace with your actual output path\n   model_path = \"./output/upgrayedd_distilgpt2_20250405_120000/hf_model\"\n   \n   # Load the model and tokenizer\n   model = AutoModelForCausalLM.from_pretrained(model_path)\n   tokenizer = AutoTokenizer.from_pretrained(model_path)\n   \n   # Generate text\n   inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\")\n   outputs = model.generate(**inputs, max_length=100)\n   print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n   ```\n\n5. **Benchmark**: Compare the upgraded model with the original model in terms of:\n   - Inference speed\n   - Memory usage\n   - Perplexity on different datasets\n   - Downstream task performance\n\n6. **Further Compress**: If needed, use additional compression techniques like quantization to further reduce the model size.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## How Controller-Plasticity Integration Works\n\nThe core of Upgrayedd is the Controller-Plasticity Integration system, which creates a feedback loop for continuous optimization:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                                                                        ‚îÇ\n‚îÇ                          OPTIMIZATION CYCLE                            ‚îÇ\n‚îÇ                                                                        ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n‚îÇ  ‚îÇ                    ‚îÇ                      ‚îÇ                     ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  CONTROLLER SYSTEM ‚îÇ                      ‚îÇ PLASTICITY SYSTEM   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ                    ‚îÇ                      ‚îÇ                     ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Analyze head ‚îÇ  ‚îÇ                      ‚îÇ ‚îÇ Prune heads   ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ metrics      ‚îÇ  ‚îÇ                      ‚îÇ ‚îÇ based on      ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                      ‚îÇ ‚îÇ controller    ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ         ‚îÇ          ‚îÇ                      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ         ‚ñº          ‚îÇ                      ‚îÇ         ‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                      ‚îÇ         ‚ñº           ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Generate     ‚îÇ  ‚îÇ     Gate values      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ gate values  ‚îÇ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ ‚îÇ Measure       ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                      ‚îÇ ‚îÇ impact        ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ         ‚îÇ          ‚îÇ                      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ         ‚ñº          ‚îÇ                      ‚îÇ         ‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ                      ‚îÇ         ‚ñº           ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ Update       ‚îÇ  ‚îÇ      Metrics         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îÇ controller   ‚îÇ‚óÑ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îÇ ‚îÇ Grow heads    ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ                     ‚îÇ‚îÇ ‚îÇ strategically ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ                    ‚îÇ                     ‚îÇ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ‚îÇ         ‚îÇ           ‚îÇ   ‚îÇ\n‚îÇ                                             ‚îÇ‚îÇ         ‚ñº           ‚îÇ   ‚îÇ\n‚îÇ                                             ‚îÇ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n‚îÇ                                             ‚îÇ‚îÇ ‚îÇ Apply         ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ                                             ‚îî‚î§ ‚îÇ differential  ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ                                              ‚îÇ ‚îÇ learning      ‚îÇ   ‚îÇ   ‚îÇ\n‚îÇ                                              ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n‚îÇ                                              ‚îÇ                     ‚îÇ   ‚îÇ\n‚îÇ                                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n‚îÇ                                                                        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\nThis integration creates a virtuous cycle:\n\n1. **Controller Analysis**: The neural controller analyzes head importance metrics\n2. **Dynamic Gating**: Generates gate values to indicate which heads should be kept or pruned\n3. **Guided Pruning**: The plasticity system prunes heads according to controller guidance\n4. **Impact Measurement**: System measures the effect of pruning on model performance\n5. **Strategic Regrowth**: Heads are regrown in areas that need them most\n6. **Differential Learning**: Fine-tuning with specialized learning rates for different heads\n7. **Controller Update**: The controller learns from the results, improving over time\n\nThrough repeated cycles, this system creates a self-optimizing neural network that continuously adapts its structure for maximum efficiency."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}