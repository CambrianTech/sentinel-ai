{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "ğŸ‘¾ Upgrayedd Runtime: Transform Models with Adaptive Pruning and Regrowth (v1.0.0)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CambrianTech/sentinel-ai/blob/feature/implement-adaptive-plasticity/colab_notebooks/UpgrayeddColab.ipynb)\n\nThis notebook demonstrates the full Upgrayedd adaptive transformation system, which turns any HuggingFace transformer model into a self-optimizing network through:\n\n1. **Controller-guided pruning**: Dynamically identifies and removes unnecessary attention heads\n2. **Strategic regrowth**: Reconstructs critical pathways where needed\n3. **Differential learning**: Applies custom learning rates for optimal adaptation\n4. **Compression options**: Optional size reduction with pruned models\n\n> *Spelled with two D's for a double dose of adaptive optimization.*",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Environment Setup\n\nFirst, check if we're running in a Colab environment and set up GPU acceleration:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Check environment and GPU availability\nimport sys\nimport torch\nimport os\nfrom datetime import datetime\n\n# Determine if running in Colab\nIN_COLAB = 'google.colab' in sys.modules\nprint(f\"Running in Colab: {IN_COLAB}\")\n\n# Check GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"ğŸš€ GPU available: {gpu_name}\")\n    \n    # Display memory info\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    \n    # For A100, recommend using larger models\n    if 'A100' in gpu_name:\n        print(\"ğŸ’ª A100 detected! You can use larger models (up to 7B parameters)\")\n    \n    # Set default device\n    torch.set_default_device(device)\nelse:\n    device = torch.device(\"cpu\")\n    print(\"âš ï¸ No GPU detected. Running on CPU will be significantly slower.\")\n\n# Install required packages\nif IN_COLAB:\n    print(\"Installing required packages...\")\n    !pip install -q transformers datasets accelerate evaluate bitsandbytes safetensors"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository and set up the environment\nif IN_COLAB:\n    # Clone the repository\n    print(\"Cloning Sentinel-AI repository...\")\n    !git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n    \n    # Add to Python path\n    import sys\n    sys.path.append('/content/sentinel-ai')\n    \n    # Change to the repository directory\n    %cd /content/sentinel-ai\n    \n    # Create output directory\n    !mkdir -p output\nelse:\n    # If not in Colab, assume we're already in the right directory\n    print(\"Running in local environment, make sure you're in the sentinel-ai directory\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Import Required Modules\n\nNow we'll import the actual Upgrayedd system components:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import core Sentinel-AI components\nimport os\nimport sys\nimport time\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nimport logging\nimport json\nimport re\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler()]\n)\nlogger = logging.getLogger(\"Upgrayedd\")\n\n# Import transformers components\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)\n\n# Import datasets\nfrom datasets import load_dataset\n\n# Import Sentinel-AI components\ntry:\n    # Direct import from scripts (most reliable path)\n    from scripts.upgrayedd import ModelUpgrader\n    print(\"âœ… Successfully imported ModelUpgrader directly from scripts\")\nexcept ImportError as e:\n    # Try wrapper modules\n    try:\n        from sentinel.upgrayedd import ModelUpgrader\n        print(\"âœ… Successfully imported ModelUpgrader from sentinel package\")\n    except ImportError as e:\n        print(f\"âŒ Could not import ModelUpgrader: {e}\")\n        print(\"Please make sure you're in the sentinel-ai directory and the repository is properly cloned.\")\n        raise e\n\n# Import controller-plasticity integration\ntry:\n    from scripts.controller_plasticity_integration import ControllerPlasticityIntegration\n    print(\"âœ… Successfully imported ControllerPlasticityIntegration\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Could not import ControllerPlasticityIntegration: {e}\")\n    print(\"Some advanced features may not be available.\")\n\n# Import pruning and utils\ntry:\n    from utils.pruning.pruning_module import PruningModule\n    from utils.pruning.strategies import EntropyPruningStrategy, GradientPruningStrategy\n    from utils.pruning.fine_tuner import FineTuner\n    print(\"âœ… Successfully imported pruning modules\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Could not import pruning modules: {e}\")\n\n# Import adaptive modules\ntry:\n    from utils.adaptive.adaptive_plasticity import AdaptivePlasticitySystem\n    print(\"âœ… Successfully imported adaptive plasticity system\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Could not import adaptive plasticity system: {e}\")\n\n# Import model adapters if available\ntry:\n    from models.adaptive_transformer import AdaptiveTransformer\n    from models.unet_transformer import UNetTransformer\n    print(\"âœ… Successfully imported adaptive model adapters\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Could not import adaptive model adapters: {e}\")\n\nprint(\"âœ… Successfully imported all required modules\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Selection\n\nSelect a model to upgrade with Sentinel-AI's adaptive plasticity system:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Model selection based on available GPU\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n    \n    if 'A100' in gpu_name and gpu_memory > 30:\n        # Recommend larger models for A100\n        print(\"A100 GPU detected! Recommended models:\")\n        print(\"- facebook/opt-1.3b (1.3B parameters)\")\n        print(\"- EleutherAI/pythia-1.4b (1.4B parameters)\")\n        print(\"- bigscience/bloom-1b7 (1.7B parameters)\")\n        \n        # Default to a medium-sized model\n        DEFAULT_MODEL = \"EleutherAI/pythia-1.4b\"\n    else:\n        # Recommend smaller models for other GPUs\n        print(f\"{gpu_name} GPU detected! Recommended models:\")\n        print(\"- distilgpt2 (82M parameters)\")\n        print(\"- facebook/opt-350m (350M parameters)\")\n        print(\"- EleutherAI/pythia-410m (410M parameters)\")\n        \n        # Default to a smaller model\n        DEFAULT_MODEL = \"distilgpt2\"\nelse:\n    # CPU-only recommendations\n    print(\"CPU detected. Recommended smaller models:\")\n    print(\"- distilgpt2 (82M parameters)\")\n    print(\"- facebook/opt-125m (125M parameters)\")\n    print(\"- EleutherAI/pythia-70m (70M parameters)\")\n    \n    # Default to the smallest viable model\n    DEFAULT_MODEL = \"distilgpt2\"\n\n# Set model name - CHANGE THIS LINE TO SELECT A DIFFERENT MODEL\nMODEL_NAME = DEFAULT_MODEL\n\nprint(f\"\\nğŸ‘‰ Selected model: {MODEL_NAME}\")\n\n# Verify the model can be loaded\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    print(f\"âœ… Successfully loaded tokenizer for {MODEL_NAME}\")\nexcept Exception as e:\n    print(f\"âŒ Error loading tokenizer: {e}\")\n    print(\"Please select a different model.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration\n\nConfigure the Upgrayedd system with advanced options:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Advanced configuration options\nconfig = {\n    # Dataset selection\n    \"dataset\": \"tiny_shakespeare\",  # Options: tiny_shakespeare, wikitext, custom\n    \n    # Optimization cycles\n    \"cycles\": 3,                    # Number of plasticity cycles to run\n    \n    # Pruning settings\n    \"pruning_level\": 0.3,           # Initial pruning level (30% of heads)\n    \"growth_ratio\": 0.5,            # Growth ratio (50% of pruned heads)\n    \n    # Controller settings\n    \"controller_config\": {\n        \"controller_type\": \"ann\",   # Options: ann, static\n        \"controller_lr\": 0.01,      # Controller learning rate\n        \"update_frequency\": 50,     # Update frequency\n        \"warmup_steps\": 100,        # Warmup steps\n        \"entropy_threshold\": 0.7,   # Entropy threshold for gating\n        \"gradient_scale\": 1.0,      # Gradient scale for updates\n    },\n    \n    # Plasticity settings\n    \"plasticity_config\": {\n        \"max_degeneration_score\": 3.0,   # Maximum acceptable degeneration score\n        \"max_perplexity_increase\": 0.15, # Maximum acceptable perplexity increase\n        \"training_steps\": 100,           # Training steps per cycle\n        \"memory_capacity\": 5,            # Memory capacity for recording transformations\n        \"entropy_weighted\": True,        # Whether to use entropy weighting\n    },\n    \n    # Learning settings\n    \"learning_rate\": 5e-5,          # Learning rate for fine-tuning\n    \"training_epochs\": 3,           # Training epochs per cycle\n    \"batch_size\": 4,                # Batch size for training\n    \"gradient_accumulation\": 4,     # Gradient accumulation steps\n    \n    # Output options\n    \"compress_model\": True,         # Whether to compress the model after optimization\n    \"compression_type\": \"mask\",     # Options: mask, remove, distill\n    \"run_inference\": True,          # Run inference after optimization\n    \"plot\": True,                   # Generate visualizations\n    \"log_metrics\": True,            # Log detailed metrics\n}\n\n# Update based on hardware constraints\nif torch.cuda.is_available():\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n    \n    if gpu_memory > 30:  # A100 or similar\n        config[\"batch_size\"] = 8\n        config[\"gradient_accumulation\"] = 1\n    elif gpu_memory > 15:  # V100 or similar\n        config[\"batch_size\"] = 4\n        config[\"gradient_accumulation\"] = 2\n    else:  # Smaller GPUs\n        config[\"batch_size\"] = 2\n        config[\"gradient_accumulation\"] = 4\nelse:\n    # CPU settings\n    config[\"batch_size\"] = 1\n    config[\"gradient_accumulation\"] = 8\n\n# For different models, adjust some settings\nif \"pythia\" in MODEL_NAME or \"bloom\" in MODEL_NAME:\n    # Adjust for Pythia/BLOOM models\n    config[\"plasticity_config\"][\"entropy_weighted\"] = False\n    \nif \"llama\" in MODEL_NAME:\n    # Adjust for Llama models\n    config[\"plasticity_config\"][\"max_perplexity_increase\"] = 0.2\n\n# Display configuration\nprint(\"ğŸ“Š Upgrayedd Configuration:\")\nprint(f\"- Cycles: {config['cycles']}\")\nprint(f\"- Pruning level: {config['pruning_level']}\")\nprint(f\"- Growth ratio: {config['growth_ratio']}\")\nprint(f\"- Controller type: {config['controller_config']['controller_type']}\")\nprint(f\"- Learning rate: {config['learning_rate']}\")\nprint(f\"- Batch size: {config['batch_size']} (gradient accumulation: {config['gradient_accumulation']})\")\nprint(f\"- Compression: {config['compress_model']} (type: {config['compression_type']})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare Dataset\n\nLet's prepare the dataset for training our model:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare the dataset\ndef prepare_dataset(dataset_name=\"tiny_shakespeare\", tokenizer=None, max_length=512):\n    \"\"\"Prepare and tokenize dataset for training.\"\"\"\n    if tokenizer is None:\n        raise ValueError(\"Tokenizer must be provided\")\n    \n    # Load the specified dataset\n    if dataset_name == \"tiny_shakespeare\":\n        # Shakespeare dataset\n        print(\"ğŸ“š Loading Tiny Shakespeare dataset...\")\n        \n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\"tiny_shakespeare\")\n            \n            # Basic dataset info\n            print(f\"Dataset size: {len(dataset['train'])} entries\")\n            \n            # Function to tokenize and chunk text\n            def tokenize_function(examples):\n                return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n            \n            # Tokenize dataset\n            tokenized_dataset = dataset.map(\n                tokenize_function,\n                batched=True,\n                remove_columns=[\"text\"]\n            )\n            \n            print(\"âœ… Dataset preparation complete\")\n            return tokenized_dataset\n            \n        except Exception as e:\n            print(f\"âŒ Error loading dataset: {e}\")\n            raise\n            \n    elif dataset_name == \"wikitext\":\n        # WikiText dataset\n        print(\"ğŸ“š Loading WikiText dataset...\")\n        \n        try:\n            from datasets import load_dataset\n            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n            \n            # Basic dataset info\n            print(f\"Dataset size: {len(dataset['train'])} entries\")\n            \n            # Function to tokenize and chunk text\n            def tokenize_function(examples):\n                return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n            \n            # Tokenize dataset\n            tokenized_dataset = dataset.map(\n                tokenize_function,\n                batched=True,\n                remove_columns=[\"text\"]\n            )\n            \n            print(\"âœ… Dataset preparation complete\")\n            return tokenized_dataset\n            \n        except Exception as e:\n            print(f\"âŒ Error loading dataset: {e}\")\n            raise\n            \n    else:\n        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n\n# Load and prepare the dataset\ntry:\n    tokenized_dataset = prepare_dataset(\n        dataset_name=config[\"dataset\"],\n        tokenizer=tokenizer,\n        max_length=512\n    )\n    print(f\"âœ… Successfully prepared {config['dataset']} dataset\")\nexcept Exception as e:\n    print(f\"âŒ Error preparing dataset: {e}\")\n    \n    # Fallback to dummy dataset if there's an error\n    print(\"âš ï¸ Using dummy dataset for demonstration\")\n    \n    # Create dummy data\n    dummy_input_ids = torch.randint(0, 1000, (100, 512))\n    dummy_attention_mask = torch.ones((100, 512))\n    \n    # Create dummy dataset\n    from datasets import Dataset\n    \n    tokenized_dataset = Dataset.from_dict({\n        \"input_ids\": dummy_input_ids.tolist(),\n        \"attention_mask\": dummy_attention_mask.tolist()\n    })\n    \n    # Split into train and validation\n    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n    \n    print(\"âœ… Created dummy dataset for demonstration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Run Upgrayedd Transformation\n\nNow, let's run the full Upgrayedd transformation process on the selected model:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Run the Upgrayedd transformation\noutput_dir = f\"./output/upgrayedd_{MODEL_NAME.split('/')[-1]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n\nprint(f\"ğŸš€ Starting Upgrayedd transformation on {MODEL_NAME}\")\nprint(f\"ğŸ“‚ Output directory: {output_dir}\")\nprint(f\"âš™ï¸ Cycles: {config['cycles']}, Pruning level: {config['pruning_level']}, Growth ratio: {config['growth_ratio']}\")\nprint(\"\\nâš ï¸ This process will take several hours with real training!\")\nprint(\"âŒ› You can interrupt at any point with Ctrl+C and the process will try to continue to the next phase\")\n\n# Create ModelUpgrader instance\nupgrader = ModelUpgrader(\n    model_name=MODEL_NAME,\n    output_dir=output_dir,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n    config=config,\n    verbose=True\n)\n\n# Run the upgrade process\ntry:\n    result = upgrader.upgrade()\n    \n    if result:\n        print(\"\\nâœ… Upgrayedd transformation completed successfully!\")\n        print(f\"ğŸ“‚ The upgraded model is saved in: {output_dir}/hf_model\")\n        \n        # Extract key metrics\n        final_perplexity = result.get(\"final_perplexity\", \"N/A\")\n        improvement = result.get(\"improvement\", 0) * 100\n        head_reduction = result.get(\"pruned_heads_percent\", 0) * 100\n        \n        print(f\"ğŸ“Š Performance improvement: {improvement:.1f}%\")\n        print(f\"ğŸ“Š Head reduction: {head_reduction:.1f}%\")\n    else:\n        print(\"\\nâŒ Upgrayedd transformation failed!\")\nexcept KeyboardInterrupt:\n    print(\"\\nâš ï¸ Transformation was interrupted by user!\")\n    print(\"Some results may be available in the output directory.\")\nexcept Exception as e:\n    print(f\"\\nâŒ Error during transformation: {e}\")\n    print(traceback.format_exc())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Visualize Results\n\nNow, let's visualize the results of the transformation:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the optimization results\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport os\n\n# Set color schemes for plots\nsns.set_style(\"whitegrid\")\nsns.set_palette(\"viridis\")\nplt.rcParams.update({'font.size': 12, 'figure.figsize': (14, 8)})\n\n# Function to load metrics from jsonl file\ndef load_metrics(metrics_dir):\n    \"\"\"Load metrics from JSONL file.\"\"\"\n    metrics_file = os.path.join(metrics_dir, \"integration_metrics.jsonl\")\n    \n    if not os.path.exists(metrics_file):\n        print(f\"âŒ Metrics file not found: {metrics_file}\")\n        return None\n    \n    metrics = []\n    try:\n        with open(metrics_file, 'r') as f:\n            for line in f:\n                metrics.append(json.loads(line))\n        return metrics\n    except Exception as e:\n        print(f\"âŒ Error loading metrics: {e}\")\n        return None\n\n# Function to create visualization plots\ndef create_visualizations(metrics):\n    \"\"\"Create visualization plots from metrics.\"\"\"\n    if not metrics:\n        print(\"âŒ No metrics available for visualization\")\n        return\n    \n    # Extract baseline and cycle metrics\n    baseline_metrics = [m for m in metrics if m.get('phase') == 'baseline']\n    cycle_metrics = [m for m in metrics if m.get('phase') == 'cycle_complete']\n    \n    if not baseline_metrics or not cycle_metrics:\n        print(\"âŒ Insufficient metrics for visualization\")\n        return\n    \n    # Prepare cycle-based data\n    cycles = [m['cycle'] for m in cycle_metrics]\n    perplexities = [m.get('final_perplexity', 0) for m in cycle_metrics]\n    active_heads = [m.get('active_heads', 0) for m in cycle_metrics]\n    pruned_perplexities = [m.get('pruned_perplexity', 0) for m in cycle_metrics]\n    grown_perplexities = [m.get('grown_perplexity', 0) for m in cycle_metrics]\n    \n    # Create figure with multiple subplots\n    fig = plt.figure(figsize=(18, 12))\n    \n    # 1. Perplexity Over Cycles\n    ax1 = plt.subplot(2, 2, 1)\n    ax1.plot(cycles, perplexities, 'o-', linewidth=2, markersize=8, label='Final Perplexity')\n    \n    if all(p > 0 for p in pruned_perplexities) and all(p > 0 for p in grown_perplexities):\n        ax1.plot(cycles, pruned_perplexities, 's--', alpha=0.7, label='After Pruning')\n        ax1.plot(cycles, grown_perplexities, '^--', alpha=0.7, label='After Growth')\n    \n    ax1.set_title('Perplexity Across Optimization Cycles', fontsize=14)\n    ax1.set_xlabel('Cycle', fontsize=12)\n    ax1.set_ylabel('Perplexity', fontsize=12)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # 2. Active Heads Over Cycles\n    ax2 = plt.subplot(2, 2, 2)\n    ax2.plot(cycles, active_heads, 'o-', color='green', linewidth=2, markersize=8)\n    ax2.set_title('Active Attention Heads After Each Cycle', fontsize=14)\n    ax2.set_xlabel('Cycle', fontsize=12)\n    ax2.set_ylabel('Number of Active Heads', fontsize=12)\n    \n    # Add baseline head count as a horizontal line\n    if baseline_metrics and 'active_heads' in baseline_metrics[0]:\n        baseline_heads = baseline_metrics[0]['active_heads']\n        ax2.axhline(y=baseline_heads, color='red', linestyle='--', alpha=0.7,\n                   label=f'Baseline ({baseline_heads} heads)')\n        ax2.legend()\n    \n    ax2.grid(True, alpha=0.3)\n    \n    # 3. Perplexity Changes Within Cycles\n    if all(p > 0 for p in pruned_perplexities) and all(p > 0 for p in grown_perplexities):\n        ax3 = plt.subplot(2, 2, 3)\n        \n        # Prepare data for grouped bar chart\n        cycle_labels = [f'Cycle {c}' for c in cycles]\n        x = np.arange(len(cycle_labels))\n        width = 0.25\n        \n        # Plot bars for each phase\n        initial_perplexities = [m.get('initial_perplexity', 0) for m in cycle_metrics]\n        \n        ax3.bar(x - width, initial_perplexities, width, label='Initial')\n        ax3.bar(x, pruned_perplexities, width, label='After Pruning')\n        ax3.bar(x + width, perplexities, width, label='Final')\n        \n        ax3.set_title('Perplexity Changes Within Each Cycle', fontsize=14)\n        ax3.set_xlabel('Optimization Cycle', fontsize=12)\n        ax3.set_ylabel('Perplexity', fontsize=12)\n        ax3.set_xticks(x)\n        ax3.set_xticklabels(cycle_labels)\n        ax3.legend()\n        ax3.grid(True, alpha=0.3, axis='y')\n    \n    # 4. Head Reduction vs Perplexity Improvement\n    if all(p > 0 for p in perplexities) and all(a > 0 for a in active_heads):\n        ax4 = plt.subplot(2, 2, 4)\n        \n        # Calculate improvement percentages\n        if baseline_metrics and 'perplexity' in baseline_metrics[0] and 'active_heads' in baseline_metrics[0]:\n            baseline_perp = baseline_metrics[0]['perplexity']\n            baseline_heads = baseline_metrics[0]['active_heads']\n            \n            perp_improvements = [(baseline_perp - p) / baseline_perp * 100 for p in perplexities]\n            head_reductions = [(baseline_heads - a) / baseline_heads * 100 for a in active_heads]\n            \n            for i, cycle in enumerate(cycles):\n                ax4.annotate(f'Cycle {cycle}', (head_reductions[i], perp_improvements[i]),\n                           xytext=(5, 5), textcoords='offset points')\n            \n            ax4.plot(head_reductions, perp_improvements, 'o-', linewidth=2, markersize=8, color='purple')\n            ax4.set_title('Perplexity Improvement vs Head Reduction', fontsize=14)\n            ax4.set_xlabel('Head Reduction (%)', fontsize=12)\n            ax4.set_ylabel('Perplexity Improvement (%)', fontsize=12)\n            ax4.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n\n    # Efficiency plots\n    if baseline_metrics and 'active_heads' in baseline_metrics[0] and 'perplexity' in baseline_metrics[0]:\n        baseline_perp = baseline_metrics[0]['perplexity']\n        baseline_heads = baseline_metrics[0]['active_heads']\n        \n        # Calculate efficiency (perplexity per head)\n        baseline_efficiency = baseline_perp / baseline_heads\n        efficiencies = [p / a for p, a in zip(perplexities, active_heads)]\n        efficiency_improvements = [(baseline_efficiency - e) / baseline_efficiency * 100 for e in efficiencies]\n        \n        # Create efficiency figure\n        plt.figure(figsize=(10, 6))\n        plt.plot(cycles, efficiencies, 'o-', linewidth=2, markersize=8, color='teal')\n        plt.axhline(y=baseline_efficiency, color='red', linestyle='--', alpha=0.7,\n                   label=f'Baseline ({baseline_efficiency:.3f})')\n        plt.title('Model Efficiency (Perplexity per Head)', fontsize=14)\n        plt.xlabel('Cycle', fontsize=12)\n        plt.ylabel('Efficiency (Lower is Better)', fontsize=12)\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    \n    return True\n\n# Create visualizations from metrics\nmetrics_dir = os.path.join(output_dir, \"metrics\")\nmetrics = load_metrics(metrics_dir)\n\nif metrics:\n    print(\"ğŸ“Š Creating visualizations from optimization metrics...\")\n    success = create_visualizations(metrics)\n    if success:\n        print(\"âœ… Visualizations created successfully\")\nelse:\n    print(\"âš ï¸ No metrics available. Visualizations cannot be created.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Compare Model Outputs\n\nLet's compare the output of the original and upgraded models:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Compare original and upgraded models\nfrom transformers import AutoModelForCausalLM\n\ndef compare_models(original_model_name, upgraded_model_path, prompts, max_length=100):\n    \"\"\"Compare text generation between original and upgraded models.\"\"\"\n    # Load models and tokenizers\n    try:\n        print(f\"ğŸ“š Loading original model: {original_model_name}\")\n        original_tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n        if original_tokenizer.pad_token is None:\n            original_tokenizer.pad_token = original_tokenizer.eos_token\n            \n        original_model = AutoModelForCausalLM.from_pretrained(original_model_name)\n        \n        print(f\"âœ… Successfully loaded original model\")\n    except Exception as e:\n        print(f\"âŒ Error loading original model: {e}\")\n        return False\n    \n    try:\n        print(f\"ğŸ“š Loading upgraded model: {upgraded_model_path}\")\n        upgraded_tokenizer = AutoTokenizer.from_pretrained(upgraded_model_path)\n        if upgraded_tokenizer.pad_token is None:\n            upgraded_tokenizer.pad_token = upgraded_tokenizer.eos_token\n            \n        upgraded_model = AutoModelForCausalLM.from_pretrained(upgraded_model_path)\n        \n        print(f\"âœ… Successfully loaded upgraded model\")\n    except Exception as e:\n        print(f\"âŒ Error loading upgraded model: {e}\")\n        print(\"Using original model for comparison to demonstrate the interface\")\n        upgraded_model = original_model\n        upgraded_tokenizer = original_tokenizer\n        \n    # Move to correct device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    original_model.to(device)\n    upgraded_model.to(device)\n    \n    # Settings for generation\n    generation_config = {\n        \"max_length\": max_length,\n        \"do_sample\": True,\n        \"temperature\": 0.7,\n        \"top_p\": 0.9,\n        \"top_k\": 50,\n        \"pad_token_id\": original_tokenizer.pad_token_id\n    }\n    \n    # Compare generations for each prompt\n    for prompt in prompts:\n        print(f\"\\n{'=' * 40}\\nğŸ“ PROMPT: {prompt}\\n{'=' * 40}\")\n        \n        # Generate with original model\n        try:\n            print(\"\\nğŸ” ORIGINAL MODEL OUTPUT:\")\n            \n            inputs = original_tokenizer(prompt, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs = original_model.generate(**inputs, **generation_config)\n            \n            text = original_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            formatted_text = text if len(text) <= 500 else text[:500] + \"...\"\n            print(formatted_text)\n        except Exception as e:\n            print(f\"âŒ Error generating with original model: {e}\")\n        \n        # Generate with upgraded model\n        try:\n            print(\"\\nğŸŒŸ UPGRADED MODEL OUTPUT:\")\n            \n            inputs = upgraded_tokenizer(prompt, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs = upgraded_model.generate(**inputs, **generation_config)\n            \n            text = upgraded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n            formatted_text = text if len(text) <= 500 else text[:500] + \"...\"\n            print(formatted_text)\n        except Exception as e:\n            print(f\"âŒ Error generating with upgraded model: {e}\")\n            \n        print(\"\\n\" + \"-\" * 80)\n    \n    return True\n\n# Define prompts for comparison\nprompts = [\n    \"The future of artificial intelligence is\",\n    \"The most interesting aspect of neural networks is\",\n    \"In five years, language models will\",\n    \"The key to efficient model design is\",\n]\n\n# Run comparison\nhf_model_dir = os.path.join(output_dir, \"hf_model\")\nif os.path.exists(hf_model_dir):\n    print(\"ğŸ”„ Comparing original and upgraded models...\")\n    compare_models(MODEL_NAME, hf_model_dir, prompts)\nelse:\n    print(f\"âš ï¸ Upgraded model directory not found: {hf_model_dir}\")\n    print(\"Comparing original model with itself for demonstration\")\n    compare_models(MODEL_NAME, MODEL_NAME, prompts)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Generate Performance Summary\n\nLet's generate a summary of the model's performance improvements:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Generate performance summary\nfrom IPython.display import display, HTML\n\ndef generate_performance_summary(metrics_dir):\n    \"\"\"Generate a summary of model performance improvements.\"\"\"\n    metrics_file = os.path.join(metrics_dir, \"integration_metrics.jsonl\")\n    \n    if not os.path.exists(metrics_file):\n        print(f\"âŒ Metrics file not found: {metrics_file}\")\n        \n        # Create demonstration metrics\n        baseline_perplexity = 25.7\n        final_perplexity = 18.2\n        baseline_heads = 72\n        final_heads = 48\n        \n        print(\"âš ï¸ Using demonstration metrics for summary\")\n    else:\n        # Load metrics\n        metrics = []\n        with open(metrics_file, 'r') as f:\n            for line in f:\n                metrics.append(json.loads(line))\n        \n        # Extract key metrics\n        baseline_metrics = [m for m in metrics if m.get('phase') == 'baseline']\n        cycle_metrics = [m for m in metrics if m.get('phase') == 'cycle_complete']\n        \n        if not baseline_metrics or not cycle_metrics:\n            print(\"âŒ Insufficient metrics for summary\")\n            return\n        \n        baseline_perplexity = baseline_metrics[0].get('perplexity', 0)\n        final_perplexity = cycle_metrics[-1].get('final_perplexity', 0)\n        baseline_heads = baseline_metrics[0].get('active_heads', 0)\n        final_heads = cycle_metrics[-1].get('active_heads', 0)\n    \n    # Calculate improvements\n    perplexity_improvement = ((baseline_perplexity - final_perplexity) / baseline_perplexity) * 100 if baseline_perplexity > 0 else 0\n    head_reduction = ((baseline_heads - final_heads) / baseline_heads) * 100 if baseline_heads > 0 else 0\n    \n    # Calculate efficiency metrics\n    baseline_efficiency = baseline_perplexity / baseline_heads if baseline_heads > 0 else 0\n    final_efficiency = final_perplexity / final_heads if final_heads > 0 else 0\n    efficiency_improvement = ((baseline_efficiency - final_efficiency) / baseline_efficiency) * 100 if baseline_efficiency > 0 else 0\n    \n    # Create HTML table\n    html = \"\"\"\n    <style>\n        .performance-table {\n            width: 100%;\n            border-collapse: collapse;\n            margin: 20px 0;\n            font-family: Arial, sans-serif;\n        }\n        .performance-table th {\n            background-color: #4CAF50;\n            color: white;\n            padding: 12px;\n            text-align: left;\n            font-weight: bold;\n        }\n        .performance-table td {\n            padding: 12px;\n            text-align: left;\n            border-bottom: 1px solid #ddd;\n        }\n        .performance-table tr:nth-child(even) {\n            background-color: #f9f9f9;\n        }\n        .positive {\n            color: green;\n            font-weight: bold;\n        }\n        .negative {\n            color: red;\n            font-weight: bold;\n        }\n        .title {\n            font-size: 24px;\n            font-weight: bold;\n            margin: 20px 0;\n            color: #333;\n        }\n        .subtitle {\n            font-size: 18px;\n            color: #666;\n            margin-bottom: 20px;\n        }\n    </style>\n    \n    <div class=\"title\">Upgrayedd Performance Summary</div>\n    <div class=\"subtitle\">Model: {}</div>\n    \n    <table class=\"performance-table\">\n        <tr>\n            <th>Metric</th>\n            <th>Before</th>\n            <th>After</th>\n            <th>Change</th>\n        </tr>\n        <tr>\n            <td>Perplexity</td>\n            <td>{:.2f}</td>\n            <td>{:.2f}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n        <tr>\n            <td>Active Heads</td>\n            <td>{}</td>\n            <td>{}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n        <tr>\n            <td>Efficiency (Perplexity/Head)</td>\n            <td>{:.3f}</td>\n            <td>{:.3f}</td>\n            <td class=\"{}\">{}%</td>\n        </tr>\n    </table>\n    \n    <div style=\"margin-top: 30px; font-weight: bold;\">Key Findings:</div>\n    <ul>\n        <li>Performance {}improved by {:.1f}% while reducing model complexity</li>\n        <li>Model size reduced by {:.1f}% through strategic head pruning</li>\n        <li>Overall model efficiency {}improved by {:.1f}%</li>\n    </ul>\n    \"\"\".format(\n        MODEL_NAME,\n        baseline_perplexity, final_perplexity, \n        \"positive\" if perplexity_improvement > 0 else \"negative\",\n        f\"-{perplexity_improvement:.1f}\" if perplexity_improvement > 0 else f\"+{-perplexity_improvement:.1f}\",\n        baseline_heads, final_heads,\n        \"positive\" if head_reduction > 0 else \"negative\",\n        f\"-{head_reduction:.1f}\" if head_reduction > 0 else f\"+{-head_reduction:.1f}\",\n        baseline_efficiency, final_efficiency,\n        \"positive\" if efficiency_improvement > 0 else \"negative\",\n        f\"-{efficiency_improvement:.1f}\" if efficiency_improvement > 0 else f\"+{-efficiency_improvement:.1f}\",\n        \"\" if perplexity_improvement > 0 else \"did not \",\n        abs(perplexity_improvement),\n        head_reduction,\n        \"\" if efficiency_improvement > 0 else \"did not \",\n        abs(efficiency_improvement)\n    )\n    \n    display(HTML(html))\n    \n    # Also print text version for non-HTML environments\n    print(\"\\nPERFORMANCE SUMMARY:\")\n    print(\"=\" * 50)\n    print(f\"Model: {MODEL_NAME}\")\n    print(f\"Perplexity: {baseline_perplexity:.2f} â†’ {final_perplexity:.2f} ({perplexity_improvement:.1f}% improvement)\")\n    print(f\"Active Heads: {baseline_heads} â†’ {final_heads} ({head_reduction:.1f}% reduction)\")\n    print(f\"Efficiency: {baseline_efficiency:.3f} â†’ {final_efficiency:.3f} ({efficiency_improvement:.1f}% improvement)\")\n    print(\"=\" * 50)\n\n# Generate summary\nmetrics_dir = os.path.join(output_dir, \"metrics\")\ngenerate_performance_summary(metrics_dir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\nAfter completing the model transformation, you might want to:\n\n1. **Try Different Models**: Experiment with larger models like OPT-1.3B, Pythia-1.4B, or BLOOM-1B7.\n\n2. **Adjust Configurations**: Modify pruning levels, growth ratios, and learning rates to optimize for your specific use case.\n\n3. **Use Custom Datasets**: Replace the sample datasets with your own domain-specific data.\n\n4. **Export the Model**: Use the upgraded model for inference in your applications:\n   ```python\n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   \n   # Replace with your actual output path\n   model_path = \"./output/upgrayedd_distilgpt2_20250405_120000/hf_model\"\n   \n   # Load the model and tokenizer\n   model = AutoModelForCausalLM.from_pretrained(model_path)\n   tokenizer = AutoTokenizer.from_pretrained(model_path)\n   \n   # Generate text\n   inputs = tokenizer(\"The future of AI is\", return_tensors=\"pt\")\n   outputs = model.generate(**inputs, max_length=100)\n   print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n   ```\n\n5. **Benchmark**: Compare the upgraded model with the original model in terms of:\n   - Inference speed\n   - Memory usage\n   - Perplexity on different datasets\n   - Downstream task performance\n\n6. **Further Compress**: If needed, use additional compression techniques like quantization to further reduce the model size.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "## How Controller-Plasticity Integration Works\n\nThe core of Upgrayedd is the Controller-Plasticity Integration system, which creates a feedback loop for continuous optimization:\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                                        â”‚\nâ”‚                          OPTIMIZATION CYCLE                            â”‚\nâ”‚                                                                        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚                    â”‚                      â”‚                     â”‚   â”‚\nâ”‚  â”‚  CONTROLLER SYSTEM â”‚                      â”‚ PLASTICITY SYSTEM   â”‚   â”‚\nâ”‚  â”‚                    â”‚                      â”‚                     â”‚   â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚  â”‚  â”‚ Analyze head â”‚  â”‚                      â”‚ â”‚ Prune heads   â”‚   â”‚   â”‚\nâ”‚  â”‚  â”‚ metrics      â”‚  â”‚                      â”‚ â”‚ based on      â”‚   â”‚   â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚ â”‚ controller    â”‚   â”‚   â”‚\nâ”‚  â”‚         â”‚          â”‚                      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚  â”‚         â–¼          â”‚                      â”‚         â”‚           â”‚   â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚         â–¼           â”‚   â”‚\nâ”‚  â”‚  â”‚ Generate     â”‚  â”‚     Gate values      â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚  â”‚  â”‚ gate values  â”‚â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â”‚ Measure       â”‚   â”‚   â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚ â”‚ impact        â”‚   â”‚   â”‚\nâ”‚  â”‚         â”‚          â”‚                      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚  â”‚         â–¼          â”‚                      â”‚         â”‚           â”‚   â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚         â–¼           â”‚   â”‚\nâ”‚  â”‚  â”‚ Update       â”‚  â”‚      Metrics         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚  â”‚  â”‚ controller   â”‚â—„â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚ Grow heads    â”‚   â”‚   â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                     â”‚â”‚ â”‚ strategically â”‚   â”‚   â”‚\nâ”‚  â”‚                    â”‚                     â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚â”‚         â”‚           â”‚   â”‚\nâ”‚                                             â”‚â”‚         â–¼           â”‚   â”‚\nâ”‚                                             â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚                                             â”‚â”‚ â”‚ Apply         â”‚   â”‚   â”‚\nâ”‚                                             â””â”¤ â”‚ differential  â”‚   â”‚   â”‚\nâ”‚                                              â”‚ â”‚ learning      â”‚   â”‚   â”‚\nâ”‚                                              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚                                              â”‚                     â”‚   â”‚\nâ”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\nThis integration creates a virtuous cycle:\n\n1. **Controller Analysis**: The neural controller analyzes head importance metrics\n2. **Dynamic Gating**: Generates gate values to indicate which heads should be kept or pruned\n3. **Guided Pruning**: The plasticity system prunes heads according to controller guidance\n4. **Impact Measurement**: System measures the effect of pruning on model performance\n5. **Strategic Regrowth**: Heads are regrown in areas that need them most\n6. **Differential Learning**: Fine-tuning with specialized learning rates for different heads\n7. **Controller Update**: The controller learns from the results, improving over time\n\nThrough repeated cycles, this system creates a self-optimizing neural network that continuously adapts its structure for maximum efficiency."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}