{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nPruning and Fine-Tuning Colab (v0.0.34)\n\nThis script demonstrates making a GPT-2 model smaller and more powerful by:\n1. Applying pruning to remove less important attention heads\n2. Fine-tuning the pruned model to recover performance\n3. Showing clear metrics of improvement\n\nIt's designed to be run in Google Colab using real-world data (not tiny Shakespeare).\n\nVersion History:\n- v0.0.34 (April 2025): Fixed missing variable error, visualization issues and enhanced CUDA error handling\n- v0.0.33 (April 2025): Fixed visualization issues, improved model compatibility and enhanced error handling\n- v0.0.32 (April 2025): Added CUDA error handling for Colab compatibility and memory management\n- v0.0.31 (April 2025): Fixed get_strategy parameters issue and improved Colab compatibility \n- v0.0.30 (April 2025): Added OPT model support and chart improvements\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport json\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, \n    get_linear_schedule_with_warmup, \n    GPT2LMHeadModel\n)\n\n# Initialize plotting style\nplt.style.use('seaborn-v0_8-pastel')\n\n# Configure device and optimize for Colab environment\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Half-precision for GPU to reduce memory usage\nUSE_FP16 = DEVICE == \"cuda\"\n\n# Handle TPU if available (Colab-specific optimization)\nif 'COLAB_TPU_ADDR' in os.environ:\n    try:\n        import torch_xla.core.xla_model as xm\n        DEVICE = xm.xla_device()\n        print(f\"TPU detected and configured!\")\n        USE_FP16 = False  # TPUs have their own optimization\n    except ImportError:\n        print(\"TPU environment detected but torch_xla not installed.\")\n\n# Global variables\nOUTPUT_DIR = \"pruning_results\"\nMODEL_CACHE_DIR = \"model_cache\"\nDATA_DIR = \"data\"\n\nclass ProgressMetrics:\n    \"\"\"Track metrics throughout the pruning and fine-tuning process.\"\"\"\n    \n    def __init__(self):\n        self.metrics = {\n            \"loss\": [],\n            \"perplexity\": [],\n            \"steps\": [],\n            \"pruning_level\": None,\n            \"strategy\": None,\n            \"pruned_heads\": [],\n            \"gate_values\": [],\n            \"head_importance\": [],\n            \"generation_samples\": []\n        }\n        \n        # Create visualizations\n        self.fig, self.axes = plt.subplots(1, 2, figsize=(15, 5))\n        self.loss_line = None\n        self.ppl_line = None\n        \n    def update(self, step, loss, perplexity, head_info=None, gate_values=None, \n               generation_sample=None):\n        \"\"\"Update metrics with new values.\"\"\"\n        self.metrics[\"steps\"].append(step)\n        self.metrics[\"loss\"].append(loss)\n        self.metrics[\"perplexity\"].append(perplexity)\n        \n        if head_info is not None:\n            self.metrics[\"head_importance\"] = head_info\n            \n        if gate_values is not None:\n            self.metrics[\"gate_values\"] = gate_values\n            \n        if generation_sample is not None:\n            self.metrics[\"generation_samples\"].append({\n                \"step\": step,\n                \"text\": generation_sample\n            })\n        \n        # Update visualization\n        self._update_plots()\n        \n    def set_pruning_info(self, strategy, level, pruned_heads):\n        \"\"\"Set pruning information.\"\"\"\n        self.metrics[\"strategy\"] = strategy\n        self.metrics[\"pruning_level\"] = level\n        self.metrics[\"pruned_heads\"] = pruned_heads\n        \n    def _update_plots(self):\n        \"\"\"Update visualization plots.\"\"\"\n        steps = self.metrics[\"steps\"]\n        loss = self.metrics[\"loss\"]\n        ppl = self.metrics[\"perplexity\"]\n        \n        if not steps:\n            return\n            \n        # Clear previous plots\n        self.axes[0].clear()\n        self.axes[1].clear()\n        \n        # Plot loss\n        self.axes[0].plot(steps, loss, 'b-')\n        self.axes[0].set_title('Training Loss')\n        self.axes[0].set_xlabel('Step')\n        self.axes[0].set_ylabel('Loss')\n        self.axes[0].grid(True)\n        \n        # Plot perplexity\n        self.axes[1].plot(steps, ppl, 'r-')\n        self.axes[1].set_title('Perplexity (lower is better)')\n        self.axes[1].set_xlabel('Step')\n        self.axes[1].set_ylabel('Perplexity')\n        self.axes[1].grid(True)\n        \n        self.fig.tight_layout()\n        plt.draw()\n        plt.pause(0.001)\n        \n    def save_plots(self, path):\n        \"\"\"Save plots to file.\"\"\"\n        plt.savefig(path)\n        \n    def save_metrics(self, path):\n        \"\"\"Save metrics to file.\"\"\"\n        with open(path, 'w') as f:\n            json.dump(self.metrics, f, indent=2)\n            \n    def get_summary(self):\n        \"\"\"Return a summary of key metrics.\"\"\"\n        if not self.metrics[\"perplexity\"] or len(self.metrics[\"perplexity\"]) <= 1:\n            return {\"error\": \"Not enough data points for summary\"}\n            \n        return {\n            \"strategy\": self.metrics[\"strategy\"],\n            \"pruning_level\": self.metrics[\"pruning_level\"],\n            \"pruned_heads_count\": len(self.metrics[\"pruned_heads\"]),\n            \"initial_loss\": self.metrics[\"loss\"][0],\n            \"final_loss\": self.metrics[\"loss\"][-1],\n            \"initial_perplexity\": self.metrics[\"perplexity\"][0],\n            \"final_perplexity\": self.metrics[\"perplexity\"][-1],\n            \"improvement_percent\": ((self.metrics[\"perplexity\"][0] - self.metrics[\"perplexity\"][-1]) / \n                                   self.metrics[\"perplexity\"][0] * 100)\n        }\n\ndef setup_directories():\n    \"\"\"Create necessary directories for outputs and data.\"\"\"\n    os.makedirs(OUTPUT_DIR, exist_ok=True)\n    os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n    os.makedirs(DATA_DIR, exist_ok=True)\n    \n    return OUTPUT_DIR, MODEL_CACHE_DIR, DATA_DIR\n\ndef download_wikitext():\n    \"\"\"Download Wikitext dataset if not already present.\"\"\"\n    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n    \n    if not os.path.exists(wikitext_file):\n        print(\"Downloading Wikitext-2 dataset...\")\n        try:\n            # Using HF datasets library\n            from datasets import load_dataset\n            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n            \n            # Save validation text\n            with open(wikitext_file, \"w\", encoding=\"utf-8\") as f:\n                for item in tqdm(dataset[\"validation\"], desc=\"Saving dataset\"):\n                    if item[\"text\"].strip():\n                        f.write(item[\"text\"] + \"\\n\")\n                        \n            print(f\"Dataset saved to {wikitext_file}\")\n        except Exception as e:\n            print(f\"Error downloading dataset: {e}\")\n            \n            # Fallback: download using requests\n            try:\n                import requests\n                url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\"\n                r = requests.get(url)\n                \n                # Save zip file\n                zip_path = os.path.join(DATA_DIR, \"wikitext-2-raw-v1.zip\")\n                with open(zip_path, \"wb\") as f:\n                    f.write(r.content)\n                \n                # Extract\n                import zipfile\n                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n                    zip_ref.extractall(DATA_DIR)\n                \n                print(f\"Dataset downloaded and extracted to {DATA_DIR}\")\n            except Exception as e2:\n                print(f\"Fallback download also failed: {e2}\")\n                return False\n    \n    return True\n\ndef load_wikitext_data(tokenizer, max_length=512, batch_size=4):\n    \"\"\"Load and prepare Wikitext data for fine-tuning and evaluation.\"\"\"\n    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n    \n    if not os.path.exists(wikitext_file):\n        success = download_wikitext()\n        if not success:\n            print(\"Failed to download dataset\")\n            return None, None\n    \n    # Read the data\n    print(\"Loading Wikitext-2 data...\")\n    with open(wikitext_file, \"r\", encoding=\"utf-8\") as f:\n        text = f.read()\n    \n    # Split into train and validation (80/20)\n    paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n    \n    # Ensure we have at least 100 paragraphs of reasonable length\n    paragraphs = [p for p in paragraphs if len(p) > 100]\n    \n    if len(paragraphs) < 100:\n        # Fall back to splitting by newline if needed\n        paragraphs = [p for p in text.split(\"\\n\") if len(p.strip()) > 100]\n    \n    # Shuffle and split\n    np.random.seed(42)\n    np.random.shuffle(paragraphs)\n    \n    split_idx = int(len(paragraphs) * 0.8)\n    train_paragraphs = paragraphs[:split_idx]\n    val_paragraphs = paragraphs[split_idx:]\n    \n    print(f\"Tokenizing {len(train_paragraphs)} training and {len(val_paragraphs)} validation paragraphs...\")\n    \n    # Tokenize and prepare datasets\n    train_data = prepare_dataset(train_paragraphs, tokenizer, max_length, batch_size)\n    val_data = prepare_dataset(val_paragraphs, tokenizer, max_length, batch_size)\n    \n    return train_data, val_data\n\ndef prepare_dataset(paragraphs, tokenizer, max_length, batch_size):\n    \"\"\"Tokenize and prepare paragraphs into a PyTorch dataset.\"\"\"\n    # Tokenize\n    tokenized = tokenizer(\n        paragraphs,\n        max_length=max_length,\n        truncation=True,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    input_ids = tokenized[\"input_ids\"]\n    attention_mask = tokenized[\"attention_mask\"]\n    \n    # Create dataset\n    dataset = TensorDataset(input_ids, attention_mask)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    return dataloader\n\ndef load_model_and_tokenizer(model_name, cache_dir=None):\n    \"\"\"Load pre-trained model and tokenizer.\"\"\"\n    print(f\"Loading model: {model_name}\")\n    \n    # Determine model type from name\n    if \"gpt2\" in model_name.lower():\n        model_type = \"gpt2\"\n    elif \"opt\" in model_name.lower() or \"facebook\" in model_name.lower():\n        model_type = \"opt\"\n    elif \"pythia\" in model_name.lower() or \"eleutherai\" in model_name.lower():\n        model_type = \"pythia\"\n    else:\n        model_type = \"gpt2\"  # Default to gpt2\n        \n    print(f\"Detected model type: {model_type}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n    \n    # Ensure padding token is set\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load model with potential FP16 optimization\n    try:\n        if USE_FP16:\n            print(\"Using FP16 for model loading\")\n            # For FP16, we need to set torch_dtype\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name, \n                cache_dir=cache_dir,\n                torch_dtype=torch.float16\n            )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n    except Exception as e:\n        print(f\"Error loading model with AutoModelForCausalLM: {e}\")\n        print(\"Falling back to GPT2LMHeadModel\")\n        model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)\n    \n    model.to(DEVICE)\n    \n    # Store model type for later use\n    model.model_type = model_type\n    \n    # Print model size information\n    param_count = sum(p.numel() for p in model.parameters())\n    print(f\"Model loaded with {param_count/1e6:.2f}M parameters\")\n    \n    # Add head_count attribute if we can determine it\n    try:\n        # Count number of attention heads\n        if hasattr(model.config, \"n_head\"):\n            model.head_count = model.config.n_head\n        elif hasattr(model.config, \"num_attention_heads\"):\n            model.head_count = model.config.num_attention_heads\n        elif hasattr(model.config, \"num_heads\"):\n            model.head_count = model.config.num_heads\n        else:\n            model.head_count = 12  # Reasonable default\n        print(f\"Model has {model.head_count} attention heads per layer\")\n    except Exception as e:\n        print(f\"Could not determine head count: {e}\")\n        model.head_count = 12  # Reasonable default\n    \n    return model, tokenizer\n\ndef get_attention_modules(model):\n    \"\"\"Extract attention modules from model.\"\"\"\n    # Set default model type if not already set\n    if not hasattr(model, \"model_type\"):\n        model.model_type = \"gpt2\"\n    \n    attention_modules = []\n    \n    # Function to safely get attribute path\n    def get_nested_attr(obj, attr_path):\n        \"\"\"Safely get attribute path without raising AttributeError.\"\"\"\n        attrs = attr_path.split(\".\")\n        current = obj\n        for attr in attrs:\n            if hasattr(current, attr):\n                current = getattr(current, attr)\n            else:\n                return None\n        return current\n    \n    # Try different model architectures\n    if model.model_type == \"gpt2\":\n        # GPT-2 style models\n        transformer = get_nested_attr(model, \"transformer\")\n        if transformer:\n            blocks = get_nested_attr(transformer, \"h\")\n            if blocks:\n                for i, block in enumerate(blocks):\n                    attn = get_nested_attr(block, \"attn\")\n                    if attn:\n                        attention_modules.append((i, attn))\n    \n    elif model.model_type == \"opt\":\n        # OPT models\n        model_module = get_nested_attr(model, \"model\")\n        if model_module:\n            decoder = get_nested_attr(model_module, \"decoder\")\n            if decoder:\n                layers = get_nested_attr(decoder, \"layers\")\n                if layers:\n                    for i, layer in enumerate(layers):\n                        self_attn = get_nested_attr(layer, \"self_attn\")\n                        if self_attn:\n                            attention_modules.append((i, self_attn))\n    \n    elif model.model_type == \"pythia\":\n        # Pythia models (similar to GPT-2)\n        transformer = get_nested_attr(model, \"transformer\") or get_nested_attr(model, \"gpt_neox\")\n        if transformer:\n            blocks = get_nested_attr(transformer, \"h\") or get_nested_attr(transformer, \"layers\")\n            if blocks:\n                for i, block in enumerate(blocks):\n                    attn = get_nested_attr(block, \"attn\") or get_nested_attr(block, \"attention\")\n                    if attn:\n                        attention_modules.append((i, attn))\n    \n    # Generic fallback if nothing matched\n    if not attention_modules:\n        # Try common patterns across different architectures\n        candidate_paths = [\n            \"transformer.h\",              # GPT-2 style\n            \"model.decoder.layers\",       # OPT style\n            \"encoder.layers\",             # Encoder style models\n            \"decoder.layers\",             # Decoder style models\n            \"layers\",                     # Direct layers attribute\n            \"transformer.layers\",         # Some transformers\n            \"gpt_neox.layers\"             # Pythia/GPT-NeoX\n        ]\n        \n        for path in candidate_paths:\n            try:\n                blocks = get_nested_attr(model, path)\n                if blocks and isinstance(blocks, (list, tuple)) or hasattr(blocks, \"__getitem__\"):\n                    for i, block in enumerate(blocks):\n                        # Try common attention module names\n                        for attn_name in [\"attn\", \"attention\", \"self_attn\", \"self_attention\", \"mha\"]:\n                            attn = get_nested_attr(block, attn_name)\n                            if attn:\n                                attention_modules.append((i, attn))\n                                break\n                    \n                    if attention_modules:\n                        # Found some attention modules, can stop looking\n                        break\n            except Exception as e:\n                continue\n    \n    if attention_modules:\n        print(f\"Found {len(attention_modules)} attention modules\")\n        \n        # Try to add head_size attribute if not present\n        for _, attn in attention_modules:\n            if not hasattr(attn, \"head_size\") and hasattr(model, \"head_count\"):\n                # Try to determine head size from attention module\n                if hasattr(attn, \"head_dim\"):\n                    attn.head_size = attn.head_dim\n                elif hasattr(model.config, \"hidden_size\"):\n                    attn.head_size = model.config.hidden_size // model.head_count\n                elif hasattr(attn, \"q_proj\") and hasattr(attn.q_proj, \"weight\"):\n                    # Common in models like OPT\n                    attn.head_size = attn.q_proj.weight.shape[0] // model.head_count\n                elif hasattr(attn, \"c_attn\") and hasattr(attn.c_attn, \"weight\"):\n                    # Common in GPT-2 models\n                    q_weight = attn.c_attn.weight\n                    attn.head_size = q_weight.shape[1] // (3 * model.head_count)\n            \n            # Add num_heads attribute if not present\n            if not hasattr(attn, \"num_heads\") and hasattr(model, \"head_count\"):\n                attn.num_heads = model.head_count\n    else:\n        print(\"Warning: Could not find attention modules. Unsupported model architecture.\")\n    \n    return attention_modules\n\ndef get_head_importances(model, val_dataloader, strategy=\"entropy\"):\n    \"\"\"\n    Calculate importance scores for each attention head.\n    \n    Args:\n        model: The model to analyze\n        val_dataloader: Validation data for computing metrics\n        strategy: Pruning strategy ('entropy', 'magnitude', 'random')\n        \n    Returns:\n        List of (layer_idx, head_idx, importance) tuples\n    \"\"\"\n    print(f\"Calculating head importances using {strategy} strategy...\")\n    attention_modules = get_attention_modules(model)\n    head_importances = []\n    \n    # Set default model type if not already set\n    if not hasattr(model, \"model_type\"):\n        model.model_type = \"gpt2\"\n    \n    if strategy == \"random\":\n        # For random strategy, just assign random importances\n        for layer_idx, attn in attention_modules:\n            # Get number of heads based on model type\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Try to infer from model name\n                if model.model_type == \"gpt2\":\n                    num_heads = 12  # Default for GPT-2\n                elif model.model_type == \"opt\":\n                    num_heads = 12  # Default for smaller OPT\n                elif model.model_type == \"pythia\":\n                    num_heads = 12  # Default for smaller Pythia\n                else:\n                    num_heads = 12  # fallback\n                print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n                    \n            for head_idx in range(num_heads):\n                importance = np.random.random()\n                head_importances.append((layer_idx, head_idx, importance))\n    \n    elif strategy == \"magnitude\":\n        # For magnitude strategy, use the L2 norm of the head weights\n        for layer_idx, attn in attention_modules:\n            # Determine number of heads\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Model-specific defaults\n                if model.model_type == \"gpt2\":\n                    num_heads = 12\n                elif model.model_type == \"opt\":\n                    num_heads = 12\n                elif model.model_type == \"pythia\":\n                    num_heads = 12\n                else:\n                    num_heads = 12\n                print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n            \n            # Get the appropriate projection weights based on model type\n            if model.model_type == \"gpt2\":\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            elif model.model_type == \"opt\":\n                if hasattr(attn, \"out_proj\") and hasattr(attn, \"out_proj\"):\n                    q_weight = attn.q_proj.weight\n                    head_size = q_weight.shape[0] // num_heads\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            elif model.model_type == \"pythia\":\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size  \n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            else:\n                # Default to GPT-2 pattern\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n                \n            # Compute importance for each head\n            for head_idx in range(num_heads):\n                try:\n                    # Get weights for this head\n                    start_idx = head_idx * head_size\n                    end_idx = (head_idx + 1) * head_size\n                    \n                    # Extract weights for Q, K, V for this head - GPT2-specific\n                    if model.model_type == \"gpt2\" or model.model_type == \"pythia\":\n                        q_head = q_weight[:, start_idx:end_idx]\n                        k_head = q_weight[:, num_heads*head_size + start_idx:num_heads*head_size + end_idx]\n                        v_head = q_weight[:, 2*num_heads*head_size + start_idx:2*num_heads*head_size + end_idx]\n                    elif model.model_type == \"opt\":\n                        # For OPT, we need to get separate Q, K, V projections\n                        q_head = attn.q_proj.weight[start_idx:end_idx, :]\n                        k_head = attn.k_proj.weight[start_idx:end_idx, :]\n                        v_head = attn.v_proj.weight[start_idx:end_idx, :]\n                    else:\n                        # Fallback to GPT2 pattern\n                        q_head = q_weight[:, start_idx:end_idx]\n                        k_head = q_weight[:, num_heads*head_size + start_idx:num_heads*head_size + end_idx]\n                        v_head = q_weight[:, 2*num_heads*head_size + start_idx:2*num_heads*head_size + end_idx]\n                    \n                    # Compute L2 norm (magnitude)\n                    q_norm = torch.norm(q_head).item()\n                    k_norm = torch.norm(k_head).item()\n                    v_norm = torch.norm(v_head).item()\n                    \n                    # Use average of Q, K, V norms as importance\n                    importance = (q_norm + k_norm + v_norm) / 3\n                    head_importances.append((layer_idx, head_idx, importance))\n                except Exception as e:\n                    print(f\"Error processing head {head_idx} in layer {layer_idx}: {e}\")\n                    # Assign random importance as fallback\n                    importance = np.random.random()\n                    head_importances.append((layer_idx, head_idx, importance))\n    \n    elif strategy == \"entropy\":\n        # For entropy strategy, measure attention entropy on validation data\n        model.eval()\n        \n        # Store attention outputs\n        attention_outputs = {}\n        \n        # Register hooks to capture attention\n        handles = []\n        \n        def get_attention_hook(layer_idx):\n            def hook(module, input, output):\n                # Shape is usually [batch, num_heads, seq_len, seq_len]\n                # But format can differ by model type\n                if isinstance(output, tuple) and len(output) > 1 and isinstance(output[1], torch.Tensor):\n                    attention_outputs[layer_idx] = output[1].detach()\n                elif isinstance(output, torch.Tensor):\n                    # Some models directly return attention weights\n                    attention_outputs[layer_idx] = output.detach()\n            return hook\n        \n        # Register hooks for each attention module\n        for layer_idx, attn in attention_modules:\n            handles.append(attn.register_forward_hook(get_attention_hook(layer_idx)))\n        \n        # Run a few batches to collect attention patterns\n        with torch.no_grad():\n            for batch_idx, (input_ids, attention_mask) in enumerate(val_dataloader):\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                \n                # Forward pass to trigger hooks\n                try:\n                    model(input_ids=input_ids, attention_mask=attention_mask)\n                except Exception as e:\n                    print(f\"Error during forward pass: {e}\")\n                    continue\n                \n                if batch_idx >= 5:  # Collect data from 5 batches\n                    break\n        \n        # Remove hooks\n        for handle in handles:\n            handle.remove()\n        \n        if not attention_outputs:\n            print(\"Warning: No attention outputs captured. Falling back to magnitude strategy.\")\n            return get_head_importances(model, val_dataloader, strategy=\"magnitude\")\n        \n        # Calculate entropy for each head\n        for layer_idx, attn in attention_modules:\n            if layer_idx not in attention_outputs:\n                continue\n                \n            attn_outputs = attention_outputs[layer_idx]\n            \n            # Determine number of heads\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Try to infer from the output shape\n                if len(attn_outputs.shape) >= 2:\n                    num_heads = attn_outputs.shape[1]\n                else:\n                    # Model-specific defaults\n                    if model.model_type == \"gpt2\":\n                        num_heads = 12\n                    elif model.model_type == \"opt\":\n                        num_heads = 12\n                    elif model.model_type == \"pythia\":\n                        num_heads = 12\n                    else:\n                        num_heads = 12\n                    print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n                \n            for head_idx in range(num_heads):\n                try:\n                    # Extract attention weights for this head\n                    if head_idx < attn_outputs.shape[1]:  # Check if index is valid\n                        head_attn = attn_outputs[:, head_idx, :, :]\n                    else:\n                        print(f\"Warning: Head index {head_idx} out of bounds. Skipping.\")\n                        continue\n                    \n                    # Calculate entropy (we want low entropy = focused attention)\n                    entropy = 0\n                    \n                    # Process each item in the batch\n                    for item_idx in range(head_attn.size(0)):\n                        item_attn = head_attn[item_idx]\n                        \n                        # Calculate entropy along the attention dimension\n                        # Add small epsilon to avoid log(0)\n                        eps = 1e-10\n                        item_entropy = -torch.sum(item_attn * torch.log(item_attn + eps), dim=-1)\n                        entropy += torch.mean(item_entropy).item()\n                    \n                    # Average entropy across batch\n                    entropy /= head_attn.size(0)\n                    \n                    # Negated entropy, so that higher values = more important (focused attention)\n                    importance = -entropy\n                    head_importances.append((layer_idx, head_idx, importance))\n                    \n                except Exception as e:\n                    print(f\"Error calculating entropy for head {head_idx} in layer {layer_idx}: {e}\")\n                    # Fall back to random importance\n                    importance = np.random.random()\n                    head_importances.append((layer_idx, head_idx, importance))\n    \n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n    \n    # If no head importances were calculated, fall back to random\n    if not head_importances:\n        print(\"Warning: No head importances calculated. Falling back to random strategy.\")\n        return get_head_importances(model, val_dataloader, strategy=\"random\")\n    \n    # Sort by importance (ascending order, so lowest importance first)\n    head_importances.sort(key=lambda x: x[2])\n    \n    return head_importances\n\ndef prune_heads(model, head_importances, pruning_level=0.3):\n    \"\"\"\n    Prune specified fraction of attention heads.\n    \n    Args:\n        model: The model to prune\n        head_importances: List of (layer_idx, head_idx, importance) tuples\n        pruning_level: Fraction of heads to prune (0.0 to 1.0)\n        \n    Returns:\n        List of pruned heads as (layer_idx, head_idx) tuples\n    \"\"\"\n    attention_modules = get_attention_modules(model)\n    \n    # Count total heads\n    total_heads = sum(attn.num_heads for _, attn in attention_modules)\n    \n    # Calculate how many heads to prune\n    num_to_prune = int(total_heads * pruning_level)\n    \n    # Get heads to prune (lowest importance first)\n    heads_to_prune = [(layer_idx, head_idx) for layer_idx, head_idx, _ in head_importances[:num_to_prune]]\n    \n    print(f\"Pruning {len(heads_to_prune)}/{total_heads} attention heads ({pruning_level:.1%})\")\n    \n    # Create/initialize gates if they don't exist\n    for layer_idx, attn in attention_modules:\n        num_heads = attn.num_heads\n        \n        # Check if gate exists\n        if not hasattr(attn, \"head_gates\"):\n            # Create gates with default value 1.0\n            attn.head_gates = torch.ones(num_heads, device=DEVICE)\n    \n    # Apply pruning by setting gates to 0\n    for layer_idx, head_idx in heads_to_prune:\n        for i, (module_layer_idx, attn) in enumerate(attention_modules):\n            if module_layer_idx == layer_idx:\n                attn.head_gates[head_idx] = 0.0\n                break\n    \n    # Modify forward pass to use gates\n    for layer_idx, attn in attention_modules:\n        # Save original method if not already saved\n        if not hasattr(attn, \"original_forward\"):\n            attn.original_forward = attn.forward\n            \n            # Create gated forward method\n            def make_gated_forward(original_forward, head_gates):\n                def gated_forward(self, *args, **kwargs):\n                    # Call original forward\n                    outputs = original_forward(*args, **kwargs)\n                    \n                    # Apply gates to attention outputs\n                    # outputs[0] = result, outputs[1] = attention weights\n                    if len(outputs) > 1 and isinstance(outputs[1], torch.Tensor):\n                        # outputs[1] shape: [batch_size, num_heads, seq_len, seq_len]\n                        gates = head_gates.view(1, -1, 1, 1)\n                        gated_attention = outputs[1] * gates\n                        \n                        return (outputs[0], gated_attention) + outputs[2:] if len(outputs) > 2 else (outputs[0], gated_attention)\n                    \n                    return outputs\n                \n                return gated_forward\n            \n            # Set new forward method\n            attn.forward = make_gated_forward(attn.original_forward, attn.head_gates).__get__(attn, type(attn))\n    \n    return heads_to_prune\n\ndef fine_tune_model(model, train_dataloader, val_dataloader, tokenizer, \n                   learning_rate=5e-5, num_epochs=3, progress_tracker=None):\n    \"\"\"\n    Fine-tune model after pruning.\n    \n    Args:\n        model: The model to fine-tune\n        train_dataloader: Training data\n        val_dataloader: Validation data\n        tokenizer: Tokenizer\n        learning_rate: Learning rate for optimization\n        num_epochs: Number of training epochs\n        progress_tracker: ProgressMetrics object for tracking progress\n        \n    Returns:\n        Dictionary with training results\n    \"\"\"\n    model.train()\n    \n    # Prepare optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    \n    # Calculate total steps and prepare scheduler\n    total_steps = len(train_dataloader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps\n    )\n    \n    # Import memory monitoring if available\n    try:\n        import psutil\n        has_psutil = True\n    except ImportError:\n        has_psutil = False\n    \n    # Train the model\n    step = 0\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training loop\n        model.train()\n        epoch_losses = []\n        \n        # Check available memory before starting epoch\n        if has_psutil and DEVICE == \"cuda\" and torch.cuda.is_available():\n            try:\n                # Get available system memory\n                available_memory_gb = psutil.virtual_memory().available / (1024**3)\n                # Get GPU memory info\n                gpu_memory_allocated = torch.cuda.memory_allocated() / (1024**3)\n                gpu_memory_reserved = torch.cuda.memory_reserved() / (1024**3)\n                \n                print(f\"Memory before epoch - System: {available_memory_gb:.2f}GB free, \"\n                      f\"GPU: {gpu_memory_allocated:.2f}GB allocated, {gpu_memory_reserved:.2f}GB reserved\")\n                \n                # If memory is low, try to clear it\n                if available_memory_gb < 2.0 or gpu_memory_allocated > 3.0:\n                    print(\"Low memory detected - clearing caches\")\n                    gc.collect()\n                    torch.cuda.empty_cache()\n            except Exception as e:\n                print(f\"Memory check error: {e}\")\n        \n        # Create progress bar\n        progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}\")\n        \n        for batch_idx, (input_ids, attention_mask) in enumerate(progress_bar):\n            try:\n                # Prepare data\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                \n                # Forward pass\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                \n                loss = outputs.loss\n                \n                # Backward pass\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                scheduler.step()\n                \n                # Track metrics\n                loss_val = loss.item()\n                epoch_losses.append(loss_val)\n                perplexity = torch.exp(torch.tensor(loss_val)).item()\n                \n                progress_bar.set_postfix(loss=f\"{loss_val:.4f}\", ppl=f\"{perplexity:.2f}\")\n                \n                # Generate sample text every 50 steps with error handling\n                if step % 50 == 0:\n                    try:\n                        sample_text = generate_text(model, tokenizer, prompt=\"A large language model is\")\n                        \n                        if progress_tracker:\n                            # Get gate values for visualization\n                            gate_values = get_gate_values(model)\n                            \n                            progress_tracker.update(\n                                step=step,\n                                loss=loss_val,\n                                perplexity=perplexity,\n                                gate_values=gate_values,\n                                generation_sample=sample_text\n                            )\n                    except Exception as gen_error:\n                        print(f\"Error during sample generation: {gen_error}\")\n                        # Still update metrics without the generation sample\n                        if progress_tracker:\n                            progress_tracker.update(\n                                step=step,\n                                loss=loss_val,\n                                perplexity=perplexity\n                            )\n                \n                step += 1\n                \n                # Clear memory periodically\n                if step % 100 == 0 and DEVICE == \"cuda\":\n                    gc.collect()\n                    torch.cuda.empty_cache()\n                    \n            except RuntimeError as e:\n                if \"CUDA\" in str(e) or \"out of memory\" in str(e).lower():\n                    print(f\"\\nCUDA error during training: {e}\")\n                    \n                    # Try to recover by clearing memory\n                    if DEVICE == \"cuda\":\n                        print(\"Attempting to clear GPU memory...\")\n                        torch.cuda.empty_cache()\n                        gc.collect()\n                    \n                    # Skip this batch and continue\n                    print(\"Skipping batch and continuing...\")\n                    continue\n                else:\n                    # For non-CUDA errors, print and continue\n                    print(f\"Error during training: {e}\")\n                    continue\n            except Exception as gen_error:\n                print(f\"Unexpected error: {gen_error}\")\n                continue\n        \n        # Evaluate after each epoch with error handling\n        try:\n            eval_loss, eval_ppl = evaluate_model(model, val_dataloader)\n            \n            # Print epoch summary\n            if epoch_losses:  # Make sure we have some losses to average\n                epoch_loss = sum(epoch_losses) / len(epoch_losses)\n                epoch_ppl = torch.exp(torch.tensor(epoch_loss)).item()\n                \n                print(f\"Epoch {epoch+1} summary:\")\n                print(f\"  Train loss: {epoch_loss:.4f}, perplexity: {epoch_ppl:.2f}\")\n                print(f\"  Val loss: {eval_loss:.4f}, perplexity: {eval_ppl:.2f}\")\n            else:\n                print(f\"Epoch {epoch+1} summary:\")\n                print(f\"  No valid training steps completed\")\n                print(f\"  Val loss: {eval_loss:.4f}, perplexity: {eval_ppl:.2f}\")\n            \n            # Track validation metrics\n            if progress_tracker:\n                try:\n                    sample_text = generate_text(\n                        model, tokenizer, \n                        prompt=\"In recent years, artificial intelligence has\"\n                    )\n                    progress_tracker.update(\n                        step=step,\n                        loss=eval_loss,\n                        perplexity=eval_ppl,\n                        generation_sample=sample_text\n                    )\n                except Exception as e:\n                    print(f\"Error during evaluation text generation: {e}\")\n                    # Still update metrics\n                    progress_tracker.update(\n                        step=step,\n                        loss=eval_loss,\n                        perplexity=eval_ppl\n                    )\n        except Exception as eval_error:\n            print(f\"Error during evaluation: {eval_error}\")\n            # Try to continue to the next epoch\n    \n    # Final evaluation with error handling\n    try:\n        final_loss, final_ppl = evaluate_model(model, val_dataloader)\n        print(f\"Final evaluation - Loss: {final_loss:.4f}, Perplexity: {final_ppl:.2f}\")\n    except Exception as e:\n        print(f\"Error during final evaluation: {e}\")\n        # Use the last known evaluation metrics\n        if 'eval_loss' in locals() and 'eval_ppl' in locals():\n            final_loss, final_ppl = eval_loss, eval_ppl\n        else:\n            # If we have no metrics at all, use placeholder values\n            final_loss, final_ppl = 999.0, 999.0\n        print(f\"Using fallback metrics - Loss: {final_loss:.4f}, Perplexity: {final_ppl:.2f}\")\n    \n    return {\n        \"final_loss\": final_loss,\n        \"final_perplexity\": final_ppl,\n        \"steps\": step\n    }\n\ndef evaluate_model(model, dataloader):\n    \"\"\"Evaluate model on dataloader and return loss and perplexity.\"\"\"\n    model.eval()\n    total_loss = 0\n    total_batches = 0\n    \n    with torch.no_grad():\n        for input_ids, attention_mask in dataloader:\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            \n            try:\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                \n                loss = outputs.loss\n                total_loss += loss.item()\n                total_batches += 1\n            except RuntimeError as e:\n                if \"CUDA\" in str(e):\n                    print(f\"CUDA error during evaluation, attempting fallback: {e}\")\n                    try:\n                        # Try with CPU\n                        cpu_model = model.cpu()\n                        cpu_input_ids = input_ids.cpu()\n                        cpu_attention_mask = attention_mask.cpu()\n                        \n                        outputs = cpu_model(\n                            input_ids=cpu_input_ids,\n                            attention_mask=cpu_attention_mask,\n                            labels=cpu_input_ids\n                        )\n                        \n                        loss = outputs.loss\n                        total_loss += loss.item()\n                        total_batches += 1\n                        \n                        # Move model back\n                        model.to(DEVICE)\n                    except Exception as cpu_error:\n                        print(f\"CPU fallback failed: {cpu_error}\")\n                        # Skip this batch\n                        continue\n                else:\n                    print(f\"Error during evaluation: {e}\")\n                    # Skip this batch\n                    continue\n    \n    # Calculate average loss and perplexity\n    if total_batches == 0:\n        return 999.0, 999.0  # Return high values if all batches failed\n        \n    avg_loss = total_loss / total_batches\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n    \n    return avg_loss, perplexity\n\ndef get_gate_values(model):\n    \"\"\"Extract gate values from model for visualization.\"\"\"\n    attention_modules = get_attention_modules(model)\n    \n    gate_values = {}\n    for layer_idx, attn in attention_modules:\n        if hasattr(attn, \"head_gates\"):\n            gate_values[f\"layer_{layer_idx}\"] = attn.head_gates.detach().cpu().numpy()\n    \n    return gate_values\n\ndef generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7):\n    \"\"\"Generate text from a prompt using the model.\"\"\"\n    model.eval()\n    \n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n    \n    try:\n        with torch.no_grad():\n            # First try with all options enabled\n            output = model.generate(\n                input_ids,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                num_return_sequences=1\n            )\n        \n        # Decode the output\n        return tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    except RuntimeError as e:\n        if \"CUDA\" in str(e):\n            print(\"CUDA error during generation, attempting fallback to CPU...\")\n            # Try moving to CPU for generation\n            try:\n                cpu_model = model.cpu()\n                cpu_input_ids = input_ids.cpu()\n                \n                with torch.no_grad():\n                    output = cpu_model.generate(\n                        cpu_input_ids,\n                        max_length=max_length,\n                        temperature=temperature,\n                        top_p=0.9,\n                        do_sample=True,\n                        pad_token_id=tokenizer.eos_token_id,\n                        num_return_sequences=1\n                    )\n                \n                # Move model back to original device\n                model.to(DEVICE)\n                \n                # Decode the output\n                return tokenizer.decode(output[0], skip_special_tokens=True)\n            \n            except Exception as cpu_error:\n                print(f\"CPU fallback also failed: {cpu_error}\")\n                # Try with safer parameters\n                model.to(DEVICE)  # Ensure model is back on the original device\n                try:\n                    with torch.no_grad():\n                        # Try with simpler generation parameters\n                        output = model.generate(\n                            input_ids,\n                            max_length=max_length,\n                            do_sample=False,  # Use greedy decoding\n                            pad_token_id=tokenizer.eos_token_id,\n                            num_return_sequences=1\n                        )\n                    \n                    return tokenizer.decode(output[0], skip_special_tokens=True)\n                except Exception as safe_error:\n                    print(f\"Safe generation also failed: {safe_error}\")\n                    return f\"{prompt} [Error: Text generation failed]\"\n        \n        # For other types of errors, try with safer parameters\n        try:\n            print(f\"Error during generation: {e}, trying with safer parameters...\")\n            with torch.no_grad():\n                # Try with simpler generation parameters\n                output = model.generate(\n                    input_ids,\n                    max_length=max_length,\n                    do_sample=False,  # Use greedy decoding\n                    pad_token_id=tokenizer.eos_token_id\n                )\n            \n            return tokenizer.decode(output[0], skip_special_tokens=True)\n        except Exception as safe_error:\n            print(f\"Safe generation also failed: {safe_error}\")\n            return f\"{prompt} [Error: Text generation failed]\"\n\ndef create_head_importance_visualization(head_importances, pruned_heads, output_path=None):\n    \"\"\"Create visualization of head importances and pruned heads.\"\"\"\n    # Organize by layer\n    layers = {}\n    for layer_idx, head_idx, importance in head_importances:\n        if layer_idx not in layers:\n            layers[layer_idx] = []\n        layers[layer_idx].append((head_idx, importance))\n    \n    # Convert pruned_heads to a set for faster lookup\n    pruned_set = set((layer, head) for layer, head in pruned_heads)\n    \n    # Create figure - adjust size based on layer count\n    num_layers = len(layers)\n    max_heads_per_layer = max([len(heads) for heads in layers.values()]) if layers else 0\n    \n    # Calculate appropriate figure height (minimum 6, scales with layer count)\n    fig_height = max(6, num_layers * 0.4)\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(12, fig_height))\n    \n    # Prepare data for plotting - LIMIT displayed items if too many\n    layer_labels = []\n    head_importance_data = []\n    colors = []\n    \n    # Determine if we need to limit labels (if too many layers/heads)\n    too_many_layers = num_layers > 30\n    \n    # Track which layers we're showing\n    included_layers = set()\n    \n    # Process data for visualization, prioritizing pruned heads and limiting total items\n    max_labels = 40  # Maximum number of labels to show\n    \n    # First add all pruned heads\n    for layer_idx in sorted(layers.keys()):\n        heads = layers[layer_idx]\n        for head_idx, importance in sorted(heads, key=lambda x: x[0]):\n            if (layer_idx, head_idx) in pruned_set:\n                layer_labels.append(f\"L{layer_idx}-H{head_idx}\")\n                head_importance_data.append(importance)\n                colors.append('red')  # Pruned heads are red\n                included_layers.add(layer_idx)\n    \n    # Then add remaining heads until we hit the limit, sampling across layers\n    remaining_slots = max_labels - len(layer_labels)\n    if remaining_slots > 0:\n        # Get layers that have at least one non-pruned head\n        remaining_layers = sorted(set(layer_idx for layer_idx, head_idx, _ in head_importances \n                               if (layer_idx, head_idx) not in pruned_set))\n        \n        # If we have too many layers, sample evenly\n        if len(remaining_layers) > remaining_slots:\n            # Sample layers evenly\n            step = len(remaining_layers) / remaining_slots\n            sampled_indices = [int(i * step) for i in range(remaining_slots)]\n            remaining_layers = [remaining_layers[i] for i in sampled_indices]\n        \n        # Add one head from each remaining layer\n        for layer_idx in remaining_layers:\n            if len(layer_labels) >= max_labels:\n                break\n                \n            heads = layers[layer_idx]\n            # Find first non-pruned head\n            for head_idx, importance in sorted(heads, key=lambda x: x[0]):\n                if (layer_idx, head_idx) not in pruned_set:\n                    layer_labels.append(f\"L{layer_idx}-H{head_idx}\")\n                    head_importance_data.append(importance)\n                    colors.append('blue')  # Kept heads are blue\n                    included_layers.add(layer_idx)\n                    break\n    \n    # Create horizontal bar chart\n    y_pos = np.arange(len(layer_labels))\n    ax.barh(y_pos, head_importance_data, color=colors)\n    \n    # Add labels\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(layer_labels)\n    ax.invert_yaxis()  # Labels read top-to-bottom\n    ax.set_xlabel('Importance Score')\n    \n    # Create title with info on displayed vs total heads\n    total_heads = sum(len(heads) for heads in layers.values())\n    displayed_heads = len(layer_labels)\n    title = f'Attention Head Importance (red = pruned, showing {displayed_heads}/{total_heads} heads)'\n    ax.set_title(title)\n    \n    # Add a note if not all heads are shown\n    if displayed_heads < total_heads:\n        total_layers = len(layers)\n        shown_layers = len(included_layers)\n        plt.figtext(0.5, 0.01, \n                   f\"Note: Only a subset of heads are shown for clarity. {shown_layers}/{total_layers} layers represented.\", \n                   ha=\"center\", fontsize=9, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n    \n    # Save figure if path provided\n    plt.tight_layout()\n    if output_path:\n        plt.savefig(output_path)\n    \n    return fig\n\ndef visualize_gate_values(gate_values, output_path=None):\n    \"\"\"Create visualization of gate values across layers.\"\"\"\n    if not gate_values:\n        return None\n    \n    # Prepare data\n    layers = sorted(gate_values.keys())\n    data = [gate_values[layer] for layer in layers]\n    \n    # Create figure\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # Plot heatmap-like visualization\n    for i, (layer, values) in enumerate(zip(layers, data)):\n        # Create scatter plot for each layer\n        x = np.arange(len(values))\n        y = np.ones_like(x) * i\n        \n        # Use values to determine color and size\n        colors = ['red' if v < 0.01 else 'blue' for v in values]\n        sizes = [10 + 40 * v for v in values]\n        \n        ax.scatter(x, y, c=colors, s=sizes, alpha=0.7)\n    \n    # Customize plot\n    ax.set_yticks(np.arange(len(layers)))\n    ax.set_yticklabels([layer.replace('layer_', 'Layer ') for layer in layers])\n    ax.set_xlabel('Head Index')\n    ax.set_ylabel('Layer')\n    ax.set_title('Attention Head Gate Values (Red = Pruned)')\n    \n    # Add colorbar legend\n    import matplotlib.patches as mpatches\n    red_patch = mpatches.Patch(color='red', label='Pruned (gate \u2248 0)')\n    blue_patch = mpatches.Patch(color='blue', label='Active (gate = 1)')\n    ax.legend(handles=[red_patch, blue_patch], loc='upper right')\n    \n    # Save figure if path provided\n    plt.tight_layout()\n    if output_path:\n        plt.savefig(output_path)\n    \n    return fig\n\ndef main(args):\n    \"\"\"Main function.\"\"\"\n    print(\"Starting pruning and fine-tuning experiment...\")\n    print(f\"Running on device: {DEVICE}\")\n    \n    # Clear memory if possible (helpful for large models)\n    try:\n        import gc\n        import torch\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            print(\"Cleared CUDA cache\")\n    except Exception as e:\n        print(f\"Memory management error: {e}\")\n    \n    # Set up directories\n    output_dir, model_cache_dir, data_dir = setup_directories()\n    \n    # Create timestamp for output\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    run_dir = os.path.join(output_dir, f\"{args.model_name.replace('/', '_')}_{args.strategy}_{timestamp}\")\n    os.makedirs(run_dir, exist_ok=True)\n    \n    # Initialize progress tracker\n    progress = ProgressMetrics()\n    \n    # Load model and tokenizer\n    model, tokenizer = load_model_and_tokenizer(args.model_name, cache_dir=model_cache_dir)\n    \n    # Load data\n    train_dataloader, val_dataloader = load_wikitext_data(\n        tokenizer, \n        max_length=args.max_length, \n        batch_size=args.batch_size\n    )\n    \n    if train_dataloader is None or val_dataloader is None:\n        print(\"Failed to load data. Exiting.\")\n        return 1\n    \n    # Initial evaluation\n    print(\"Evaluating initial model performance...\")\n    initial_loss, initial_ppl = evaluate_model(model, val_dataloader)\n    print(f\"Initial loss: {initial_loss:.4f}, perplexity: {initial_ppl:.2f}\")\n    \n    # Generate example text\n    initial_generation = generate_text(\n        model, tokenizer, \n        prompt=\"Artificial intelligence is becoming increasingly important because\"\n    )\n    print(\"\\nInitial generation example:\")\n    print(initial_generation)\n    \n    # Record initial metrics\n    progress.update(step=0, loss=initial_loss, perplexity=initial_ppl, \n                   generation_sample=initial_generation)\n    \n    # Calculate head importances\n    head_importances = get_head_importances(model, val_dataloader, strategy=args.strategy)\n    \n    # Prune heads\n    pruned_heads = prune_heads(model, head_importances, pruning_level=args.pruning_level)\n    \n    # Record pruning information\n    progress.set_pruning_info(args.strategy, args.pruning_level, pruned_heads)\n    \n    # Save head importance visualization\n    importance_viz_path = os.path.join(run_dir, \"head_importances.png\")\n    create_head_importance_visualization(\n        head_importances, pruned_heads, importance_viz_path\n    )\n    \n    # Evaluate after pruning\n    print(\"\\nEvaluating pruned model performance...\")\n    pruned_loss, pruned_ppl = evaluate_model(model, val_dataloader)\n    print(f\"After pruning: loss: {pruned_loss:.4f}, perplexity: {pruned_ppl:.2f}\")\n    \n    # Generate example text with pruned model\n    pruned_generation = generate_text(\n        model, tokenizer, \n        prompt=\"Artificial intelligence is becoming increasingly important because\"\n    )\n    print(\"\\nAfter pruning generation example:\")\n    print(pruned_generation)\n    \n    # Record metrics after pruning\n    progress.update(step=1, loss=pruned_loss, perplexity=pruned_ppl, \n                   gate_values=get_gate_values(model),\n                   generation_sample=pruned_generation)\n    \n    # Save gate visualization\n    gate_viz_path = os.path.join(run_dir, \"gate_values.png\")\n    visualize_gate_values(get_gate_values(model), gate_viz_path)\n    \n    # Fine-tune pruned model\n    print(\"\\nFine-tuning pruned model...\")\n    try:\n        # Clear GPU memory before fine-tuning\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        # Check if we should use CPU directly\n        try:\n            import psutil\n            ram_gb = psutil.virtual_memory().available / (1024 ** 3)\n            if DEVICE == \"cpu\" or (DEVICE == \"cuda\" and ram_gb < 2.0):\n                print(\"Using CPU for fine-tuning due to memory constraints\")\n                model = model.cpu()\n                DEVICE = \"cpu\"\n                \n                # Reduce batch size and epochs on CPU\n                train_dataloader = torch.utils.data.DataLoader(\n                    train_dataloader.dataset, \n                    batch_size=max(1, args.batch_size // 2),\n                    shuffle=True\n                )\n                val_dataloader = torch.utils.data.DataLoader(\n                    val_dataloader.dataset, \n                    batch_size=max(1, args.batch_size // 2),\n                    shuffle=False\n                )\n                \n                args.epochs = min(args.epochs, 1)\n        except ImportError:\n            pass\n        \n        # Use autocast for better memory efficiency\n        import contextlib\n        \n        @contextlib.contextmanager\n        def autocast_if_available():\n            if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast') and USE_FP16:\n                with torch.cuda.amp.autocast():\n                    yield\n            else:\n                yield\n                \n        with autocast_if_available():\n            fine_tune_results = fine_tune_model(\n                model, \n                train_dataloader, \n                val_dataloader, \n                tokenizer,\n                learning_rate=args.learning_rate,\n                num_epochs=args.epochs,\n                progress_tracker=progress\n            )\n    except RuntimeError as e:\n        if \"CUDA\" in str(e):\n            print(f\"\\nCUDA error during fine-tuning: {e}\")\n            print(\"\\nAttempting to continue with CPU...\\n\")\n            \n            # Try CPU fallback\n            try:\n                # Reset device to CPU\n                DEVICE = \"cpu\"\n                model = model.cpu()\n                \n                # Try with reduced parameters\n                fine_tune_results = fine_tune_model(\n                    model, \n                    train_dataloader, \n                    val_dataloader, \n                    tokenizer,\n                    learning_rate=args.learning_rate / 2,  # Lower learning rate\n                    num_epochs=1,  # Single epoch\n                    progress_tracker=progress\n                )\n            except Exception as cpu_error:\n                print(f\"CPU fallback also failed: {cpu_error}\")\n                # Use pruned model metrics since fine-tuning failed\n                fine_tune_results = {\n                    \"final_loss\": pruned_loss, \n                    \"final_perplexity\": pruned_ppl,\n                    \"steps\": 0\n                }\n        else:\n            print(f\"Error during fine-tuning: {e}\")\n            # Use pruned model metrics since fine-tuning failed\n            fine_tune_results = {\n                \"final_loss\": pruned_loss,\n                \"final_perplexity\": pruned_ppl,\n                \"steps\": 0\n            }\n    \n    # Generate final examples\n    final_generation = generate_text(\n        model, tokenizer, \n        prompt=\"Artificial intelligence is becoming increasingly important because\",\n        temperature=0.7\n    )\n    print(\"\\nFinal generation example:\")\n    print(final_generation)\n    \n    # Compare results\n    summary = progress.get_summary()\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"EXPERIMENT SUMMARY\")\n    print(\"=\"*50)\n    print(f\"Model: {args.model_name}\")\n    print(f\"Pruning strategy: {args.strategy}\")\n    print(f\"Pruning level: {args.pruning_level:.1%}\")\n    print(f\"Pruned heads: {len(pruned_heads)}\")\n    print(\"\\nPerformance:\")\n    print(f\"  Initial perplexity: {initial_ppl:.2f}\")\n    print(f\"  After pruning: {pruned_ppl:.2f} ({(pruned_ppl-initial_ppl)/initial_ppl*100:+.2f}%)\")\n    print(f\"  After fine-tuning: {fine_tune_results['final_perplexity']:.2f} ({(fine_tune_results['final_perplexity']-initial_ppl)/initial_ppl*100:+.2f}%)\")\n    \n    # Save final plots\n    progress.save_plots(os.path.join(run_dir, \"training_progress.png\"))\n    progress.save_metrics(os.path.join(run_dir, \"metrics.json\"))\n    \n    # Save text samples\n    with open(os.path.join(run_dir, \"text_samples.txt\"), \"w\") as f:\n        f.write(\"INITIAL MODEL\\n\")\n        f.write(\"============\\n\")\n        f.write(initial_generation)\n        f.write(\"\\n\\nAFTER PRUNING\\n\")\n        f.write(\"============\\n\")\n        f.write(pruned_generation)\n        f.write(\"\\n\\nAFTER FINE-TUNING\\n\")\n        f.write(\"===============\\n\")\n        f.write(final_generation)\n    \n    print(f\"\\nAll results and visualizations saved to: {run_dir}\")\n    \n    return 0\n\ndef parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Prune and fine-tune GPT-2\")\n    \n    # Model configuration\n    parser.add_argument(\"--model_name\", type=str, default=\"distilgpt2\",\n                        help=\"Name of the model to use (e.g., distilgpt2, facebook/opt-125m, EleutherAI/pythia-70m)\")\n    parser.add_argument(\"--max_length\", type=int, default=256,\n                       help=\"Maximum sequence length\")\n    \n    # Pruning configuration\n    parser.add_argument(\"--strategy\", type=str, default=\"entropy\",\n                        choices=[\"random\", \"magnitude\", \"entropy\"],\n                        help=\"Pruning strategy to use\")\n    parser.add_argument(\"--pruning_level\", type=float, default=0.3,\n                       help=\"Fraction of heads to prune (0.0 to 1.0)\")\n    \n    # Training configuration\n    parser.add_argument(\"--batch_size\", type=int, default=4,\n                       help=\"Batch size for training and evaluation\")\n    parser.add_argument(\"--learning_rate\", type=float, default=5e-5,\n                       help=\"Learning rate for fine-tuning\")\n    parser.add_argument(\"--epochs\", type=int, default=3,\n                       help=\"Number of training epochs\")\n    parser.add_argument(\"--fp16\", action=\"store_true\",\n                       help=\"Use FP16/mixed precision (default: auto-detect)\")\n    \n    args = parser.parse_args()\n    \n    # Adjust batch size and sequence length based on device\n    if DEVICE == \"cpu\" and args.batch_size > 2:\n        print(\"Running on CPU - reducing batch size to 2\")\n        args.batch_size = 2\n        \n    if DEVICE == \"cpu\" and args.max_length > 128:\n        print(\"Running on CPU - reducing max_length to 128\")\n        args.max_length = 128\n        \n    # Override FP16 setting if specified\n    if args.fp16:\n        global USE_FP16\n        USE_FP16 = True\n        print(\"FP16 manually enabled\")\n    \n    return args\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    sys.exit(main(args))"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}, "colab": {"name": "PruningAndFineTuningColab", "provenance": []}}, "nbformat": 4, "nbformat_minor": 0}