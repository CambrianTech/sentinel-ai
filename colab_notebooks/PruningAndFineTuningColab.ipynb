{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pruning and Fine-Tuning Benchmark for Google Colab (v1.0.0)\n\nThis is the Python script version of our notebook for Google Colab.\n\nInstructions:\n1. Upload to a new Colab notebook using File > Upload notebook > Upload\n2. Runtime > Change runtime type > Select GPU or TPU hardware accelerator\n3. Run cells to execute pruning and fine-tuning experiments\n\n## Overview\n\n1. **Baseline Evaluation**: Establish the initial model performance\n2. **Pruning Phase**: Apply different pruning strategies and evaluate post-pruning performance\n3. **Fine-Tuning Phase**: Fine-tune pruned models to recover or improve performance\n4. **Analysis**: Compare performance across pruning levels and fine-tuning epochs\n\nThis experiment will run until interrupted, continuously improving the models and updating visualizations.\n\n## Setup\n\nFirst, let's install dependencies and clone the repository:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages and make sure HuggingFace datasets is properly installed\n!pip install -q jax jaxlib flax transformers matplotlib numpy pandas seaborn tqdm optax\n!pip install -q 'datasets>=2.0.0' multiprocess"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository (version 1.0.0) but make sure it's not in the Python path yet\n!git clone -b main https://github.com/CambrianTech/sentinel-ai.git\n# Don't cd into it yet"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import huggingface datasets directly before changing directory\n# We want to make sure we're using the system package\nfrom datasets import load_dataset\nimport datasets\nprint(f\"Using datasets from: {datasets.__file__}\")\n\n# Now safely change to the repository directory\n%cd sentinel-ai\n\n# Import rest of the libraries\nimport os\nimport sys\nimport json\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import JAX/Flax\nimport jax\nimport jax.numpy as jnp\nimport optax\nfrom flax.training.train_state import TrainState\n\n# Import Hugging Face libraries\nfrom transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n\n# Add the current directory to path and import our modules\nsys.path.append(\".\")\nfrom utils.pruning import (\n    Environment,\n    ResultsManager,\n    PruningModule, \n    get_strategy\n)\n\n# Set up plotting\nplt.style.use('ggplot')\nsns.set_theme(style=\"whitegrid\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "# Import our pruning library\n",
    "from utils.pruning import (\n",
    "    Environment,\n",
    "    ResultsManager,\n",
    "    PruningModule, \n",
    "    get_strategy\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Detection\n",
    "\n",
    "Let's detect our environment capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()\n",
    "\n",
    "# Check JAX capabilities\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Implementation\n",
    "\n",
    "First, let's implement the fine-tuning functionality:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class FineTuner:\n    \"\"\"Fine-tunes a pruned model to recover performance\"\"\"\n    \n    def __init__(self, pruning_module, dataset_name=\"openwebtext\", dataset_config=None, batch_size=4):\n        self.pruning_module = pruning_module\n        self.dataset_name = dataset_name\n        self.dataset_config = dataset_config\n        self.batch_size = batch_size\n        self.max_seq_length = 128  # Modest sequence length for faster training\n        self.train_state = None\n        self.metrics_history = []\n        \n        # Detect number of devices\n        self.devices = jax.devices()\n        self.n_devices = len(self.devices)\n        if self.n_devices > 1:\n            print(f\"Using {self.n_devices} devices for training\")\n            self.batch_size = max(self.batch_size, self.n_devices)\n            # Make batch size divisible by device count\n            self.batch_size = (self.batch_size // self.n_devices) * self.n_devices\n            print(f\"Adjusted batch size to {self.batch_size} for multi-device training\")\n    \n    def _prepare_dataset(self):\n        \"\"\"Load and prepare the dataset for fine-tuning\"\"\"\n        try:\n            # Try to load a small portion of the dataset for faster loading\n            if self.dataset_config:\n                print(f\"Loading dataset {self.dataset_name} with config {self.dataset_config}\")\n                dataset = load_dataset(self.dataset_name, self.dataset_config, split=\"train[:5000]\")\n            else:\n                print(f\"Loading dataset {self.dataset_name}\")\n                dataset = load_dataset(self.dataset_name, split=\"train[:5000]\")\n                \n            print(f\"Dataset loaded: {len(dataset)} examples\")\n            \n            # Process dataset\n            tokenizer = self.pruning_module.tokenizer\n            \n            # Ensure tokenizer has pad_token\n            if tokenizer.pad_token is None:\n                if tokenizer.eos_token is not None:\n                    tokenizer.pad_token = tokenizer.eos_token\n                else:\n                    tokenizer.pad_token = tokenizer.eos_token = \"[PAD]\"\n                print(f\"Set pad_token to {tokenizer.pad_token}\")\n            \n            def tokenize_function(examples):\n                # Tokenize the texts\n                if \"text\" in examples:\n                    texts = examples[\"text\"]\n                else:\n                    # Try to find text field (wikitext has different format)\n                    keys = examples.keys()\n                    text_key = next((k for k in keys if \"text\" in k.lower()), None)\n                    if text_key:\n                        texts = examples[text_key]\n                    else:\n                        # If no text field found, concatenate all string fields\n                        texts = []\n                        for i in range(len(examples[next(iter(keys))])):\n                            example_text = \" \".join(str(examples[k][i]) for k in keys \n                                                if isinstance(examples[k][i], str))\n                            texts.append(example_text)\n                \n                tokenized = tokenizer(\n                    texts, \n                    padding='max_length',\n                    truncation=True,\n                    max_length=self.max_seq_length,\n                    return_tensors=\"np\"\n                )\n                return tokenized\n            \n            # Remove columns that aren't strings\n            columns_to_remove = []\n            for col in dataset.column_names:\n                if isinstance(dataset[0][col], (int, float, bool)) or dataset[0][col] is None:\n                    continue\n                columns_to_remove.append(col)\n            \n            tokenized_dataset = dataset.map(\n                tokenize_function,\n                batched=True,\n                num_proc=1,\n                remove_columns=columns_to_remove\n            )\n            \n            # Create data loader function to process batches\n            def process_batch(batch):\n                # Prepare batch with consistent shape\n                input_ids = np.array(batch[\"input_ids\"])\n                attention_mask = np.array(batch[\"attention_mask\"])\n                \n                # Create labels for causal language modeling (shifted version of input_ids)\n                labels = input_ids.copy()\n                \n                return {\n                    \"input_ids\": input_ids,\n                    \"attention_mask\": attention_mask,\n                    \"labels\": labels\n                }\n            \n            # Batch the dataset\n            batched_dataset = []\n            for i in range(0, len(tokenized_dataset), self.batch_size):\n                end_idx = min(i + self.batch_size, len(tokenized_dataset))\n                batch = tokenized_dataset[i:end_idx]\n                processed_batch = process_batch(batch)\n                batched_dataset.append(processed_batch)\n            \n            return batched_dataset\n            \n        except Exception as e:\n            print(f\"Error preparing dataset: {e}\")\n            print(\"Falling back to synthetic data for training\")\n            import traceback\n            traceback.print_exc()\n            return self._prepare_synthetic_dataset()\n    \n    def _prepare_synthetic_dataset(self):\n        \"\"\"Create synthetic data for training when dataset loading fails\"\"\"\n        tokenizer = self.pruning_module.tokenizer\n        \n        # Ensure tokenizer has pad_token\n        if tokenizer.pad_token is None:\n            if tokenizer.eos_token is not None:\n                tokenizer.pad_token = tokenizer.eos_token\n            else:\n                tokenizer.pad_token = tokenizer.eos_token = \"[PAD]\"\n            print(f\"Set pad_token to {tokenizer.pad_token}\")\n        \n        # Generate random token IDs (avoid special tokens)\n        vocab_size = tokenizer.vocab_size\n        \n        # Get special token IDs (safely)\n        special_tokens = set()\n        for token_name in ['pad_token_id', 'eos_token_id', 'bos_token_id', 'unk_token_id']:\n            token_id = getattr(tokenizer, token_name, None)\n            if token_id is not None:\n                special_tokens.add(token_id)\n        \n        print(f\"Creating synthetic dataset with vocab_size={vocab_size}, special_tokens={special_tokens}\")\n        \n        # Create 100 samples of random token sequences\n        samples = []\n        for _ in range(100):\n            # Generate random length between 10 and max_seq_length\n            length = np.random.randint(10, self.max_seq_length)\n            \n            # Generate random token IDs\n            token_ids = np.random.randint(0, vocab_size, size=length)\n            \n            # Replace special tokens with normal tokens\n            for i, token_id in enumerate(token_ids):\n                if token_id in special_tokens:\n                    token_ids[i] = (token_id + 1) % vocab_size\n                    # Make sure we're not just cycling through special tokens\n                    while token_ids[i] in special_tokens:\n                        token_ids[i] = (token_ids[i] + 1) % vocab_size\n            \n            # Create sample\n            sample = {\n                \"input_ids\": token_ids,\n                \"attention_mask\": np.ones_like(token_ids),\n                \"labels\": token_ids.copy()\n            }\n            samples.append(sample)\n        \n        # Create batches\n        batches = []\n        for i in range(0, len(samples), self.batch_size):\n            batch_samples = samples[i:i+self.batch_size]\n            \n            # Pad to the same length within batch\n            max_len = max(len(s[\"input_ids\"]) for s in batch_samples)\n            max_len = min(max_len, self.max_seq_length)  # Ensure we don't exceed max_seq_length\n            \n            batch = {\n                \"input_ids\": [],\n                \"attention_mask\": [],\n                \"labels\": []\n            }\n            \n            for sample in batch_samples:\n                # Truncate if needed\n                if len(sample[\"input_ids\"]) > max_len:\n                    input_ids = sample[\"input_ids\"][:max_len]\n                    attention_mask = sample[\"attention_mask\"][:max_len]\n                    labels = sample[\"labels\"][:max_len]\n                else:\n                    input_ids = sample[\"input_ids\"]\n                    attention_mask = sample[\"attention_mask\"]\n                    labels = sample[\"labels\"]\n                \n                # Pad if needed\n                pad_len = max_len - len(input_ids)\n                if pad_len > 0:\n                    input_ids = np.pad(input_ids, (0, pad_len), \n                                      constant_values=tokenizer.pad_token_id)\n                    attention_mask = np.pad(attention_mask, (0, pad_len), \n                                          constant_values=0)\n                    labels = np.pad(labels, (0, pad_len), \n                                   constant_values=tokenizer.pad_token_id)\n                \n                batch[\"input_ids\"].append(input_ids)\n                batch[\"attention_mask\"].append(attention_mask)\n                batch[\"labels\"].append(labels)\n            \n            # Convert to arrays\n            batch = {\n                \"input_ids\": np.array(batch[\"input_ids\"]),\n                \"attention_mask\": np.array(batch[\"attention_mask\"]),\n                \"labels\": np.array(batch[\"labels\"])\n            }\n            batches.append(batch)\n        \n        print(f\"Created {len(batches)} synthetic batches\")\n        return batches\n    \n    def _create_train_state(self, params, learning_rate=5e-5):\n        \"\"\"Create a training state for the fine-tuning process\"\"\"\n        # Create optimizer\n        optimizer = optax.adam(learning_rate)\n        \n        # Create train state\n        model = self.pruning_module.model\n        return TrainState.create(\n            apply_fn=model.__call__,\n            params=params,\n            tx=optimizer\n        )\n    \n    def _loss_fn(self, params, batch):\n        \"\"\"Loss function for the language modeling task\"\"\"\n        model = self.pruning_module.model\n        \n        # Extract labels from batch but don't pass them to the model\n        labels = batch.pop(\"labels\", None)\n        \n        # Handle different model architectures\n        try:\n            # Get logits from model - don't pass 'train' param for OPT models\n            # Check if model name contains 'opt' to detect OPT models\n            is_opt_model = 'opt' in self.pruning_module.model_name.lower()\n            \n            if is_opt_model:\n                # OPT models don't accept 'train' parameter\n                outputs = model(**batch, params=params)\n            else:\n                # Other models like GPT-2 might need the 'train' parameter\n                outputs = model(**batch, params=params, train=True)\n                \n            logits = outputs.logits\n            \n            # Add labels back to batch for next iteration\n            batch[\"labels\"] = labels\n            \n            # Create loss mask (don't compute loss for padding tokens)\n            loss_mask = (labels != self.pruning_module.tokenizer.pad_token_id)\n            \n            # Shift logits and labels for next token prediction\n            shift_logits = logits[:, :-1]\n            shift_labels = labels[:, 1:]\n            shift_mask = loss_mask[:, 1:]\n            \n            # Calculate cross entropy loss\n            loss = optax.softmax_cross_entropy_with_integer_labels(\n                shift_logits, shift_labels\n            )\n            \n            # Apply mask and calculate mean\n            loss = (loss * shift_mask).sum() / shift_mask.sum()\n            \n            return loss\n        except Exception as e:\n            print(f\"Model inference error: {e}\")\n            # Add labels back to batch\n            batch[\"labels\"] = labels\n            raise\n    \n    def _train_step(self, state, batch):\n        \"\"\"Single training step\"\"\"\n        grad_fn = jax.value_and_grad(self._loss_fn)\n        loss, grads = grad_fn(state.params, batch)\n        new_state = state.apply_gradients(grads=grads)\n        return new_state, loss\n    \n    def fine_tune(self, pruned_params, num_epochs=1, learning_rate=5e-5, evaluate_interval=5):\n        \"\"\"Fine-tune the pruned model\"\"\"\n        print(f\"\\nFine-tuning model with {self.dataset_name} dataset for {num_epochs} epochs...\")\n        \n        # Prepare dataset\n        dataset = self._prepare_dataset()\n        \n        # Create training state\n        self.train_state = self._create_train_state(pruned_params, learning_rate)\n        self.metrics_history = []\n        \n        # Training loop\n        total_steps = 0\n        perplexity_history = []\n        \n        for epoch in range(num_epochs):\n            # Shuffled dataset for each epoch (if it's a list of batches)\n            if isinstance(dataset, list):\n                np.random.shuffle(dataset)\n                epoch_dataset = dataset\n            else:\n                # If it's a datasets.Dataset, shuffle\n                epoch_dataset = dataset.shuffle()\n            \n            # Create progress bar\n            epoch_desc = f\"Epoch {epoch+1}/{num_epochs}\"\n            batch_count = len(epoch_dataset) if hasattr(epoch_dataset, \"__len__\") else \"?\"\n            progress_bar = tqdm(enumerate(epoch_dataset), desc=epoch_desc, \n                               total=batch_count if batch_count != \"?\" else None)\n            \n            epoch_losses = []\n            \n            for step, batch in progress_bar:\n                # Train step\n                try:\n                    self.train_state, loss = self._train_step(self.train_state, batch)\n                    total_steps += 1\n                    epoch_losses.append(loss.item())\n                    \n                    # Update progress bar\n                    progress_bar.set_description(f\"{epoch_desc} - Loss: {loss.item():.4f}\")\n                    \n                    # Evaluate periodically\n                    if total_steps % evaluate_interval == 0:\n                        # Generate dummy text to check progress\n                        prompt = \"Artificial intelligence will transform\"\n                        try:\n                            generated = self.pruning_module.generate_text(\n                                self.train_state.params, prompt, max_length=30\n                            )\n                            perplexity = self.pruning_module.evaluate_perplexity(\n                                self.train_state.params, prompt\n                            )\n                            perplexity_history.append((total_steps, perplexity))\n                            print(f\"\\nStep {total_steps} - Perplexity: {perplexity:.4f}\")\n                            print(f\"Generated: {generated}\")\n                        except Exception as e:\n                            print(f\"Error evaluating model: {e}\")\n                except Exception as e:\n                    print(f\"Error in training step: {e}\")\n                    # Continue to next batch\n                    continue\n            \n            # End of epoch metrics\n            epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0\n            print(f\"\\nEpoch {epoch+1} completed. Average loss: {epoch_loss:.4f}\")\n            \n            self.metrics_history.append({\n                \"epoch\": epoch + 1,\n                \"loss\": epoch_loss,\n                \"perplexity_history\": perplexity_history\n            })\n        \n        print(\"\\nFine-tuning completed!\")\n        return self.train_state.params, self.metrics_history\n    \n    def plot_training_progress(self, figsize=(10, 5)):\n        \"\"\"Plot training progress\"\"\"\n        if not self.metrics_history:\n            print(\"No training metrics available yet\")\n            return\n        \n        # Set better plot styling\n        plt.rcParams.update({\n            'figure.figsize': figsize,\n            'figure.titlesize': 14,\n            'axes.titlesize': 12,\n            'axes.labelsize': 11,\n            'xtick.labelsize': 10,\n            'ytick.labelsize': 10,\n            'legend.fontsize': 9,\n            'font.family': 'sans-serif'\n        })\n        \n        # Extract epoch losses\n        epochs = [m[\"epoch\"] for m in self.metrics_history]\n        losses = [m[\"loss\"] for m in self.metrics_history]\n        \n        # Extract perplexity history\n        steps = []\n        perplexities = []\n        for m in self.metrics_history:\n            for step, perplexity in m.get(\"perplexity_history\", []):\n                steps.append(step)\n                perplexities.append(perplexity)\n        \n        # Create figure\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n        \n        # Plot losses\n        ax1.plot(epochs, losses, \"o-\", color=\"blue\")\n        ax1.set_xlabel(\"Epoch\")\n        ax1.set_ylabel(\"Loss\")\n        ax1.set_title(\"Training Loss\")\n        ax1.grid(True, linestyle=\"--\", alpha=0.7)\n        \n        # Plot perplexities\n        if steps and perplexities:\n            ax2.plot(steps, perplexities, \"o-\", color=\"green\")\n            ax2.set_xlabel(\"Step\")\n            ax2.set_ylabel(\"Perplexity\")\n            ax2.set_title(\"Perplexity During Training\")\n            ax2.grid(True, linestyle=\"--\", alpha=0.7)\n        else:\n            ax2.text(0.5, 0.5, \"No perplexity data available\",\n                    ha=\"center\", va=\"center\", fontsize=12)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        return fig"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Manager\n",
    "\n",
    "Let's create an experiment manager to run the full experiment:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "class PruningFineTuningExperiment:\n    \"\"\"Manages the pruning + fine-tuning experiment\"\"\"\n    \n    def __init__(self, results_dir=\"pruning_finetuning_results\"):\n        self.results_dir = Path(results_dir)\n        self.results_dir.mkdir(exist_ok=True, parents=True)\n        self.results = []\n        self.current_experiment = {}\n        \n        # Initialize environment\n        self.env = Environment()\n        \n        # Get suitable models for this environment\n        self.available_models = self.env.get_suitable_models()\n        print(f\"Models available: {', '.join(self.available_models)}\")\n        \n        # Setup Results Manager\n        self.results_manager = ResultsManager(str(self.results_dir))\n        self.results_df = pd.DataFrame()\n        \n        # Model size limits based on environment\n        self.model_size_limits = {\n            \"gpt2\": 1000,  # Always allow GPT-2 (124M params)\n            \"gpt2-medium\": 1000,  # Always allow GPT-2 Medium (355M params)\n            \"opt-350m\": 1000,  # Always allow OPT-350M\n            \"opt-125m\": 1000,  # Always allow OPT-125M\n            \"facebook/opt-125m\": 1000,  # Always allow OPT-125M\n            \"facebook/opt-350m\": 1000,  # Always allow OPT-350M\n        }\n        \n        # Add larger models only if we have enough resources\n        if self.env.has_gpu or self.env.has_tpu:\n            # If we have a GPU/TPU, allow medium-sized models\n            self.model_size_limits.update({\n                \"gpt2-large\": 1000,  # Allow GPT-2 Large (774M params) with GPU/TPU\n                \"facebook/opt-1.3b\": 0.3,  # Allow OPT-1.3B with lower pruning levels only\n            })\n            \n            if self.env.has_high_ram:\n                # If we have high RAM, allow larger models\n                self.model_size_limits.update({\n                    \"facebook/opt-2.7b\": 0.2,  # Allow OPT-2.7B with lower pruning levels only\n                })\n    \n    def run_experiment(self, strategies, pruning_levels, prompt, fine_tuning_epochs=1, max_runtime=3600):\n        \"\"\"Run the full experiment\"\"\"\n        if not self.available_models:\n            print(\"No suitable models found for this environment\")\n            return\n        \n        # Start time for runtime tracking\n        start_time = time.time()\n        \n        # Generate all experiment combinations\n        experiments = []\n        for model in self.available_models:\n            for strategy in strategies:\n                for level in pruning_levels:\n                    # Skip model/pruning combinations that would exceed memory limits\n                    model_key = model.split('/')[-1] if '/' in model else model\n                    model_size_limit = self.model_size_limits.get(model, self.model_size_limits.get(model_key, 0.0))\n                    \n                    if level > model_size_limit:\n                        print(f\"Skipping {model} with pruning level {level:.2f} - exceeds memory limits\")\n                        continue\n                        \n                    experiments.append({\n                        \"model\": model,\n                        \"strategy\": strategy,\n                        \"pruning_level\": level,\n                        \"prompt\": prompt,\n                        \"fine_tuning_epochs\": fine_tuning_epochs\n                    })\n        \n        # Shuffle to get more diverse results early\n        random.shuffle(experiments)\n        \n        # Create progress bar\n        pbar = tqdm(total=len(experiments), desc=\"Running experiments\")\n        \n        # Run experiments\n        for i, exp in enumerate(experiments):\n            # Check if we've exceeded the runtime limit\n            current_runtime = time.time() - start_time\n            if max_runtime is not None and current_runtime > max_runtime:\n                print(f\"\\nReached maximum runtime of {max_runtime/3600:.1f} hours\")\n                break\n                \n            # Update progress bar\n            pbar.set_description(f\"Testing {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}\")\n            \n            # Run experiment\n            try:\n                result = self.run_single_experiment(**exp)\n                if result is not None:\n                    self.results.append(result)\n                \n                # Update progress bar\n                pbar.update(1)\n                \n                # Plot intermediate results every few experiments\n                if (i + 1) % 1 == 0 or i == len(experiments) - 1:\n                    self.plot_results()\n            except Exception as e:\n                print(f\"Error in experiment {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}: {e}\")\n                import traceback\n                traceback.print_exc()\n                # Still update progress bar\n                pbar.update(1)\n        \n        # Close progress bar\n        pbar.close()\n        \n        # Final results\n        print(f\"\\nCompleted {len(self.results)} experiments out of {len(experiments)} attempted\")\n        runtime = time.time() - start_time\n        print(f\"Total runtime: {runtime/3600:.2f} hours ({runtime/60:.2f} minutes)\")\n        \n        # Plot final results\n        self.plot_results()\n        \n        return self.results\n    \n    def run_single_experiment(self, model, strategy, pruning_level, prompt, fine_tuning_epochs=1):\n        \"\"\"Run a single experiment with pruning and fine-tuning\"\"\"\n        print(f\"\\n{'='*80}\")\n        print(f\"Experiment: {model}, {strategy} strategy, {pruning_level:.2f} pruning level\")\n        print(f\"{'='*80}\")\n        \n        # Initialize pruning module\n        pruning_module = PruningModule(model)\n        if not pruning_module.load_model():\n            print(f\"Failed to load model {model}\")\n            return None\n        \n        # Store model name in the module for architecture detection\n        pruning_module.model_name = model\n        \n        # Safety check - if model is too large, skip\n        if \"opt-1.3b\" in model.lower() and pruning_level < 0.3:\n            print(f\"WARNING: {model} with pruning level {pruning_level:.2f} may cause memory issues\")\n        \n        # Setup experiment record\n        self.current_experiment = {\n            \"model\": model,\n            \"strategy\": strategy,\n            \"pruning_level\": pruning_level,\n            \"prompt\": prompt,\n            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"stages\": {}\n        }\n        \n        # 1. Evaluate baseline model\n        print(\"\\n>> Stage 1: Evaluating baseline model\")\n        original_params = pruning_module.original_params\n        \n        # Evaluate perplexity and generation\n        perplexity_baseline = pruning_module.evaluate_perplexity(original_params, prompt)\n        print(f\"Baseline perplexity: {perplexity_baseline:.4f}\")\n        \n        generated_baseline = pruning_module.generate_text(original_params, prompt)\n        print(f\"Baseline generated: {generated_baseline}\")\n        \n        # Record baseline results\n        self.current_experiment[\"stages\"][\"baseline\"] = {\n            \"perplexity\": float(perplexity_baseline),\n            \"generated_text\": generated_baseline\n        }\n        \n        # 2. Apply pruning\n        print(\"\\n>> Stage 2: Applying pruning\")\n        pruning_strat = get_strategy(strategy, pruning_module, prompt)\n        \n        # Calculate importance scores\n        print(\"Calculating head importance...\")\n        all_head_importance = pruning_strat.get_head_importance(original_params)\n        \n        # Sort by importance (ascending)\n        all_head_importance.sort(key=lambda x: x[2])\n        \n        # Determine number of heads to prune\n        total_heads = pruning_module.num_layers * pruning_module.num_heads\n        heads_to_prune = int(total_heads * pruning_level)\n        print(f\"Pruning {heads_to_prune} out of {total_heads} heads\")\n        \n        # Get head indices to prune (least important first)\n        head_indices = [(l, h) for l, h, _ in all_head_importance[:heads_to_prune]]\n        \n        # Prune heads\n        print(\"Pruning heads...\")\n        pruned_params = pruning_strat.prune_heads(original_params, head_indices)\n        \n        # Evaluate after pruning\n        perplexity_pruned = pruning_module.evaluate_perplexity(pruned_params, prompt)\n        print(f\"Pruned perplexity: {perplexity_pruned:.4f}\")\n        \n        generated_pruned = pruning_module.generate_text(pruned_params, prompt)\n        print(f\"Pruned generated: {generated_pruned}\")\n        \n        # Record pruning results\n        self.current_experiment[\"stages\"][\"pruned\"] = {\n            \"perplexity\": float(perplexity_pruned),\n            \"perplexity_change\": float(perplexity_pruned - perplexity_baseline),\n            \"generated_text\": generated_pruned,\n            \"pruned_heads\": heads_to_prune,\n            \"total_heads\": total_heads,\n            \"head_indices\": head_indices\n        }\n        \n        # 3. Fine-tune the pruned model\n        print(\"\\n>> Stage 3: Fine-tuning the pruned model\")\n        \n        # Create fine-tuner - use specific wikitext config and OpenWebText as fallback\n        dataset_name = \"wikitext-2-v1\"  # Specify the config name\n        dataset_config = \"wikitext\"\n        \n        # Adjust batch size based on model size and available hardware\n        if self.env.in_colab and self.env.has_tpu:\n            # TPUs can handle larger batch sizes\n            batch_size = 16\n        elif self.env.in_colab and self.env.has_gpu:\n            batch_size = 8\n            # Reduce batch size for larger models\n            if \"1.3b\" in model.lower() or \"large\" in model.lower():\n                batch_size = 4\n        else:\n            batch_size = 4\n            \n        fine_tuner = FineTuner(\n            pruning_module, \n            dataset_name=dataset_config, \n            dataset_config=dataset_name, \n            batch_size=batch_size\n        )\n        \n        # Fine-tune model\n        try:\n            tuned_params, metrics = fine_tuner.fine_tune(\n                pruned_params, \n                num_epochs=fine_tuning_epochs,\n                learning_rate=5e-5,\n                evaluate_interval=5\n            )\n            \n            # Plot training progress\n            fine_tuner.plot_training_progress()\n            \n            # Evaluate fine-tuned model\n            perplexity_tuned = pruning_module.evaluate_perplexity(tuned_params, prompt)\n            print(f\"Fine-tuned perplexity: {perplexity_tuned:.4f}\")\n            \n            generated_tuned = pruning_module.generate_text(tuned_params, prompt)\n            print(f\"Fine-tuned generated: {generated_tuned}\")\n            \n            # Record fine-tuning results\n            self.current_experiment[\"stages\"][\"fine_tuned\"] = {\n                \"perplexity\": float(perplexity_tuned),\n                \"perplexity_change_from_baseline\": float(perplexity_tuned - perplexity_baseline),\n                \"perplexity_change_from_pruned\": float(perplexity_tuned - perplexity_pruned),\n                \"generated_text\": generated_tuned,\n                \"training_epochs\": fine_tuning_epochs,\n                \"training_metrics\": metrics\n            }\n            \n            # Compute recovery percentage\n            if perplexity_pruned > perplexity_baseline:\n                # Calculate how much of the perplexity increase was recovered\n                perplexity_increase = perplexity_pruned - perplexity_baseline\n                perplexity_recovery = perplexity_pruned - perplexity_tuned\n                recovery_percentage = (perplexity_recovery / perplexity_increase) * 100 if perplexity_increase > 0 else 0\n                \n                self.current_experiment[\"stages\"][\"fine_tuned\"][\"recovery_percentage\"] = float(recovery_percentage)\n                print(f\"Recovery percentage: {recovery_percentage:.2f}%\")\n            else:\n                # Pruning improved perplexity, so we measure improvement from baseline\n                improvement_percentage = ((perplexity_baseline - perplexity_tuned) / perplexity_baseline) * 100\n                \n                self.current_experiment[\"stages\"][\"fine_tuned\"][\"improvement_percentage\"] = float(improvement_percentage)\n                print(f\"Improvement percentage: {improvement_percentage:.2f}%\")\n        \n        except Exception as e:\n            print(f\"Error during fine-tuning: {e}\")\n            # Continue with partial results\n            import traceback\n            traceback.print_exc()\n        \n        # 4. Save results\n        print(\"\\n>> Stage 4: Saving results\")\n        \n        # Save to disk\n        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        result_filename = f\"{model.replace('/', '_')}_{strategy}_{pruning_level:.2f}_{timestamp}.json\"\n        result_path = self.results_dir / result_filename\n        \n        import json\n        with open(result_path, \"w\") as f:\n            json.dump(self.current_experiment, f, indent=2)\n            \n        print(f\"Results saved to {result_path}\")\n        \n        # Update DataFrame for plotting\n        self._update_dataframe()\n        \n        return self.current_experiment\n    \n    def _update_dataframe(self):\n        \"\"\"Update DataFrame for visualization\"\"\"\n        # Extract data for DataFrame\n        data = []\n        \n        for result in self.results:\n            # Extract model and strategy info\n            model = result[\"model\"]\n            strategy = result[\"strategy\"]\n            pruning_level = result[\"pruning_level\"]\n            \n            # Add baseline stage\n            if \"baseline\" in result[\"stages\"]:\n                baseline = result[\"stages\"][\"baseline\"]\n                data.append({\n                    \"model\": model,\n                    \"strategy\": strategy,\n                    \"pruning_level\": pruning_level,\n                    \"stage\": \"baseline\",\n                    \"perplexity\": baseline[\"perplexity\"]\n                })\n            \n            # Add pruned stage\n            if \"pruned\" in result[\"stages\"]:\n                pruned = result[\"stages\"][\"pruned\"]\n                data.append({\n                    \"model\": model,\n                    \"strategy\": strategy,\n                    \"pruning_level\": pruning_level,\n                    \"stage\": \"pruned\",\n                    \"perplexity\": pruned[\"perplexity\"],\n                    \"perplexity_change\": pruned.get(\"perplexity_change\", 0)\n                })\n                \n            # Add fine-tuned stage\n            if \"fine_tuned\" in result[\"stages\"]:\n                fine_tuned = result[\"stages\"][\"fine_tuned\"]\n                data.append({\n                    \"model\": model,\n                    \"strategy\": strategy,\n                    \"pruning_level\": pruning_level,\n                    \"stage\": \"fine_tuned\",\n                    \"perplexity\": fine_tuned[\"perplexity\"],\n                    \"perplexity_change_from_baseline\": fine_tuned.get(\"perplexity_change_from_baseline\", 0),\n                    \"perplexity_change_from_pruned\": fine_tuned.get(\"perplexity_change_from_pruned\", 0),\n                    \"recovery_percentage\": fine_tuned.get(\"recovery_percentage\", None),\n                    \"improvement_percentage\": fine_tuned.get(\"improvement_percentage\", None)\n                })\n        \n        self.results_df = pd.DataFrame(data)\n    \n    def plot_results(self, figsize=(10, 8)):\n        \"\"\"Plot comprehensive experiment results\"\"\"\n        if not self.results:\n            print(\"No results available yet\")\n            return\n            \n        # Update DataFrame\n        self._update_dataframe()\n            \n        if self.results_df.empty:\n            print(\"No data available for plotting\")\n            return\n        \n        # Set plot styling to fix layout issues\n        plt.rcParams.update({\n            'figure.figsize': figsize,\n            'figure.titlesize': 14,\n            'axes.titlesize': 11,\n            'axes.labelsize': 10,\n            'xtick.labelsize': 9,\n            'ytick.labelsize': 9,\n            'legend.fontsize': 7,\n            'legend.title_fontsize': 8,\n            'font.family': 'sans-serif'\n        })\n        \n        # Create figure\n        fig = plt.figure(figsize=figsize)\n        \n        # 1. Perplexity across stages by model and strategy\n        plt.subplot(2, 2, 1)\n        \n        # Get unique models and strategies\n        models = self.results_df[\"model\"].unique()\n        strategies = self.results_df[\"strategy\"].unique()\n        \n        # For display, shorten model names\n        model_display = {m: m.split('/')[-1] if '/' in m else m for m in models}\n        \n        # Filter to main stages\n        stages_df = self.results_df[self.results_df[\"stage\"].isin([\"baseline\", \"pruned\", \"fine_tuned\"])]\n        \n        # Plot lines connecting stages for each experiment\n        for model in models:\n            model_df = stages_df[stages_df[\"model\"] == model]\n            \n            for strategy in strategies:\n                strategy_df = model_df[model_df[\"strategy\"] == strategy]\n                \n                for pruning_level in strategy_df[\"pruning_level\"].unique():\n                    experiment_df = strategy_df[strategy_df[\"pruning_level\"] == pruning_level]\n                    \n                    # Sort by stage to ensure correct order\n                    stage_order = {\"baseline\": 0, \"pruned\": 1, \"fine_tuned\": 2}\n                    experiment_df = experiment_df.sort_values(by=\"stage\", key=lambda x: x.map(stage_order))\n                    \n                    # Plot if we have all stages\n                    if len(experiment_df) >= 2:\n                        label = f\"{model_display[model][:6]}, {strategy[:3]}, {pruning_level:.1f}\"\n                        plt.plot(experiment_df[\"stage\"], experiment_df[\"perplexity\"], \"o-\", label=label)\n        \n        plt.title(\"Perplexity Across Stages\")\n        plt.xlabel(\"Stage\")\n        plt.ylabel(\"Perplexity\")\n        plt.xticks(rotation=45)\n        plt.legend(fontsize=7, loc='best', ncol=2)\n        plt.grid(True, alpha=0.3)\n        \n        # 2. Recovery percentage vs pruning level\n        plt.subplot(2, 2, 2)\n        \n        # Get data with recovery information\n        recovery_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n        \n        if not recovery_df.empty:\n            # Create recovery column (combining both metrics)\n            recovery_df[\"recovery\"] = recovery_df[\"recovery_percentage\"]\n            # If improvement percentage exists and recovery is NaN, use negative of improvement\n            mask = recovery_df[\"recovery\"].isna() & recovery_df[\"improvement_percentage\"].notna()\n            recovery_df.loc[mask, \"recovery\"] = -recovery_df.loc[mask, \"improvement_percentage\"]\n            \n            # Plot by strategy\n            for strategy in strategies:\n                strategy_df = recovery_df[recovery_df[\"strategy\"] == strategy]\n                if not strategy_df.empty:\n                    for model in models:\n                        model_strategy_df = strategy_df[strategy_df[\"model\"] == model]\n                        if not model_strategy_df.empty:\n                            # Sort by pruning level\n                            model_strategy_df = model_strategy_df.sort_values(\"pruning_level\")\n                            plt.plot(model_strategy_df[\"pruning_level\"], model_strategy_df[\"recovery\"], \n                                    \"o-\", label=f\"{model_display[model][:6]}, {strategy[:3]}\")\n            \n            plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n            plt.axhline(y=100, color=\"g\", linestyle=\"--\", alpha=0.3)\n            plt.text(0.01, 100, \"Full Recovery\", color=\"green\", ha=\"left\", va=\"bottom\", fontsize=8)\n            plt.text(0.01, -5, \"Improvement\", color=\"blue\", ha=\"left\", va=\"top\", fontsize=8)\n            \n            plt.title(\"Recovery Percentage by Pruning Level\")\n            plt.xlabel(\"Pruning Level\")\n            plt.ylabel(\"Recovery % (negative means improvement)\")\n            plt.legend(fontsize=7, loc='best')\n            plt.grid(True, alpha=0.3)\n        else:\n            plt.text(0.5, 0.5, \"No recovery data available yet\", \n                    ha=\"center\", va=\"center\", fontsize=12)\n        \n        # 3. Perplexity change: pruning vs fine-tuning effect\n        plt.subplot(2, 2, 3)\n        \n        if \"perplexity_change\" in self.results_df.columns and \"perplexity_change_from_pruned\" in self.results_df.columns:\n            # Get pruning change\n            pruned_df = self.results_df[self.results_df[\"stage\"] == \"pruned\"].copy()\n            pruned_df = pruned_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity_change\"]]\n            \n            # Get fine-tuning change\n            finetuned_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n            finetuned_df = finetuned_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity_change_from_pruned\"]]\n            \n            # Merge\n            effects_df = pd.merge(\n                pruned_df, finetuned_df,\n                on=[\"model\", \"strategy\", \"pruning_level\"],\n                suffixes=(\"_pruning\", \"_finetuning\")\n            )\n            \n            if not effects_df.empty:\n                # Plot scatter with size based on pruning level\n                for strategy in strategies:\n                    strategy_df = effects_df[effects_df[\"strategy\"] == strategy]\n                    if not strategy_df.empty:\n                        for model in models:\n                            model_df = strategy_df[strategy_df[\"model\"] == model]\n                            if not model_df.empty:\n                                plt.scatter(\n                                    model_df[\"perplexity_change\"], \n                                    model_df[\"perplexity_change_from_pruned\"],\n                                    s=model_df[\"pruning_level\"] * 300,  # Size based on pruning level\n                                    label=f\"{model_display[model][:6]}, {strategy[:3]}\",\n                                    alpha=0.7\n                                )\n                \n                plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n                plt.axvline(x=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n                \n                # Add quadrant labels (smaller font)\n                plt.text(-5, -5, \"Both improved\", fontsize=8, ha=\"center\", va=\"center\",\n                        bbox=dict(facecolor=\"lightgreen\", alpha=0.5))\n                plt.text(5, -5, \"Pruning hurt,\\nFine-tuning fixed\", fontsize=8, ha=\"center\", va=\"center\",\n                        bbox=dict(facecolor=\"lightblue\", alpha=0.5))\n                plt.text(-5, 5, \"Pruning helped,\\nFine-tuning hurt\", fontsize=8, ha=\"center\", va=\"center\",\n                        bbox=dict(facecolor=\"lightyellow\", alpha=0.5))\n                plt.text(5, 5, \"Both hurt\", fontsize=8, ha=\"center\", va=\"center\",\n                        bbox=dict(facecolor=\"lightcoral\", alpha=0.5))\n                \n                plt.title(\"Effect of Pruning vs. Fine-tuning\")\n                plt.xlabel(\"Perplexity Change from Pruning\")\n                plt.ylabel(\"Perplexity Change from Fine-tuning\")\n                plt.legend(fontsize=7, loc='best')\n                plt.grid(True, alpha=0.3)\n            else:\n                plt.text(0.5, 0.5, \"No effect data available yet\", \n                        ha=\"center\", va=\"center\", fontsize=12)\n        else:\n            plt.text(0.5, 0.5, \"No effect data available yet\", \n                    ha=\"center\", va=\"center\", fontsize=12)\n        \n        # 4. Final results: perplexity reduction by pruning level and strategy\n        plt.subplot(2, 2, 4)\n        \n        if \"perplexity_change_from_baseline\" in self.results_df.columns:\n            # Get baseline and final results\n            baseline_df = self.results_df[self.results_df[\"stage\"] == \"baseline\"].copy()\n            baseline_df = baseline_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n            baseline_df = baseline_df.rename(columns={\"perplexity\": \"baseline_perplexity\"})\n            \n            final_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n            final_df = final_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity\", \"perplexity_change_from_baseline\"]]\n            final_df = final_df.rename(columns={\"perplexity\": \"final_perplexity\"})\n            \n            # Merge\n            final_results = pd.merge(\n                baseline_df, final_df,\n                on=[\"model\", \"strategy\", \"pruning_level\"]\n            )\n            \n            if not final_results.empty:\n                # Plot as bar chart\n                # Group by pruning level and strategy\n                grouped = final_results.groupby([\"pruning_level\", \"strategy\"])[\"perplexity_change_from_baseline\"].mean().reset_index()\n                \n                # Pivot for grouped bar chart\n                pivot_df = grouped.pivot(index=\"pruning_level\", columns=\"strategy\", values=\"perplexity_change_from_baseline\")\n                \n                # Plot\n                pivot_df.plot(kind=\"bar\", ax=plt.gca())\n                \n                plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n                plt.title(\"Final Perplexity Change from Baseline\")\n                plt.xlabel(\"Pruning Level\")\n                plt.ylabel(\"Perplexity Change\")\n                plt.legend(title=\"Strategy\", fontsize=7)\n                plt.grid(True, alpha=0.3, axis=\"y\")\n            else:\n                plt.text(0.5, 0.5, \"No final results available yet\", \n                        ha=\"center\", va=\"center\", fontsize=12)\n        else:\n            plt.text(0.5, 0.5, \"No final results available yet\", \n                    ha=\"center\", va=\"center\", fontsize=12)\n        \n        # Apply tight layout to reduce white space\n        plt.tight_layout(pad=1.5)\n        plt.subplots_adjust(bottom=0.15)\n        plt.show()\n        \n        return fig"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Now we can run the full experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment\n",
    "experiment = PruningFineTuningExperiment(\"pruning_finetuning_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "PRUNING_LEVELS = [0.1, 0.3, 0.5]\n",
    "PROMPT = \"Artificial intelligence will transform society by\"\n",
    "FINE_TUNING_EPOCHS = 2  # Small number for quick iterations\n",
    "MAX_RUNTIME = 6 * 3600  # 6 hours\n",
    "\n",
    "# Start the experiment\n",
    "results = experiment.run_experiment(\n",
    "    strategies=STRATEGIES,\n",
    "    pruning_levels=PRUNING_LEVELS,\n",
    "    prompt=PROMPT,\n",
    "    fine_tuning_epochs=FINE_TUNING_EPOCHS,\n",
    "    max_runtime=MAX_RUNTIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Overnight Run\n",
    "\n",
    "For an extended overnight run, uncomment and run this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Real-time Experiment Monitoring\n\nThe cell below can be executed independently while experiments are running to visualize the current state of experiments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# This cell can be run at any time to visualize current experiment progress\nimport os\nimport glob\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\ndef visualize_ongoing_experiments(results_dir=\"pruning_finetuning_results\", figsize=(10, 8)):\n    \"\"\"\n    Create a real-time visualization of ongoing experiments\n    This can be run independently while experiments are in progress\n    \"\"\"\n    # Set better default styling for plots to fix layout issues\n    plt.rcParams.update({\n        'figure.figsize': (10, 8),\n        'figure.titlesize': 14,\n        'axes.titlesize': 11,\n        'axes.labelsize': 10,\n        'xtick.labelsize': 9,\n        'ytick.labelsize': 9,\n        'legend.fontsize': 7,\n        'legend.title_fontsize': 8,\n        'font.family': 'sans-serif'\n    })\n    \n    # Check if results directory exists\n    if not os.path.exists(results_dir):\n        print(f\"Results directory '{results_dir}' not found\")\n        return\n    \n    # List all result files\n    result_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n    \n    if not result_files:\n        print(f\"No result files found in '{results_dir}'\")\n        return\n    \n    # Load all result files\n    results = []\n    for file_path in result_files:\n        try:\n            with open(file_path, 'r') as f:\n                result = json.load(f)\n                results.append(result)\n        except Exception as e:\n            print(f\"Error loading {file_path}: {e}\")\n    \n    if not results:\n        print(\"No valid result files found\")\n        return\n    \n    print(f\"Found {len(results)} experiment results\")\n    \n    # Extract data for visualization\n    data = []\n    \n    for result in results:\n        # Extract experiment info\n        model = result.get(\"model\", \"unknown\")\n        # Shorten model name for better display\n        if '/' in model:\n            model = model.split('/')[-1]\n        strategy = result.get(\"strategy\", \"unknown\")\n        pruning_level = result.get(\"pruning_level\", 0)\n        timestamp = result.get(\"timestamp\", \"unknown\")\n        \n        # Extract perplexity data from different stages\n        stages = result.get(\"stages\", {})\n        \n        baseline_perplexity = stages.get(\"baseline\", {}).get(\"perplexity\", None)\n        pruned_perplexity = stages.get(\"pruned\", {}).get(\"perplexity\", None)\n        finetuned_perplexity = stages.get(\"fine_tuned\", {}).get(\"perplexity\", None)\n        \n        recovery_percentage = stages.get(\"fine_tuned\", {}).get(\"recovery_percentage\", None)\n        improvement_percentage = stages.get(\"fine_tuned\", {}).get(\"improvement_percentage\", None)\n        \n        # Add to dataframe\n        if baseline_perplexity is not None:\n            data.append({\n                \"model\": model,\n                \"strategy\": strategy,\n                \"pruning_level\": pruning_level,\n                \"stage\": \"baseline\",\n                \"perplexity\": baseline_perplexity,\n                \"timestamp\": timestamp\n            })\n        \n        if pruned_perplexity is not None:\n            data.append({\n                \"model\": model,\n                \"strategy\": strategy,\n                \"pruning_level\": pruning_level,\n                \"stage\": \"pruned\",\n                \"perplexity\": pruned_perplexity,\n                \"timestamp\": timestamp\n            })\n        \n        if finetuned_perplexity is not None:\n            data.append({\n                \"model\": model,\n                \"strategy\": strategy,\n                \"pruning_level\": pruning_level,\n                \"stage\": \"fine_tuned\",\n                \"perplexity\": finetuned_perplexity,\n                \"recovery_percentage\": recovery_percentage,\n                \"improvement_percentage\": improvement_percentage,\n                \"timestamp\": timestamp\n            })\n    \n    # Convert to dataframe\n    df = pd.DataFrame(data)\n    \n    if df.empty:\n        print(\"No valid data extracted from results\")\n        return\n    \n    # Create figure with a more compact layout\n    fig = plt.figure(figsize=figsize)\n    \n    # 1. Perplexity across stages by model and strategy\n    plt.subplot(2, 2, 1)\n    \n    # Get unique values for grouping\n    models = df[\"model\"].unique()\n    strategies = df[\"strategy\"].unique()\n    \n    # Filter to main stages\n    stages_df = df[df[\"stage\"].isin([\"baseline\", \"pruned\", \"fine_tuned\"])]\n    \n    # Plot lines connecting stages for each experiment\n    for model in models:\n        model_df = stages_df[stages_df[\"model\"] == model]\n        \n        for strategy in strategies:\n            strategy_df = model_df[model_df[\"strategy\"] == strategy]\n            \n            for pruning_level in strategy_df[\"pruning_level\"].unique():\n                experiment_df = strategy_df[strategy_df[\"pruning_level\"] == pruning_level]\n                \n                # Sort by stage\n                stage_order = {\"baseline\": 0, \"pruned\": 1, \"fine_tuned\": 2}\n                experiment_df = experiment_df.sort_values(by=\"stage\", key=lambda x: x.map(stage_order))\n                \n                # Only plot if we have at least 2 stages\n                if len(experiment_df) >= 2:\n                    label = f\"{model[:6]}-{strategy[:3]}-{pruning_level:.1f}\"\n                    plt.plot(experiment_df[\"stage\"], experiment_df[\"perplexity\"], \"o-\", label=label)\n    \n    plt.title(\"Perplexity Across Stages\")\n    plt.xlabel(\"Stage\")\n    plt.ylabel(\"Perplexity\")\n    plt.xticks(rotation=45)\n    plt.legend(fontsize=7, loc='upper right', ncol=2)\n    plt.grid(True, alpha=0.3)\n    \n    # 2. Pruning level vs perplexity by strategy\n    plt.subplot(2, 2, 2)\n    \n    # Filter to specific stages\n    baseline_df = df[df[\"stage\"] == \"baseline\"]\n    pruned_df = df[df[\"stage\"] == \"pruned\"]\n    finetuned_df = df[df[\"stage\"] == \"fine_tuned\"]\n    \n    for strategy in strategies:\n        # Get strategy data for each stage\n        baseline_strategy = baseline_df[baseline_df[\"strategy\"] == strategy]\n        pruned_strategy = pruned_df[pruned_df[\"strategy\"] == strategy]\n        finetuned_strategy = finetuned_df[finetuned_df[\"strategy\"] == strategy]\n        \n        # Plot lines for each stage if data exists\n        if not baseline_strategy.empty:\n            plt.plot(baseline_strategy[\"pruning_level\"], baseline_strategy[\"perplexity\"], \n                    \"o--\", label=f\"Base-{strategy[:3]}\", alpha=0.7)\n        \n        if not pruned_strategy.empty:\n            plt.plot(pruned_strategy[\"pruning_level\"], pruned_strategy[\"perplexity\"], \n                    \"s--\", label=f\"Pruned-{strategy[:3]}\", alpha=0.7)\n        \n        if not finetuned_strategy.empty:\n            plt.plot(finetuned_strategy[\"pruning_level\"], finetuned_strategy[\"perplexity\"], \n                    \"^-\", label=f\"Tuned-{strategy[:3]}\", alpha=0.7)\n    \n    plt.title(\"Perplexity vs Pruning Level\")\n    plt.xlabel(\"Pruning Level\")\n    plt.ylabel(\"Perplexity\")\n    plt.legend(fontsize=7, loc='best')\n    plt.grid(True, alpha=0.3)\n    \n    # 3. Recovery/improvement percentages\n    plt.subplot(2, 2, 3)\n    \n    # Create dataframe with recovery metrics\n    recovery_df = finetuned_df.copy()\n    \n    if not recovery_df.empty:\n        # Create unified recovery column (negative means improvement)\n        recovery_df[\"recovery\"] = recovery_df[\"recovery_percentage\"]\n        # If recovery is NaN but improvement exists, use negative of improvement\n        mask = recovery_df[\"recovery\"].isna() & recovery_df[\"improvement_percentage\"].notna()\n        recovery_df.loc[mask, \"recovery\"] = -recovery_df.loc[mask, \"improvement_percentage\"]\n        \n        # Plot by strategy\n        for strategy in strategies:\n            strategy_recovery = recovery_df[recovery_df[\"strategy\"] == strategy]\n            if not strategy_recovery.empty:\n                # Sort by pruning level\n                strategy_recovery = strategy_recovery.sort_values(\"pruning_level\")\n                plt.plot(strategy_recovery[\"pruning_level\"], strategy_recovery[\"recovery\"], \n                        \"o-\", label=strategy)\n        \n        plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n        plt.axhline(y=100, color=\"g\", linestyle=\"--\", alpha=0.3)\n        plt.text(0.01, 100, \"Full Recovery\", color=\"green\", ha=\"left\", va=\"bottom\", fontsize=8)\n        plt.text(0.01, -5, \"Improvement\", color=\"blue\", ha=\"left\", va=\"top\", fontsize=8)\n        \n        plt.title(\"Recovery/Improvement %\")\n        plt.xlabel(\"Pruning Level\")\n        plt.ylabel(\"% (negative = improvement)\")\n        plt.legend(fontsize=7, loc='best')\n        plt.grid(True, alpha=0.3)\n    else:\n        plt.text(0.5, 0.5, \"No recovery data available yet\", \n                ha=\"center\", va=\"center\", fontsize=12)\n    \n    # 4. Status overview\n    plt.subplot(2, 2, 4)\n    \n    # Count experiments by status\n    total_exps = len(set([(r[\"model\"], r[\"strategy\"], r[\"pruning_level\"]) for r in results]))\n    completed_exps = len(finetuned_df)\n    pruned_only = len(set(pruned_df[\"timestamp\"])) - completed_exps\n    baseline_only = len(set(baseline_df[\"timestamp\"])) - pruned_only - completed_exps\n    \n    # Create status labels and counts\n    status_labels = [\"Completed\", \"Pruned\", \"Baseline\", \"Planned\"]\n    status_counts = [\n        completed_exps,\n        pruned_only,\n        baseline_only,\n        total_exps - completed_exps - pruned_only - baseline_only\n    ]\n    \n    # Create status bar chart\n    colors = [\"green\", \"orange\", \"blue\", \"gray\"]\n    plt.bar(status_labels, status_counts, color=colors)\n    \n    for i, count in enumerate(status_counts):\n        plt.text(i, count + 0.1, str(count), ha=\"center\")\n    \n    plt.title(f\"Experiment Status (Total: {total_exps})\")\n    plt.xlabel(\"Status\")\n    plt.ylabel(\"Count\")\n    plt.xticks(rotation=45)\n    \n    # Add timestamp\n    plt.figtext(0.5, 0.01, f\"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", \n               ha=\"center\", fontsize=8)\n    \n    # Apply tight layout to reduce white space\n    plt.tight_layout(pad=1.5)\n    plt.subplots_adjust(bottom=0.15)\n    plt.show()\n    \n    return df\n\n# Run the visualization - you can run this cell repeatedly to refresh\ndf = visualize_ongoing_experiments()\n\n# Display success count by strategy if we have data\nif df is not None and not df.empty and \"fine_tuned\" in df[\"stage\"].values:\n    finetuned = df[df[\"stage\"] == \"fine_tuned\"]\n    \n    # Calculate improvement status\n    finetuned[\"status\"] = \"No Change\"\n    finetuned.loc[finetuned[\"perplexity\"] < finetuned[\"perplexity\"], \"status\"] = \"Improved\"\n    finetuned.loc[finetuned[\"perplexity\"] > finetuned[\"perplexity\"], \"status\"] = \"Degraded\"\n    \n    # Count by strategy and status\n    status_by_strategy = pd.crosstab(finetuned[\"strategy\"], finetuned[\"status\"])\n    display(status_by_strategy)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Head Importance Visualization\n\nThe cell below can be used to visualize which heads are most important in your model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Visualize attention head importance for different pruning strategies\n# This can help identify which heads are most critical for model performance\n\n# Initialize the model\nmodel_name = \"gpt2\"  # Change to one of the models you're using\npruning_module = PruningModule(model_name)\nif not pruning_module.load_model():\n    print(f\"Failed to load model {model_name}\")\nelse:\n    # Get original parameters\n    original_params = pruning_module.original_params\n    \n    # Set up a sample prompt\n    prompt = \"Artificial intelligence will transform society by\"\n    \n    # Calculate importance for different strategies\n    strategies = {}\n    \n    try:\n        # Random strategy (baseline)\n        random_strategy = get_strategy(\"random\", pruning_module, prompt)\n        random_importance = random_strategy.get_head_importance(original_params)\n        strategies[\"random\"] = random_importance\n        \n        # Magnitude strategy\n        magnitude_strategy = get_strategy(\"magnitude\", pruning_module, prompt)\n        magnitude_importance = magnitude_strategy.get_head_importance(original_params)\n        strategies[\"magnitude\"] = magnitude_importance\n        \n        # Entropy strategy\n        entropy_strategy = get_strategy(\"entropy\", pruning_module, prompt)\n        entropy_importance = entropy_strategy.get_head_importance(original_params)\n        strategies[\"entropy\"] = entropy_importance\n        \n        # Set better plot styling\n        plt.rcParams.update({\n            'figure.figsize': (12, 1.5 * pruning_module.num_layers),\n            'figure.titlesize': 14,\n            'axes.titlesize': 12,\n            'axes.labelsize': 10,\n            'xtick.labelsize': 9,\n            'ytick.labelsize': 9,\n            'legend.fontsize': 8,\n            'font.family': 'sans-serif'\n        })\n        \n        # Now visualize the head importance scores\n        fig, axes = plt.subplots(pruning_module.num_layers, 3, figsize=(12, 1.5 * pruning_module.num_layers))\n        \n        # Create title\n        fig.suptitle(f\"Attention Head Importance by Strategy for {model_name}\", fontsize=16)\n        \n        # Set column titles\n        for i, strategy_name in enumerate([\"random\", \"magnitude\", \"entropy\"]):\n            axes[0, i].set_title(f\"{strategy_name.capitalize()} Strategy\")\n        \n        # Create a heatmap for each strategy showing head importance\n        for layer in range(pruning_module.num_layers):\n            for i, strategy_name in enumerate([\"random\", \"magnitude\", \"entropy\"]):\n                # Extract importance scores for this layer\n                layer_scores = [score for l, h, score in strategies[strategy_name] if l == layer]\n                \n                # Create array for visualization\n                scores_array = np.array(layer_scores).reshape(1, -1)\n                \n                # Create heatmap\n                cax = axes[layer, i].imshow(scores_array, cmap=\"viridis\", aspect=\"auto\")\n                \n                # Add labels\n                axes[layer, i].set_yticks([0])\n                axes[layer, i].set_yticklabels([f\"Layer {layer}\"])\n                axes[layer, i].set_xticks(range(pruning_module.num_heads))\n                axes[layer, i].set_xticklabels([f\"H{h}\" for h in range(pruning_module.num_heads)], \n                                              rotation=90 if pruning_module.num_heads > 8 else 0)\n                \n                # Add importance values as text\n                for h in range(pruning_module.num_heads):\n                    score = scores_array[0, h]\n                    if np.isnan(score):\n                        text_color = \"black\"\n                    else:\n                        text_color = \"white\" if score > 0.5 else \"black\"\n                    axes[layer, i].text(h, 0, f\"{score:.2f}\", ha=\"center\", va=\"center\", \n                                       color=text_color, fontsize=8)\n        \n        # Add a colorbar\n        fig.colorbar(cax, ax=axes.ravel().tolist(), shrink=0.6)\n        \n        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for the title\n        plt.show()\n        \n        # Now show the top 10 most and least important heads according to entropy\n        # (usually considered the most accurate measure)\n        sorted_entropy = sorted(entropy_importance, key=lambda x: x[2])\n        \n        print(\"Top 10 Least Important Heads (candidates for pruning):\")\n        for i, (layer, head, score) in enumerate(sorted_entropy[:10]):\n            print(f\"{i+1}. Layer {layer}, Head {head}: {score:.4f}\")\n            \n        print(\"\\nTop 10 Most Important Heads (preserved even with aggressive pruning):\")\n        for i, (layer, head, score) in enumerate(sorted_entropy[-10:]):\n            print(f\"{i+1}. Layer {layer}, Head {head}: {score:.4f}\")\n    \n    except Exception as e:\n        print(f\"Error calculating head importance: {e}\")\n        import traceback\n        traceback.print_exc()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Overnight Configuration - More conservative settings\nOVERNIGHT_STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\nOVERNIGHT_PRUNING_LEVELS = [0.1, 0.2, 0.3, 0.4, 0.5]  # Skip higher levels to avoid memory issues\nOVERNIGHT_PROMPT = \"Artificial intelligence will revolutionize industries by\"\nOVERNIGHT_FINE_TUNING_EPOCHS = 3  # Reduced from 5 to avoid memory issues with larger models\nOVERNIGHT_MAX_RUNTIME = 20 * 3600  # 20 hours\n\n# Initialize experiment for overnight run\novernight_experiment = PruningFineTuningExperiment(\"overnight_results\")\n\n# Run overnight experiment (uncomment to run)\novernight_results = overnight_experiment.run_experiment(\n    strategies=OVERNIGHT_STRATEGIES,\n    pruning_levels=OVERNIGHT_PRUNING_LEVELS,\n    prompt=OVERNIGHT_PROMPT,\n    fine_tuning_epochs=OVERNIGHT_FINE_TUNING_EPOCHS,\n    max_runtime=OVERNIGHT_MAX_RUNTIME\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis\n",
    "\n",
    "After collecting results, run a comprehensive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Plot results with improved sizing\nexperiment.plot_results(figsize=(10, 8))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "if not experiment.results_df.empty:\n",
    "    # Get data for different stages\n",
    "    baseline_df = experiment.results_df[experiment.results_df[\"stage\"] == \"baseline\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    baseline_df = baseline_df.rename(columns={\"perplexity\": \"baseline_perplexity\"})\n",
    "    \n",
    "    pruned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"pruned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    pruned_df = pruned_df.rename(columns={\"perplexity\": \"pruned_perplexity\"})\n",
    "    \n",
    "    finetuned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"fine_tuned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    finetuned_df = finetuned_df.rename(columns={\"perplexity\": \"finetuned_perplexity\"})\n",
    "    \n",
    "    # Merge dataframes\n",
    "    summary = pd.merge(baseline_df, pruned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    summary = pd.merge(summary, finetuned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    \n",
    "    # Calculate changes\n",
    "    summary[\"pruning_effect\"] = summary[\"pruned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    summary[\"finetuning_effect\"] = summary[\"finetuned_perplexity\"] - summary[\"pruned_perplexity\"]\n",
    "    summary[\"net_change\"] = summary[\"finetuned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    \n",
    "    # Display summary\n",
    "    summary.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}