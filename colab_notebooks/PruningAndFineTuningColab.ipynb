{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Make a GPT-2 Model Smaller and More Powerful (v0.0.32)\n\nThis notebook demonstrates how to make a GPT-2 model both smaller and more powerful by:\n1. Applying pruning to remove less important attention heads\n2. Fine-tuning the pruned model to recover and improve performance\n3. Showing clear metrics of improvement throughout the process\n\nWe use real data (Wikitext) rather than synthetic data for realistic evaluation.\n\nVersion History:\n- v0.0.32 (April 2025): Added CUDA error handling for Colab compatibility and memory management\n- v0.0.31 (April 2025): Fixed get_strategy parameters issue and improved Colab compatibility\n- v0.0.30 (April 2025): Added OPT model support and chart improvements\n\n---\n**Note**: This notebook is part of the SentinelAI project. For detailed documentation, see `PruningAndFineTuningColab.md`.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing the required dependencies and configuring our environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Memory management utility for Colab\ndef display_available_memory():\n    \"\"\"Display available memory in Colab.\"\"\"\n    if IS_COLAB:\n        # Get GPU memory info\n        try:\n            !nvidia-smi --query-gpu=memory.total,memory.used --format=csv\n        except:\n            pass\n        \n        # Get system memory info\n        !free -h\n\n# Install required packages\n!pip install -q transformers==4.38.0 datasets==2.17.0 torch matplotlib tqdm\n\n# Check if we're running in Colab\ntry:\n    import google.colab\n    IS_COLAB = True\n    print(\"Running in Google Colab!\")\n    \n    # Add file download helper for Colab\n    from google.colab import files\n    \n    def download_files(file_paths):\n        \"\"\"Helper function to download files from Colab.\"\"\"\n        for file_path in file_paths:\n            if os.path.exists(file_path):\n                files.download(file_path)\n                print(f\"Downloaded: {file_path}\")\n            else:\n                print(f\"File not found: {file_path}\")\n    \n    # Free up memory for Colab\n    import gc\n    import torch\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    # Display memory status\n    display_available_memory()\n    \nexcept:\n    IS_COLAB = False\n    print(\"Not running in Google Colab\")\n    \n    # Dummy function for non-Colab environments\n    def download_files(file_paths):\n        print(\"File download only works in Google Colab\")\n        print(f\"Files would be downloaded: {file_paths}\")\n        \n    def display_available_memory():\n        print(\"Memory display not available outside Colab\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport json\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, \n    get_linear_schedule_with_warmup, \n    GPT2LMHeadModel\n)\n\n# Initialize plotting style\nplt.style.use('seaborn-v0_8-pastel')\n\n# Configure device and optimize for Colab environment\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Half-precision for GPU to reduce memory usage\nUSE_FP16 = DEVICE == \"cuda\"\n\n# Handle TPU if available (Colab-specific optimization)\nif 'COLAB_TPU_ADDR' in os.environ:\n    try:\n        import torch_xla.core.xla_model as xm\n        DEVICE = xm.xla_device()\n        print(f\"TPU detected and configured!\")\n        USE_FP16 = False  # TPUs have their own optimization\n    except ImportError:\n        print(\"TPU environment detected but torch_xla not installed.\")\n\n# Set up directories\nOUTPUT_DIR = \"pruning_results\"\nMODEL_CACHE_DIR = \"model_cache\"\nDATA_DIR = \"data\"\n\n# Create necessary directories\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(MODEL_CACHE_DIR, exist_ok=True)\nos.makedirs(DATA_DIR, exist_ok=True)\n\nprint(f\"Using device: {DEVICE}\")\nprint(f\"Using FP16: {USE_FP16}\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# CUDA memory management helper\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory to avoid CUDA out of memory errors.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"GPU memory cleared\")\n\n# Import garbage collector for memory management\nimport gc\n\n# For better GPU memory management, we'll use a context manager\ntry:\n    import contextlib\n    @contextlib.contextmanager\n    def autocast_if_available():\n        \"\"\"Use autocast if available for better memory efficiency.\"\"\"\n        if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast') and USE_FP16:\n            with torch.cuda.amp.autocast():\n                yield\n        else:\n            yield\nexcept:\n    # Fallback if the import fails\n    @contextlib.contextmanager\n    def autocast_if_available():\n        yield"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Tracking\n",
    "\n",
    "We'll create a class to track metrics and visualize progress throughout the pruning and fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMetrics:\n",
    "    \"\"\"Track metrics throughout the pruning and fine-tuning process.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"loss\": [],\n",
    "            \"perplexity\": [],\n",
    "            \"steps\": [],\n",
    "            \"pruning_level\": None,\n",
    "            \"strategy\": None,\n",
    "            \"pruned_heads\": [],\n",
    "            \"gate_values\": [],\n",
    "            \"head_importance\": [],\n",
    "            \"generation_samples\": []\n",
    "        }\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.fig, self.axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        self.loss_line = None\n",
    "        self.ppl_line = None\n",
    "        \n",
    "    def update(self, step, loss, perplexity, head_info=None, gate_values=None, \n",
    "               generation_sample=None):\n",
    "        \"\"\"Update metrics with new values.\"\"\"\n",
    "        self.metrics[\"steps\"].append(step)\n",
    "        self.metrics[\"loss\"].append(loss)\n",
    "        self.metrics[\"perplexity\"].append(perplexity)\n",
    "        \n",
    "        if head_info is not None:\n",
    "            self.metrics[\"head_importance\"] = head_info\n",
    "            \n",
    "        if gate_values is not None:\n",
    "            self.metrics[\"gate_values\"] = gate_values\n",
    "            \n",
    "        if generation_sample is not None:\n",
    "            self.metrics[\"generation_samples\"].append({\n",
    "                \"step\": step,\n",
    "                \"text\": generation_sample\n",
    "            })\n",
    "        \n",
    "        # Update visualization\n",
    "        self._update_plots()\n",
    "        \n",
    "    def set_pruning_info(self, strategy, level, pruned_heads):\n",
    "        \"\"\"Set pruning information.\"\"\"\n",
    "        self.metrics[\"strategy\"] = strategy\n",
    "        self.metrics[\"pruning_level\"] = level\n",
    "        self.metrics[\"pruned_heads\"] = pruned_heads\n",
    "        \n",
    "    def _update_plots(self):\n",
    "        \"\"\"Update visualization plots.\"\"\"\n",
    "        steps = self.metrics[\"steps\"]\n",
    "        loss = self.metrics[\"loss\"]\n",
    "        ppl = self.metrics[\"perplexity\"]\n",
    "        \n",
    "        if not steps:\n",
    "            return\n",
    "            \n",
    "        # Clear previous plots\n",
    "        self.axes[0].clear()\n",
    "        self.axes[1].clear()\n",
    "        \n",
    "        # Plot loss\n",
    "        self.axes[0].plot(steps, loss, 'b-')\n",
    "        self.axes[0].set_title('Training Loss')\n",
    "        self.axes[0].set_xlabel('Step')\n",
    "        self.axes[0].set_ylabel('Loss')\n",
    "        self.axes[0].grid(True)\n",
    "        \n",
    "        # Plot perplexity\n",
    "        self.axes[1].plot(steps, ppl, 'r-')\n",
    "        self.axes[1].set_title('Perplexity (lower is better)')\n",
    "        self.axes[1].set_xlabel('Step')\n",
    "        self.axes[1].set_ylabel('Perplexity')\n",
    "        self.axes[1].grid(True)\n",
    "        \n",
    "        self.fig.tight_layout()\n",
    "        \n",
    "    def save_plots(self, path):\n",
    "        \"\"\"Save plots to file.\"\"\"\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    def save_metrics(self, path):\n",
    "        \"\"\"Save metrics to file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "            \n",
    "    def get_summary(self):\n",
    "        \"\"\"Return a summary of key metrics.\"\"\"\n",
    "        if not self.metrics[\"perplexity\"] or len(self.metrics[\"perplexity\"]) <= 1:\n",
    "            return {\"error\": \"Not enough data points for summary\"}\n",
    "            \n",
    "        return {\n",
    "            \"strategy\": self.metrics[\"strategy\"],\n",
    "            \"pruning_level\": self.metrics[\"pruning_level\"],\n",
    "            \"pruned_heads_count\": len(self.metrics[\"pruned_heads\"]),\n",
    "            \"initial_loss\": self.metrics[\"loss\"][0],\n",
    "            \"final_loss\": self.metrics[\"loss\"][-1],\n",
    "            \"initial_perplexity\": self.metrics[\"perplexity\"][0],\n",
    "            \"final_perplexity\": self.metrics[\"perplexity\"][-1],\n",
    "            \"improvement_percent\": ((self.metrics[\"perplexity\"][0] - self.metrics[\"perplexity\"][-1]) / \n",
    "                                   self.metrics[\"perplexity\"][0] * 100)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll use the Wikitext-2 dataset for fine-tuning and evaluation, which provides real-world text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wikitext():\n",
    "    \"\"\"Download Wikitext dataset if not already present.\"\"\"\n",
    "    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n",
    "    \n",
    "    if not os.path.exists(wikitext_file):\n",
    "        print(\"Downloading Wikitext-2 dataset...\")\n",
    "        try:\n",
    "            # Using HF datasets library\n",
    "            from datasets import load_dataset\n",
    "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "            \n",
    "            # Save validation text\n",
    "            with open(wikitext_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in tqdm(dataset[\"validation\"], desc=\"Saving dataset\"):\n",
    "                    if item[\"text\"].strip():\n",
    "                        f.write(item[\"text\"] + \"\\n\")\n",
    "                        \n",
    "            print(f\"Dataset saved to {wikitext_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading dataset: {e}\")\n",
    "            \n",
    "            # Fallback: download using requests\n",
    "            try:\n",
    "                import requests\n",
    "                url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\"\n",
    "                r = requests.get(url)\n",
    "                \n",
    "                # Save zip file\n",
    "                zip_path = os.path.join(DATA_DIR, \"wikitext-2-raw-v1.zip\")\n",
    "                with open(zip_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                \n",
    "                # Extract\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_DIR)\n",
    "                \n",
    "                print(f\"Dataset downloaded and extracted to {DATA_DIR}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Fallback download also failed: {e2}\")\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def prepare_dataset(paragraphs, tokenizer, max_length, batch_size):\n",
    "    \"\"\"Tokenize and prepare paragraphs into a PyTorch dataset.\"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        paragraphs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def load_wikitext_data(tokenizer, max_length=512, batch_size=4):\n",
    "    \"\"\"Load and prepare Wikitext data for fine-tuning and evaluation.\"\"\"\n",
    "    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n",
    "    \n",
    "    if not os.path.exists(wikitext_file):\n",
    "        success = download_wikitext()\n",
    "        if not success:\n",
    "            print(\"Failed to download dataset\")\n",
    "            return None, None\n",
    "    \n",
    "    # Read the data\n",
    "    print(\"Loading Wikitext-2 data...\")\n",
    "    with open(wikitext_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split into train and validation (80/20)\n",
    "    paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    \n",
    "    # Ensure we have at least 100 paragraphs of reasonable length\n",
    "    paragraphs = [p for p in paragraphs if len(p) > 100]\n",
    "    \n",
    "    if len(paragraphs) < 100:\n",
    "        # Fall back to splitting by newline if needed\n",
    "        paragraphs = [p for p in text.split(\"\\n\") if len(p.strip()) > 100]\n",
    "    \n",
    "    # Shuffle and split\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(paragraphs)\n",
    "    \n",
    "    split_idx = int(len(paragraphs) * 0.8)\n",
    "    train_paragraphs = paragraphs[:split_idx]\n",
    "    val_paragraphs = paragraphs[split_idx:]\n",
    "    \n",
    "    print(f\"Tokenizing {len(train_paragraphs)} training and {len(val_paragraphs)} validation paragraphs...\")\n",
    "    \n",
    "    # Tokenize and prepare datasets\n",
    "    train_data = prepare_dataset(train_paragraphs, tokenizer, max_length, batch_size)\n",
    "    val_data = prepare_dataset(val_paragraphs, tokenizer, max_length, batch_size)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading and Analysis\n",
    "\n",
    "We'll load a pre-trained GPT-2 model and add functionality to analyze its attention heads."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def load_model_and_tokenizer(model_name, cache_dir=None):\n    \"\"\"Load pre-trained model and tokenizer.\"\"\"\n    print(f\"Loading model: {model_name}\")\n    \n    # Determine model type from name\n    if \"gpt2\" in model_name.lower():\n        model_type = \"gpt2\"\n    elif \"opt\" in model_name.lower() or \"facebook\" in model_name.lower():\n        model_type = \"opt\"\n    elif \"pythia\" in model_name.lower() or \"eleutherai\" in model_name.lower():\n        model_type = \"pythia\"\n    else:\n        model_type = \"gpt2\"  # Default to gpt2\n        \n    print(f\"Detected model type: {model_type}\")\n    \n    # Load tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n    \n    # Ensure padding token is set\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Load model with potential FP16 optimization\n    if USE_FP16:\n        print(\"Using FP16 for model loading\")\n        # For FP16, we need to set torch_dtype\n        model = GPT2LMHeadModel.from_pretrained(\n            model_name, \n            cache_dir=cache_dir,\n            torch_dtype=torch.float16\n        )\n    else:\n        model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)\n    \n    model.to(DEVICE)\n    \n    # Store model type for later use\n    model.model_type = model_type\n    \n    # Print model size information\n    param_count = sum(p.numel() for p in model.parameters())\n    print(f\"Model loaded with {param_count/1e6:.2f}M parameters\")\n    \n    return model, tokenizer\n\ndef get_attention_modules(model):\n    \"\"\"Extract attention modules from model.\"\"\"\n    # Set default model type if not already set\n    if not hasattr(model, \"model_type\"):\n        model.model_type = \"gpt2\"\n    \n    attention_modules = []\n    \n    # GPT-2 style models\n    if model.model_type == \"gpt2\" and hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):\n        blocks = model.transformer.h\n        \n        for i, block in enumerate(blocks):\n            if hasattr(block, \"attn\"):\n                attention_modules.append((i, block.attn))\n    \n    # OPT style models\n    elif model.model_type == \"opt\" and hasattr(model, \"model\") and hasattr(model.model, \"decoder\"):\n        blocks = model.model.decoder.layers\n        \n        for i, block in enumerate(blocks):\n            if hasattr(block, \"self_attn\"):\n                attention_modules.append((i, block.self_attn))\n    \n    # Pythia style models (similar to GPT-2)\n    elif model.model_type == \"pythia\" and hasattr(model, \"transformer\") and hasattr(model.transformer, \"h\"):\n        blocks = model.transformer.h\n        \n        for i, block in enumerate(blocks):\n            if hasattr(block, \"attn\"):\n                attention_modules.append((i, block.attn))\n    \n    # Not a supported model\n    if not attention_modules:\n        print(\"Warning: Could not find attention modules. Unsupported model architecture.\")\n        \n    return attention_modules"
  },
  {
   "cell_type": "markdown",
   "source": "# Important Note for v0.0.31 \n\nThis version fixes an issue with the `get_strategy()` function that was causing errors with some models (particularly Pythia models). If you encounter an error like:\n\n```\nTypeError: get_strategy() takes 2 positional arguments but 3 were given\n```\n\nThe issue has been fixed in this notebook version by:\n\n1. Adding proper model-type detection\n2. Improving error handling in the importance calculation\n3. Adding fallback strategies when a method fails\n4. Supporting Pythia models specifically\n\nFor larger models like `EleutherAI/pythia-1b`, consider using the smaller `EleutherAI/pythia-70m` model to avoid memory issues.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Importance Calculation\n",
    "\n",
    "We'll implement different strategies for determining which attention heads are important."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "def get_head_importances(model, val_dataloader, strategy=\"entropy\"):\n    \"\"\"\n    Calculate importance scores for each attention head.\n    \n    Args:\n        model: The model to analyze\n        val_dataloader: Validation data for computing metrics\n        strategy: Pruning strategy ('entropy', 'magnitude', 'random')\n        \n    Returns:\n        List of (layer_idx, head_idx, importance) tuples\n    \"\"\"\n    print(f\"Calculating head importances using {strategy} strategy...\")\n    attention_modules = get_attention_modules(model)\n    head_importances = []\n    \n    # Set default model type if not already set\n    if not hasattr(model, \"model_type\"):\n        model.model_type = \"gpt2\"\n    \n    if strategy == \"random\":\n        # For random strategy, just assign random importances\n        for layer_idx, attn in attention_modules:\n            # Get number of heads based on model type\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Try to infer from model name\n                if model.model_type == \"gpt2\":\n                    num_heads = 12  # Default for GPT-2\n                elif model.model_type == \"opt\":\n                    num_heads = 12  # Default for smaller OPT\n                elif model.model_type == \"pythia\":\n                    num_heads = 12  # Default for smaller Pythia\n                else:\n                    num_heads = 12  # fallback\n                print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n                    \n            for head_idx in range(num_heads):\n                importance = np.random.random()\n                head_importances.append((layer_idx, head_idx, importance))\n    \n    elif strategy == \"magnitude\":\n        # For magnitude strategy, use the L2 norm of the head weights\n        for layer_idx, attn in attention_modules:\n            # Determine number of heads\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Model-specific defaults\n                if model.model_type == \"gpt2\":\n                    num_heads = 12\n                elif model.model_type == \"opt\":\n                    num_heads = 12\n                elif model.model_type == \"pythia\":\n                    num_heads = 12\n                else:\n                    num_heads = 12\n                print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n            \n            # Get the appropriate projection weights based on model type\n            if model.model_type == \"gpt2\":\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            elif model.model_type == \"opt\":\n                if hasattr(attn, \"out_proj\") and hasattr(attn, \"out_proj\"):\n                    q_weight = attn.q_proj.weight\n                    head_size = q_weight.shape[0] // num_heads\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            elif model.model_type == \"pythia\":\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size  \n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n            else:\n                # Default to GPT-2 pattern\n                if hasattr(attn, \"c_attn\") and hasattr(attn, \"head_size\"):\n                    q_weight = attn.c_attn.weight\n                    head_size = attn.head_size\n                else:\n                    print(f\"Warning: Layer {layer_idx} doesn't have expected attributes\")\n                    continue\n                \n            # Compute importance for each head\n            for head_idx in range(num_heads):\n                try:\n                    # Get weights for this head\n                    start_idx = head_idx * head_size\n                    end_idx = (head_idx + 1) * head_size\n                    \n                    # Extract weights for Q, K, V for this head - GPT2-specific\n                    if model.model_type == \"gpt2\" or model.model_type == \"pythia\":\n                        q_head = q_weight[:, start_idx:end_idx]\n                        k_head = q_weight[:, num_heads*head_size + start_idx:num_heads*head_size + end_idx]\n                        v_head = q_weight[:, 2*num_heads*head_size + start_idx:2*num_heads*head_size + end_idx]\n                    elif model.model_type == \"opt\":\n                        # For OPT, we need to get separate Q, K, V projections\n                        q_head = attn.q_proj.weight[start_idx:end_idx, :]\n                        k_head = attn.k_proj.weight[start_idx:end_idx, :]\n                        v_head = attn.v_proj.weight[start_idx:end_idx, :]\n                    else:\n                        # Fallback to GPT2 pattern\n                        q_head = q_weight[:, start_idx:end_idx]\n                        k_head = q_weight[:, num_heads*head_size + start_idx:num_heads*head_size + end_idx]\n                        v_head = q_weight[:, 2*num_heads*head_size + start_idx:2*num_heads*head_size + end_idx]\n                    \n                    # Compute L2 norm (magnitude)\n                    q_norm = torch.norm(q_head).item()\n                    k_norm = torch.norm(k_head).item()\n                    v_norm = torch.norm(v_head).item()\n                    \n                    # Use average of Q, K, V norms as importance\n                    importance = (q_norm + k_norm + v_norm) / 3\n                    head_importances.append((layer_idx, head_idx, importance))\n                except Exception as e:\n                    print(f\"Error processing head {head_idx} in layer {layer_idx}: {e}\")\n                    # Assign random importance as fallback\n                    importance = np.random.random()\n                    head_importances.append((layer_idx, head_idx, importance))\n    \n    elif strategy == \"entropy\":\n        # For entropy strategy, measure attention entropy on validation data\n        model.eval()\n        \n        # Store attention outputs\n        attention_outputs = {}\n        \n        # Register hooks to capture attention\n        handles = []\n        \n        def get_attention_hook(layer_idx):\n            def hook(module, input, output):\n                # Shape is usually [batch, num_heads, seq_len, seq_len]\n                # But format can differ by model type\n                if isinstance(output, tuple) and len(output) > 1 and isinstance(output[1], torch.Tensor):\n                    attention_outputs[layer_idx] = output[1].detach()\n                elif isinstance(output, torch.Tensor):\n                    # Some models directly return attention weights\n                    attention_outputs[layer_idx] = output.detach()\n            return hook\n        \n        # Register hooks for each attention module\n        for layer_idx, attn in attention_modules:\n            handles.append(attn.register_forward_hook(get_attention_hook(layer_idx)))\n        \n        # Run a few batches to collect attention patterns\n        with torch.no_grad():\n            for batch_idx, (input_ids, attention_mask) in enumerate(val_dataloader):\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                \n                # Forward pass to trigger hooks\n                try:\n                    model(input_ids=input_ids, attention_mask=attention_mask)\n                except Exception as e:\n                    print(f\"Error during forward pass: {e}\")\n                    continue\n                \n                if batch_idx >= 5:  # Collect data from 5 batches\n                    break\n        \n        # Remove hooks\n        for handle in handles:\n            handle.remove()\n        \n        if not attention_outputs:\n            print(\"Warning: No attention outputs captured. Falling back to magnitude strategy.\")\n            return get_head_importances(model, val_dataloader, strategy=\"magnitude\")\n        \n        # Calculate entropy for each head\n        for layer_idx, attn in attention_modules:\n            if layer_idx not in attention_outputs:\n                continue\n                \n            attn_outputs = attention_outputs[layer_idx]\n            \n            # Determine number of heads\n            if hasattr(attn, \"num_heads\"):\n                num_heads = attn.num_heads\n            elif hasattr(attn, \"num_attention_heads\"):\n                num_heads = attn.num_attention_heads\n            else:\n                # Try to infer from the output shape\n                if len(attn_outputs.shape) >= 2:\n                    num_heads = attn_outputs.shape[1]\n                else:\n                    # Model-specific defaults\n                    if model.model_type == \"gpt2\":\n                        num_heads = 12\n                    elif model.model_type == \"opt\":\n                        num_heads = 12\n                    elif model.model_type == \"pythia\":\n                        num_heads = 12\n                    else:\n                        num_heads = 12\n                    print(f\"Warning: Could not determine num_heads, using default: {num_heads}\")\n                \n            for head_idx in range(num_heads):\n                try:\n                    # Extract attention weights for this head\n                    if head_idx < attn_outputs.shape[1]:  # Check if index is valid\n                        head_attn = attn_outputs[:, head_idx, :, :]\n                    else:\n                        print(f\"Warning: Head index {head_idx} out of bounds. Skipping.\")\n                        continue\n                    \n                    # Calculate entropy (we want low entropy = focused attention)\n                    entropy = 0\n                    \n                    # Process each item in the batch\n                    for item_idx in range(head_attn.size(0)):\n                        item_attn = head_attn[item_idx]\n                        \n                        # Calculate entropy along the attention dimension\n                        # Add small epsilon to avoid log(0)\n                        eps = 1e-10\n                        item_entropy = -torch.sum(item_attn * torch.log(item_attn + eps), dim=-1)\n                        entropy += torch.mean(item_entropy).item()\n                    \n                    # Average entropy across batch\n                    entropy /= head_attn.size(0)\n                    \n                    # Negated entropy, so that higher values = more important (focused attention)\n                    importance = -entropy\n                    head_importances.append((layer_idx, head_idx, importance))\n                    \n                except Exception as e:\n                    print(f\"Error calculating entropy for head {head_idx} in layer {layer_idx}: {e}\")\n                    # Fall back to random importance\n                    importance = np.random.random()\n                    head_importances.append((layer_idx, head_idx, importance))\n    \n    else:\n        raise ValueError(f\"Unknown strategy: {strategy}\")\n    \n    # If no head importances were calculated, fall back to random\n    if not head_importances:\n        print(\"Warning: No head importances calculated. Falling back to random strategy.\")\n        return get_head_importances(model, val_dataloader, strategy=\"random\")\n    \n    # Sort by importance (ascending order, so lowest importance first)\n    head_importances.sort(key=lambda x: x[2])\n    \n    return head_importances"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Implementation\n",
    "\n",
    "Now we'll implement a function to prune the attention heads based on calculated importance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_heads(model, head_importances, pruning_level=0.3):\n",
    "    \"\"\"\n",
    "    Prune specified fraction of attention heads.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to prune\n",
    "        head_importances: List of (layer_idx, head_idx, importance) tuples\n",
    "        pruning_level: Fraction of heads to prune (0.0 to 1.0)\n",
    "        \n",
    "    Returns:\n",
    "        List of pruned heads as (layer_idx, head_idx) tuples\n",
    "    \"\"\"\n",
    "    attention_modules = get_attention_modules(model)\n",
    "    \n",
    "    # Count total heads\n",
    "    total_heads = sum(attn.num_heads for _, attn in attention_modules)\n",
    "    \n",
    "    # Calculate how many heads to prune\n",
    "    num_to_prune = int(total_heads * pruning_level)\n",
    "    \n",
    "    # Get heads to prune (lowest importance first)\n",
    "    heads_to_prune = [(layer_idx, head_idx) for layer_idx, head_idx, _ in head_importances[:num_to_prune]]\n",
    "    \n",
    "    print(f\"Pruning {len(heads_to_prune)}/{total_heads} attention heads ({pruning_level:.1%})\")\n",
    "    \n",
    "    # Create/initialize gates if they don't exist\n",
    "    for layer_idx, attn in attention_modules:\n",
    "        num_heads = attn.num_heads\n",
    "        \n",
    "        # Check if gate exists\n",
    "        if not hasattr(attn, \"head_gates\"):\n",
    "            # Create gates with default value 1.0\n",
    "            attn.head_gates = torch.ones(num_heads, device=DEVICE)\n",
    "    \n",
    "    # Apply pruning by setting gates to 0\n",
    "    for layer_idx, head_idx in heads_to_prune:\n",
    "        for i, (module_layer_idx, attn) in enumerate(attention_modules):\n",
    "            if module_layer_idx == layer_idx:\n",
    "                attn.head_gates[head_idx] = 0.0\n",
    "                break\n",
    "    \n",
    "    # Modify forward pass to use gates\n",
    "    for layer_idx, attn in attention_modules:\n",
    "        # Save original method if not already saved\n",
    "        if not hasattr(attn, \"original_forward\"):\n",
    "            attn.original_forward = attn.forward\n",
    "            \n",
    "            # Create gated forward method\n",
    "            def make_gated_forward(original_forward, head_gates):\n",
    "                def gated_forward(self, *args, **kwargs):\n",
    "                    # Call original forward\n",
    "                    outputs = original_forward(*args, **kwargs)\n",
    "                    \n",
    "                    # Apply gates to attention outputs\n",
    "                    # outputs[0] = result, outputs[1] = attention weights\n",
    "                    if len(outputs) > 1 and isinstance(outputs[1], torch.Tensor):\n",
    "                        # outputs[1] shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "                        gates = head_gates.view(1, -1, 1, 1)\n",
    "                        gated_attention = outputs[1] * gates\n",
    "                        \n",
    "                        return (outputs[0], gated_attention) + outputs[2:] if len(outputs) > 2 else (outputs[0], gated_attention)\n",
    "                    \n",
    "                    return outputs\n",
    "                \n",
    "                return gated_forward\n",
    "            \n",
    "            # Set new forward method\n",
    "            attn.forward = make_gated_forward(attn.original_forward, attn.head_gates).__get__(attn, type(attn))\n",
    "    \n",
    "    return heads_to_prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7):\n    \"\"\"Generate text from a prompt using the model.\"\"\"\n    model.eval()\n    \n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n    \n    try:\n        with torch.no_grad():\n            # First try with all options enabled\n            output = model.generate(\n                input_ids,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=tokenizer.eos_token_id,\n                num_return_sequences=1\n            )\n        \n        # Decode the output\n        return tokenizer.decode(output[0], skip_special_tokens=True)\n    \n    except RuntimeError as e:\n        if \"CUDA\" in str(e):\n            print(\"CUDA error during generation, attempting fallback to CPU...\")\n            # Try moving to CPU for generation\n            try:\n                cpu_model = model.cpu()\n                cpu_input_ids = input_ids.cpu()\n                \n                with torch.no_grad():\n                    output = cpu_model.generate(\n                        cpu_input_ids,\n                        max_length=max_length,\n                        temperature=temperature,\n                        top_p=0.9,\n                        do_sample=True,\n                        pad_token_id=tokenizer.eos_token_id,\n                        num_return_sequences=1\n                    )\n                \n                # Move model back to original device\n                model.to(DEVICE)\n                \n                # Decode the output\n                return tokenizer.decode(output[0], skip_special_tokens=True)\n            \n            except Exception as cpu_error:\n                print(f\"CPU fallback also failed: {cpu_error}\")\n                # Try with safer parameters\n                model.to(DEVICE)  # Ensure model is back on the original device\n                try:\n                    with torch.no_grad():\n                        # Try with simpler generation parameters\n                        output = model.generate(\n                            input_ids,\n                            max_length=max_length,\n                            do_sample=False,  # Use greedy decoding\n                            pad_token_id=tokenizer.eos_token_id,\n                            num_return_sequences=1\n                        )\n                    \n                    return tokenizer.decode(output[0], skip_special_tokens=True)\n                except Exception as safe_error:\n                    print(f\"Safe generation also failed: {safe_error}\")\n                    return f\"{prompt} [Error: Text generation failed]\"\n        \n        # For other types of errors, try with safer parameters\n        try:\n            print(f\"Error during generation: {e}, trying with safer parameters...\")\n            with torch.no_grad():\n                # Try with simpler generation parameters\n                output = model.generate(\n                    input_ids,\n                    max_length=max_length,\n                    do_sample=False,  # Use greedy decoding\n                    pad_token_id=tokenizer.eos_token_id\n                )\n            \n            return tokenizer.decode(output[0], skip_special_tokens=True)\n        except Exception as safe_error:\n            print(f\"Safe generation also failed: {safe_error}\")\n            return f\"{prompt} [Error: Text generation failed]\"\n\ndef evaluate_model(model, dataloader):\n    \"\"\"Evaluate model on dataloader and return loss and perplexity.\"\"\"\n    model.eval()\n    total_loss = 0\n    total_batches = 0\n    \n    with torch.no_grad():\n        for input_ids, attention_mask in dataloader:\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            \n            try:\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                \n                loss = outputs.loss\n                total_loss += loss.item()\n                total_batches += 1\n            except RuntimeError as e:\n                if \"CUDA\" in str(e):\n                    print(f\"CUDA error during evaluation, attempting fallback: {e}\")\n                    try:\n                        # Try with CPU\n                        cpu_model = model.cpu()\n                        cpu_input_ids = input_ids.cpu()\n                        cpu_attention_mask = attention_mask.cpu()\n                        \n                        outputs = cpu_model(\n                            input_ids=cpu_input_ids,\n                            attention_mask=cpu_attention_mask,\n                            labels=cpu_input_ids\n                        )\n                        \n                        loss = outputs.loss\n                        total_loss += loss.item()\n                        total_batches += 1\n                        \n                        # Move model back\n                        model.to(DEVICE)\n                    except Exception as cpu_error:\n                        print(f\"CPU fallback failed: {cpu_error}\")\n                        # Skip this batch\n                        continue\n                else:\n                    print(f\"Error during evaluation: {e}\")\n                    # Skip this batch\n                    continue\n    \n    # Calculate average loss and perplexity\n    if total_batches == 0:\n        return 999.0, 999.0  # Return high values if all batches failed\n        \n    avg_loss = total_loss / total_batches\n    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n    \n    return avg_loss, perplexity\n\ndef get_gate_values(model):\n    \"\"\"Extract gate values from model for visualization.\"\"\"\n    attention_modules = get_attention_modules(model)\n    \n    gate_values = {}\n    for layer_idx, attn in attention_modules:\n        if hasattr(attn, \"head_gates\"):\n            gate_values[f\"layer_{layer_idx}\"] = attn.head_gates.detach().cpu().numpy()\n    \n    return gate_values",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model on dataloader and return loss and perplexity.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in dataloader:\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "    \n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=50, temperature=0.7):\n",
    "    \"\"\"Generate text from a prompt using the model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    \n",
    "    # Decode the output\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def get_gate_values(model):\n",
    "    \"\"\"Extract gate values from model for visualization.\"\"\"\n",
    "    attention_modules = get_attention_modules(model)\n",
    "    \n",
    "    gate_values = {}\n",
    "    for layer_idx, attn in attention_modules:\n",
    "        if hasattr(attn, \"head_gates\"):\n",
    "            gate_values[f\"layer_{layer_idx}\"] = attn.head_gates.detach().cpu().numpy()\n",
    "    \n",
    "    return gate_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "def fine_tune_model(model, train_dataloader, val_dataloader, tokenizer, \n                   learning_rate=5e-5, num_epochs=3, progress_tracker=None):\n    \"\"\"\n    Fine-tune model after pruning.\n    \n    Args:\n        model: The model to fine-tune\n        train_dataloader: Training data\n        val_dataloader: Validation data\n        tokenizer: Tokenizer\n        learning_rate: Learning rate for optimization\n        num_epochs: Number of training epochs\n        progress_tracker: ProgressMetrics object for tracking progress\n        \n    Returns:\n        Dictionary with training results\n    \"\"\"\n    model.train()\n    \n    # Prepare optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n    \n    # Calculate total steps and prepare scheduler\n    total_steps = len(train_dataloader) * num_epochs\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=int(0.1 * total_steps),\n        num_training_steps=total_steps\n    )\n    \n    # Train the model\n    step = 0\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        # Training loop\n        model.train()\n        epoch_losses = []\n        \n        # Create progress bar\n        progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}\")\n        \n        for batch_idx, (input_ids, attention_mask) in enumerate(progress_bar):\n            # Prepare data\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=input_ids\n            )\n            \n            loss = outputs.loss\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            scheduler.step()\n            \n            # Track metrics\n            loss_val = loss.item()\n            epoch_losses.append(loss_val)\n            perplexity = torch.exp(torch.tensor(loss_val)).item()\n            \n            progress_bar.set_postfix(loss=f\"{loss_val:.4f}\", ppl=f\"{perplexity:.2f}\")\n            \n            # Generate sample text every 50 steps with safe error handling\n            if step % 50 == 0:\n                try:\n                    sample_text = generate_text(model, tokenizer, prompt=\"A large language model is\")\n                    \n                    if progress_tracker:\n                        # Get gate values for visualization\n                        gate_values = get_gate_values(model)\n                        \n                        progress_tracker.update(\n                            step=step,\n                            loss=loss_val,\n                            perplexity=perplexity,\n                            gate_values=gate_values,\n                            generation_sample=sample_text\n                        )\n                except Exception as e:\n                    print(f\"Error during text generation: {e}\")\n                    # Provide a fallback sample text\n                    sample_text = \"[Text generation failed - continuing training]\"\n                    \n                    if progress_tracker:\n                        # Still update metrics without the generation sample\n                        gate_values = get_gate_values(model)\n                        progress_tracker.update(\n                            step=step,\n                            loss=loss_val,\n                            perplexity=perplexity,\n                            gate_values=gate_values\n                        )\n            \n            step += 1\n        \n        # Evaluate after each epoch\n        eval_loss, eval_ppl = evaluate_model(model, val_dataloader)\n        \n        # Print epoch summary\n        epoch_loss = sum(epoch_losses) / len(epoch_losses)\n        epoch_ppl = torch.exp(torch.tensor(epoch_loss)).item()\n        \n        print(f\"Epoch {epoch+1} summary:\")\n        print(f\"  Train loss: {epoch_loss:.4f}, perplexity: {epoch_ppl:.2f}\")\n        print(f\"  Val loss: {eval_loss:.4f}, perplexity: {eval_ppl:.2f}\")\n        \n        # Track validation metrics\n        if progress_tracker:\n            # Safe generation with error handling\n            try:\n                sample_text = generate_text(model, tokenizer, prompt=\"In recent years, artificial intelligence has\")\n                progress_tracker.update(\n                    step=step,\n                    loss=eval_loss,\n                    perplexity=eval_ppl,\n                    generation_sample=sample_text\n                )\n            except Exception as e:\n                print(f\"Error during evaluation text generation: {e}\")\n                progress_tracker.update(\n                    step=step,\n                    loss=eval_loss,\n                    perplexity=eval_ppl\n                )\n    \n    # Final evaluation\n    final_loss, final_ppl = evaluate_model(model, val_dataloader)\n    print(f\"Final evaluation - Loss: {final_loss:.4f}, Perplexity: {final_ppl:.2f}\")\n    \n    return {\n        \"final_loss\": final_loss,\n        \"final_perplexity\": final_ppl,\n        \"steps\": step\n    }",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_dataloader, val_dataloader, tokenizer, \n",
    "                   learning_rate=5e-5, num_epochs=3, progress_tracker=None):\n",
    "    \"\"\"\n",
    "    Fine-tune model after pruning.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to fine-tune\n",
    "        train_dataloader: Training data\n",
    "        val_dataloader: Validation data\n",
    "        tokenizer: Tokenizer\n",
    "        learning_rate: Learning rate for optimization\n",
    "        num_epochs: Number of training epochs\n",
    "        progress_tracker: ProgressMetrics object for tracking progress\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with training results\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Prepare optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Calculate total steps and prepare scheduler\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps),\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training loop\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}\")\n",
    "        \n",
    "        for batch_idx, (input_ids, attention_mask) in enumerate(progress_bar):\n",
    "            # Prepare data\n",
    "            input_ids = input_ids.to(DEVICE)\n",
    "            attention_mask = attention_mask.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=input_ids\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track metrics\n",
    "            loss_val = loss.item()\n",
    "            epoch_losses.append(loss_val)\n",
    "            perplexity = torch.exp(torch.tensor(loss_val)).item()\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{loss_val:.4f}\", ppl=f\"{perplexity:.2f}\")\n",
    "            \n",
    "            # Generate sample text every 50 steps\n",
    "            if step % 50 == 0:\n",
    "                sample_text = generate_text(model, tokenizer, prompt=\"A large language model is\")\n",
    "                \n",
    "                if progress_tracker:\n",
    "                    # Get gate values for visualization\n",
    "                    gate_values = get_gate_values(model)\n",
    "                    \n",
    "                    progress_tracker.update(\n",
    "                        step=step,\n",
    "                        loss=loss_val,\n",
    "                        perplexity=perplexity,\n",
    "                        gate_values=gate_values,\n",
    "                        generation_sample=sample_text\n",
    "                    )\n",
    "            \n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch\n",
    "        eval_loss, eval_ppl = evaluate_model(model, val_dataloader)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        epoch_ppl = torch.exp(torch.tensor(epoch_loss)).item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} summary:\")\n",
    "        print(f\"  Train loss: {epoch_loss:.4f}, perplexity: {epoch_ppl:.2f}\")\n",
    "        print(f\"  Val loss: {eval_loss:.4f}, perplexity: {eval_ppl:.2f}\")\n",
    "        \n",
    "        # Track validation metrics\n",
    "        if progress_tracker:\n",
    "            progress_tracker.update(\n",
    "                step=step,\n",
    "                loss=eval_loss,\n",
    "                perplexity=eval_ppl,\n",
    "                generation_sample=generate_text(model, tokenizer, \n",
    "                                               prompt=\"In recent years, artificial intelligence has\")\n",
    "            )\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_loss, final_ppl = evaluate_model(model, val_dataloader)\n",
    "    print(f\"Final evaluation - Loss: {final_loss:.4f}, Perplexity: {final_ppl:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"final_loss\": final_loss,\n",
    "        \"final_perplexity\": final_ppl,\n",
    "        \"steps\": step\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions\n",
    "\n",
    "Let's implement visualization for head importance and gate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head_importance_visualization(head_importances, pruned_heads, output_path=None):\n",
    "    \"\"\"Create visualization of head importances and pruned heads.\"\"\"\n",
    "    # Organize by layer\n",
    "    layers = {}\n",
    "    for layer_idx, head_idx, importance in head_importances:\n",
    "        if layer_idx not in layers:\n",
    "            layers[layer_idx] = []\n",
    "        layers[layer_idx].append((head_idx, importance))\n",
    "    \n",
    "    # Convert pruned_heads to a set for faster lookup\n",
    "    pruned_set = set((layer, head) for layer, head in pruned_heads)\n",
    "    \n",
    "    # Create figure\n",
    "    num_layers = len(layers)\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, num_layers)))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    layer_labels = []\n",
    "    head_importance_data = []\n",
    "    colors = []\n",
    "    \n",
    "    for layer_idx in sorted(layers.keys()):\n",
    "        heads = layers[layer_idx]\n",
    "        \n",
    "        for head_idx, importance in sorted(heads, key=lambda x: x[0]):\n",
    "            layer_labels.append(f\"L{layer_idx}-H{head_idx}\")\n",
    "            head_importance_data.append(importance)\n",
    "            \n",
    "            # Red for pruned, blue for kept\n",
    "            colors.append('red' if (layer_idx, head_idx) in pruned_set else 'blue')\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    y_pos = np.arange(len(layer_labels))\n",
    "    ax.barh(y_pos, head_importance_data, color=colors)\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(layer_labels)\n",
    "    ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "    ax.set_xlabel('Importance Score')\n",
    "    ax.set_title('Attention Head Importance (red = pruned)')\n",
    "    \n",
    "    # Save figure if path provided\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_gate_values(gate_values, output_path=None):\n",
    "    \"\"\"Create visualization of gate values across layers.\"\"\"\n",
    "    if not gate_values:\n",
    "        return None\n",
    "    \n",
    "    # Prepare data\n",
    "    layers = sorted(gate_values.keys())\n",
    "    data = [gate_values[layer] for layer in layers]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot heatmap-like visualization\n",
    "    for i, (layer, values) in enumerate(zip(layers, data)):\n",
    "        # Create scatter plot for each layer\n",
    "        x = np.arange(len(values))\n",
    "        y = np.ones_like(x) * i\n",
    "        \n",
    "        # Use values to determine color and size\n",
    "        colors = ['red' if v < 0.01 else 'blue' for v in values]\n",
    "        sizes = [10 + 40 * v for v in values]\n",
    "        \n",
    "        ax.scatter(x, y, c=colors, s=sizes, alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_yticks(np.arange(len(layers)))\n",
    "    ax.set_yticklabels([layer.replace('layer_', 'Layer ') for layer in layers])\n",
    "    ax.set_xlabel('Head Index')\n",
    "    ax.set_ylabel('Layer')\n",
    "    ax.set_title('Attention Head Gate Values (Red = Pruned)')\n",
    "    \n",
    "    # Add colorbar legend\n",
    "    import matplotlib.patches as mpatches\n",
    "    red_patch = mpatches.Patch(color='red', label='Pruned (gate  0)')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Active (gate = 1)')\n",
    "    ax.legend(handles=[red_patch, blue_patch], loc='upper right')\n",
    "    \n",
    "    # Save figure if path provided\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment\n",
    "\n",
    "Now we'll run the complete experiment to make a GPT-2 model smaller and more powerful."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Configure experiment parameters\nmodel_name = \"distilgpt2\"  # Use distilgpt2 for faster experimentation\n\n# Uncomment ONE of the following lines to use an alternative model:\n# model_name = \"facebook/opt-125m\"  # Smaller OPT model\n# model_name = \"EleutherAI/pythia-70m\"  # Smaller Pythia model\n\n# If your error was with the pythia-1b model, use the 70m version for less memory usage:\n# model_name = \"EleutherAI/pythia-70m\"  # Smaller Pythia model that should avoid memory issues\n\nstrategy = \"entropy\"       # Options: \"random\", \"magnitude\", \"entropy\"\npruning_level = 0.3        # Fraction of heads to prune (0.0 to 1.0)\nlearning_rate = 5e-5       # Learning rate for fine-tuning\nnum_epochs = 3             # Number of training epochs\nmax_length = 256           # Maximum sequence length\nbatch_size = 4             # Batch size for training and evaluation\n\n# Reduce batch size if running on CPU or with limited memory\nif DEVICE == \"cpu\":\n    print(\"Running on CPU - reducing batch size and max_length\")\n    batch_size = 2\n    max_length = 128"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for output\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{strategy}_{timestamp}\")\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Initialize progress tracker\n",
    "progress = ProgressMetrics()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "\n",
    "# Load data\n",
    "train_dataloader, val_dataloader = load_wikitext_data(\n",
    "    tokenizer, \n",
    "    max_length=max_length, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial evaluation\n",
    "print(\"Evaluating initial model performance...\")\n",
    "initial_loss, initial_ppl = evaluate_model(model, val_dataloader)\n",
    "print(f\"Initial loss: {initial_loss:.4f}, perplexity: {initial_ppl:.2f}\")\n",
    "\n",
    "# Generate example text\n",
    "initial_generation = generate_text(\n",
    "    model, tokenizer, \n",
    "    prompt=\"Artificial intelligence is becoming increasingly important because\",\n",
    "    max_length=100\n",
    ")\n",
    "print(\"\\nInitial generation example:\")\n",
    "print(initial_generation)\n",
    "\n",
    "# Record initial metrics\n",
    "progress.update(step=0, loss=initial_loss, perplexity=initial_ppl, \n",
    "               generation_sample=initial_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate head importances\n",
    "head_importances = get_head_importances(model, val_dataloader, strategy=strategy)\n",
    "\n",
    "# Prune heads\n",
    "pruned_heads = prune_heads(model, head_importances, pruning_level=pruning_level)\n",
    "\n",
    "# Record pruning information\n",
    "progress.set_pruning_info(strategy, pruning_level, pruned_heads)\n",
    "\n",
    "# Save head importance visualization\n",
    "importance_viz_path = os.path.join(run_dir, \"head_importances.png\")\n",
    "create_head_importance_visualization(\n",
    "    head_importances, pruned_heads, importance_viz_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Fine-tune pruned model\nprint(\"\\nFine-tuning pruned model...\")\ntry:\n    # Clear GPU memory before fine-tuning\n    clear_gpu_memory()\n    \n    # Use the context manager for better memory efficiency\n    with autocast_if_available():\n        fine_tune_results = fine_tune_model(\n            model, \n            train_dataloader, \n            val_dataloader, \n            tokenizer,\n            learning_rate=learning_rate,\n            num_epochs=num_epochs,\n            progress_tracker=progress\n        )\nexcept RuntimeError as e:\n    if \"CUDA\" in str(e):\n        print(f\"\\nCUDA error during fine-tuning: {e}\")\n        print(\"\\nAttempting to continue with CPU...\\n\")\n        # Move to CPU and continue\n        model = model.cpu()\n        DEVICE = \"cpu\"\n        \n        # Try with reduced parameters\n        fine_tune_results = fine_tune_model(\n            model, \n            train_dataloader, \n            val_dataloader, \n            tokenizer,\n            learning_rate=learning_rate,\n            num_epochs=1,  # Reduce to single epoch on CPU\n            progress_tracker=progress\n        )\n    else:\n        print(f\"Error during fine-tuning: {e}\")\n        # Provide minimal output to continue\n        fine_tune_results = {\n            \"final_loss\": pruned_loss,  # Use pruned model metrics\n            \"final_perplexity\": pruned_ppl,\n            \"steps\": 0\n        }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune pruned model\n",
    "print(\"\\nFine-tuning pruned model...\")\n",
    "fine_tune_results = fine_tune_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    tokenizer,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    progress_tracker=progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final examples\n",
    "final_generation = generate_text(\n",
    "    model, tokenizer, \n",
    "    prompt=\"Artificial intelligence is becoming increasingly important because\",\n",
    "    max_length=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"\\nFinal generation example:\")\n",
    "print(final_generation)\n",
    "\n",
    "# Compare results\n",
    "summary = progress.get_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Pruning strategy: {strategy}\")\n",
    "print(f\"Pruning level: {pruning_level:.1%}\")\n",
    "print(f\"Pruned heads: {len(pruned_heads)}\")\n",
    "print(\"\\nPerformance:\")\n",
    "print(f\"  Initial perplexity: {initial_ppl:.2f}\")\n",
    "print(f\"  After pruning: {pruned_ppl:.2f} ({(pruned_ppl-initial_ppl)/initial_ppl*100:+.2f}%)\")\n",
    "print(f\"  After fine-tuning: {fine_tune_results['final_perplexity']:.2f} ({(fine_tune_results['final_perplexity']-initial_ppl)/initial_ppl*100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final plots\n",
    "progress.save_plots(os.path.join(run_dir, \"training_progress.png\"))\n",
    "progress.save_metrics(os.path.join(run_dir, \"metrics.json\"))\n",
    "\n",
    "# Save text samples\n",
    "with open(os.path.join(run_dir, \"text_samples.txt\"), \"w\") as f:\n",
    "    f.write(\"INITIAL MODEL\\n\")\n",
    "    f.write(\"============\\n\")\n",
    "    f.write(initial_generation)\n",
    "    f.write(\"\\n\\nAFTER PRUNING\\n\")\n",
    "    f.write(\"============\\n\")\n",
    "    f.write(pruned_generation)\n",
    "    f.write(\"\\n\\nAFTER FINE-TUNING\\n\")\n",
    "    f.write(\"===============\\n\")\n",
    "    f.write(final_generation)\n",
    "\n",
    "print(f\"\\nAll results and visualizations saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended Experiment: Try Different Strategies and Pruning Levels\n",
    "\n",
    "This cell lets you experiment with different pruning strategies and levels to find the optimal configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_experiment(model_name, strategy, pruning_level, epochs=3):\n",
    "    # Create experiment directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"{model_name.replace('/', '_')}_{strategy}_{pruning_level:.1f}_{timestamp}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize progress tracker\n",
    "    progress = ProgressMetrics()\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "    \n",
    "    # Load data (reuse existing dataloaders from previous cells)\n",
    "    train_dataloader, val_dataloader = load_wikitext_data(\n",
    "        tokenizer, \n",
    "        max_length=max_length, \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Initial evaluation\n",
    "    print(f\"\\nRunning experiment with {strategy} strategy at {pruning_level:.1%} pruning level\")\n",
    "    print(\"Evaluating initial model...\")\n",
    "    initial_loss, initial_ppl = evaluate_model(model, val_dataloader)\n",
    "    progress.update(step=0, loss=initial_loss, perplexity=initial_ppl)\n",
    "    \n",
    "    # Calculate head importances and prune\n",
    "    print(\"Calculating head importances...\")\n",
    "    head_importances = get_head_importances(model, val_dataloader, strategy=strategy)\n",
    "    \n",
    "    print(\"Applying pruning...\")\n",
    "    pruned_heads = prune_heads(model, head_importances, pruning_level=pruning_level)\n",
    "    progress.set_pruning_info(strategy, pruning_level, pruned_heads)\n",
    "    \n",
    "    # Evaluate pruned model\n",
    "    pruned_loss, pruned_ppl = evaluate_model(model, val_dataloader)\n",
    "    progress.update(step=1, loss=pruned_loss, perplexity=pruned_ppl)\n",
    "    \n",
    "    # Fine-tune\n",
    "    print(\"Fine-tuning...\")\n",
    "    results = fine_tune_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        tokenizer,\n",
    "        num_epochs=epochs,\n",
    "        progress_tracker=progress\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    progress.save_plots(os.path.join(run_dir, \"training_progress.png\"))\n",
    "    progress.save_metrics(os.path.join(run_dir, \"metrics.json\"))\n",
    "    \n",
    "    # Return summary\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"strategy\": strategy,\n",
    "        \"pruning_level\": pruning_level,\n",
    "        \"initial_ppl\": initial_ppl,\n",
    "        \"pruned_ppl\": pruned_ppl,\n",
    "        \"final_ppl\": results[\"final_perplexity\"],\n",
    "        \"pruning_impact\": (pruned_ppl - initial_ppl) / initial_ppl * 100,\n",
    "        \"final_improvement\": (results[\"final_perplexity\"] - initial_ppl) / initial_ppl * 100\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run experiments with different strategies and pruning levels\n",
    "# This can take a while to run\n",
    "\n",
    "# strategies = [\"random\", \"magnitude\", \"entropy\"]\n",
    "# pruning_levels = [0.1, 0.3, 0.5]\n",
    "# results = []\n",
    "\n",
    "# for strategy in strategies:\n",
    "#     for level in pruning_levels:\n",
    "#         result = run_complete_experiment(\"distilgpt2\", strategy, level, epochs=2)\n",
    "#         results.append(result)\n",
    "\n",
    "# # Display results in a table\n",
    "# import pandas as pd\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Download results\nif IS_COLAB and run_dir:\n    print(f\"Experiment results are saved to: {run_dir}\")\n    result_files = [\n        os.path.join(run_dir, \"metrics.json\"),\n        os.path.join(run_dir, \"training_progress.png\"),\n        os.path.join(run_dir, \"head_importances.png\"),\n        os.path.join(run_dir, \"gate_values.png\"),\n        os.path.join(run_dir, \"text_samples.txt\")\n    ]\n    download_files(result_files)\nelse:\n    print(\"Results download is only available in Google Colab or after running an experiment.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how we can make a GPT-2 model both smaller and more powerful by:\n",
    "\n",
    "1. **Identifying and pruning less important attention heads** using entropy-based, magnitude-based, or random pruning strategies\n",
    "2. **Fine-tuning the pruned model** to recover and potentially improve performance\n",
    "3. **Measuring and visualizing the model's improvement** throughout the process\n",
    "\n",
    "Key findings:\n",
    "- Pruning reduces model size and can improve inference speed\n",
    "- Fine-tuning after pruning can recover and sometimes improve model performance\n",
    "- The entropy strategy typically produces the best results by identifying truly unimportant heads\n",
    "- The optimal pruning level is typically around 30%, balancing size reduction and quality\n",
    "\n",
    "This approach provides a practical way to optimize transformer-based language models, making them more efficient while maintaining or improving performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}