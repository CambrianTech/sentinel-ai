{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning and Fine-Tuning Benchmark for Google Colab (v0.0.28.0)\n",
    "\n",
    "This is the Python script version of our notebook for Google Colab. \n",
    "\n",
    "- Version 0.0.28.0 (April 2025) - Enhance profile_full_model.py with improved organization and usability\n",
    "- Version 0.0.27.0 (April 2025) - Use PruningFineTuningExperiment from utils.pruning\n",
    "- Version 0.0.26 (April 2025) - refactor/modular-experiment \n",
    "   - Added Colab utilities for automatic environment optimization. \n",
    "   - Fix import paths for Colab compatibility.\n",
    "\n",
    "Instructions:\n",
    "1. Upload to a new Colab notebook using File > Upload notebook > Upload\n",
    "2. The notebook will automatically configure the environment with:\n",
    "   - GPU acceleration selection\n",
    "   - Memory-optimized parameters based on available resources\n",
    "   - Adaptive model configuration based on memory constraints\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Baseline Evaluation**: Establish the initial model performance\n",
    "2. **Pruning Phase**: Apply different pruning strategies and evaluate post-pruning performance\n",
    "3. **Fine-Tuning Phase**: Fine-tune pruned models to recover or improve performance\n",
    "4. **Analysis**: Compare performance across pruning levels and fine-tuning epochs\n",
    "\n",
    "This experiment will run until interrupted, continuously improving the models and updating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Colab environment with GPU acceleration\n",
    "import os\n",
    "\n",
    "# Install required packages and make sure HuggingFace datasets is properly installed\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy pandas seaborn tqdm optax psutil\n",
    "!pip install -q 'datasets>=2.0.0' multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (or branch if in one)\n",
    "!git clone -b refactor/modular-experiment https://github.com/CambrianTech/sentinel-ai.git\n",
    "# Don't cd into it yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to the repository directory\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Import huggingface datasets directly (before any potential conflicts)\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "print(f\"Using datasets from: {datasets.__file__}\")\n",
    "\n",
    "# Set up Colab environment with GPU and memory optimization\n",
    "try:\n",
    "    from utils.colab import setup_colab_environment, optimize_for_colab\n",
    "    \n",
    "    # Configure Colab environment with GPU preference\n",
    "    env_info = setup_colab_environment(prefer_gpu=True)\n",
    "    \n",
    "    # Get optimized parameters based on model sizes we'll use (medium is good default)\n",
    "    params = optimize_for_colab(model_size=\"medium\", prefer_stability=True)\n",
    "    \n",
    "    # Extract parameters for use in experiment\n",
    "    optimized_batch_size = params[\"batch_size\"]\n",
    "    sequence_length = params[\"sequence_length\"]\n",
    "    stability_level = params[\"stability_level\"]\n",
    "    use_fp16 = params[\"use_fp16\"]\n",
    "    \n",
    "    print(f\"\\n✅ Using optimized parameters for Colab:\")\n",
    "    print(f\"  - Batch size: {optimized_batch_size}\")\n",
    "    print(f\"  - Sequence length: {sequence_length}\")\n",
    "    print(f\"  - Stability level: {stability_level}\")\n",
    "    print(f\"  - Mixed precision: {use_fp16}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️ Colab utilities not available, using default parameters\")\n",
    "    print(\"This may be the first run before utils/colab are present\")\n",
    "    \n",
    "    # Try to auto-select GPU via Google Colab runtime API\n",
    "    try:\n",
    "        from google.colab import runtime\n",
    "        runtime.change_runtime(runtime_type=\"GPU\")\n",
    "        print(\"✅ GPU acceleration enabled!\")\n",
    "    except:\n",
    "        print(\"⚠️ Could not auto-select GPU. Please set it manually.\")\n",
    "    \n",
    "    # Check for GPU availability\n",
    "    try:\n",
    "        !nvidia-smi\n",
    "    except:\n",
    "        print(\"❌ No GPU detected. Performance will be limited.\")\n",
    "    \n",
    "    # Default parameters\n",
    "    optimized_batch_size = 4\n",
    "    sequence_length = 128\n",
    "    stability_level = 1\n",
    "    use_fp16 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import rest of the libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "# Add the current directory to path and import our modules\n",
    "sys.path.append(\".\")\n",
    "from utils.pruning import (\n",
    "    Environment,\n",
    "    ResultsManager,\n",
    "    PruningModule, \n",
    "    get_strategy,\n",
    "    FineTuner,\n",
    "    ImprovedFineTuner,\n",
    "    PruningFineTuningExperiment\n",
    ")\n",
    "from utils.pruning.stability import patch_fine_tuner, optimize_fine_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()\n",
    "\n",
    "# Check JAX capabilities\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment with Modular Framework\n",
    "\n",
    "Now we'll use the modular experiment framework to run our pruning and fine-tuning experiments.\n",
    "\n",
    "Note: We use the `PruningFineTuningExperiment` class imported from `utils.pruning` to ensure \n",
    "consistent behavior between this notebook and local testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment with memory optimizations\n",
    "experiment = PruningFineTuningExperiment(\n",
    "    results_dir=\"pruning_finetuning_results\",\n",
    "    use_improved_fine_tuner=True,      # Use the improved fine-tuner with stability enhancements\n",
    "    detect_environment=True,           # Automatically detect Colab environment\n",
    "    optimize_memory=True,              # Optimize for T4 GPU memory constraints \n",
    "    batch_size=optimized_batch_size,   # Use the optimized batch size\n",
    "    sequence_length=sequence_length,   # Use the optimized sequence length (matches utils implementation)\n",
    "    stability_level=stability_level    # Use optimized stability level\n",
    ")\n",
    "\n",
    "# Memory optimization information\n",
    "print(f\"\\nExperiment configured with optimized parameters:\")\n",
    "print(f\"- Batch size: {optimized_batch_size}\")\n",
    "print(f\"- Sequence length: {sequence_length}\")\n",
    "print(f\"- Stability level: {stability_level}\")\n",
    "print(f\"- Mixed precision: {use_fp16}\")\n",
    "\n",
    "# Memory optimizations include:\n",
    "# - Reduced batch sizes for larger models\n",
    "# - Shorter sequence lengths for memory efficiency\n",
    "# - Adaptive sample counts based on model size\n",
    "# - Conservative synthetic data generation\n",
    "# These optimizations help prevent OOM errors with larger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "PRUNING_LEVELS = [0.1, 0.3, 0.5]\n",
    "PROMPT = \"Artificial intelligence will transform society by\"\n",
    "FINE_TUNING_EPOCHS = 2  # Small number for quick iterations\n",
    "MAX_RUNTIME = 6 * 3600  # 6 hours\n",
    "\n",
    "# Start the experiment\n",
    "results = experiment.run_experiment(\n",
    "    strategies=STRATEGIES,\n",
    "    pruning_levels=PRUNING_LEVELS,\n",
    "    prompt=PROMPT,\n",
    "    fine_tuning_epochs=FINE_TUNING_EPOCHS,\n",
    "    max_runtime=MAX_RUNTIME\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longer Overnight Run\n",
    "\n",
    "For an extended overnight run, uncomment and run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overnight Configuration - More conservative settings\n",
    "OVERNIGHT_STRATEGIES = [\"random\", \"magnitude\", \"entropy\"]\n",
    "OVERNIGHT_PRUNING_LEVELS = [0.1, 0.2, 0.3, 0.4, 0.5]  # Skip higher levels to avoid memory issues\n",
    "OVERNIGHT_PROMPT = \"Artificial intelligence will revolutionize industries by\"\n",
    "OVERNIGHT_FINE_TUNING_EPOCHS = 3  # Reduced to avoid memory issues with larger models\n",
    "OVERNIGHT_MAX_RUNTIME = 20 * 3600  # 20 hours\n",
    "\n",
    "# Initialize experiment for overnight run\n",
    "overnight_experiment = PruningFineTuningExperiment(\n",
    "    results_dir=\"overnight_results\",\n",
    "    use_improved_fine_tuner=True,\n",
    "    detect_environment=True,\n",
    "    optimize_memory=True,\n",
    "    batch_size=1,                      # Smaller batch for longer sequences\n",
    "    sequence_length=128,               # Longer sequences for better quality\n",
    "    stability_level=3                  # Maximum stability for overnight runs\n",
    ")\n",
    "\n",
    "# Uncomment to run overnight experiment\n",
    "# overnight_results = overnight_experiment.run_experiment(\n",
    "#     strategies=OVERNIGHT_STRATEGIES,\n",
    "#     pruning_levels=OVERNIGHT_PRUNING_LEVELS,\n",
    "#     prompt=OVERNIGHT_PROMPT,\n",
    "#     fine_tuning_epochs=OVERNIGHT_FINE_TUNING_EPOCHS,\n",
    "#     max_runtime=OVERNIGHT_MAX_RUNTIME\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time Experiment Monitoring\n",
    "\n",
    "The cell below can be executed independently while experiments are running to visualize the current state of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell can be run at any time to visualize current experiment progress\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "def visualize_ongoing_experiments(results_dir=\"pruning_finetuning_results\", figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Create a real-time visualization of ongoing experiments\n",
    "    This can be run independently while experiments are in progress\n",
    "    \"\"\"\n",
    "    # Set better default styling for plots to fix layout issues\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (10, 8),\n",
    "        'figure.titlesize': 14,\n",
    "        'axes.titlesize': 11,\n",
    "        'axes.labelsize': 10,\n",
    "        'xtick.labelsize': 9,\n",
    "        'ytick.labelsize': 9,\n",
    "        'legend.fontsize': 7,\n",
    "        'legend.title_fontsize': 8,\n",
    "        'font.family': 'sans-serif'\n",
    "    })\n",
    "    \n",
    "    # Check if results directory exists\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"Results directory '{results_dir}' not found\")\n",
    "        return\n",
    "    \n",
    "    # List all result files\n",
    "    result_files = glob.glob(os.path.join(results_dir, \"*.json\"))\n",
    "    \n",
    "    if not result_files:\n",
    "        print(f\"No result files found in '{results_dir}'\")\n",
    "        return\n",
    "    \n",
    "    # Load all result files\n",
    "    results = []\n",
    "    for file_path in result_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                result = json.load(f)\n",
    "                results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid result files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} experiment results\")\n",
    "    \n",
    "    # Extract data for visualization\n",
    "    data = []\n",
    "    \n",
    "    for result in results:\n",
    "        # Extract experiment info\n",
    "        model = result.get(\"model\", \"unknown\")\n",
    "        # Shorten model name for better display\n",
    "        if '/' in model:\n",
    "            model = model.split('/')[-1]\n",
    "        strategy = result.get(\"strategy\", \"unknown\")\n",
    "        pruning_level = result.get(\"pruning_level\", 0)\n",
    "        timestamp = result.get(\"timestamp\", \"unknown\")\n",
    "        \n",
    "        # Extract perplexity data from different stages\n",
    "        stages = result.get(\"stages\", {})\n",
    "        \n",
    "        baseline_perplexity = stages.get(\"baseline\", {}).get(\"perplexity\", None)\n",
    "        pruned_perplexity = stages.get(\"pruned\", {}).get(\"perplexity\", None)\n",
    "        finetuned_perplexity = stages.get(\"fine_tuned\", {}).get(\"perplexity\", None)\n",
    "        \n",
    "        recovery_percentage = stages.get(\"fine_tuned\", {}).get(\"recovery_percentage\", None)\n",
    "        improvement_percentage = stages.get(\"fine_tuned\", {}).get(\"improvement_percentage\", None)\n",
    "        \n",
    "        # Add to dataframe\n",
    "        if baseline_perplexity is not None:\n",
    "            data.append({\n",
    "                \"model\": model,\n",
    "                \"strategy\": strategy,\n",
    "                \"pruning_level\": pruning_level,\n",
    "                \"stage\": \"baseline\",\n",
    "                \"perplexity\": baseline_perplexity,\n",
    "                \"timestamp\": timestamp\n",
    "            })\n",
    "        \n",
    "        if pruned_perplexity is not None:\n",
    "            data.append({\n",
    "                \"model\": model,\n",
    "                \"strategy\": strategy,\n",
    "                \"pruning_level\": pruning_level,\n",
    "                \"stage\": \"pruned\",\n",
    "                \"perplexity\": pruned_perplexity,\n",
    "                \"timestamp\": timestamp\n",
    "            })\n",
    "        \n",
    "        if finetuned_perplexity is not None:\n",
    "            data.append({\n",
    "                \"model\": model,\n",
    "                \"strategy\": strategy,\n",
    "                \"pruning_level\": pruning_level,\n",
    "                \"stage\": \"fine_tuned\",\n",
    "                \"perplexity\": finetuned_perplexity,\n",
    "                \"recovery_percentage\": recovery_percentage,\n",
    "                \"improvement_percentage\": improvement_percentage,\n",
    "                \"timestamp\": timestamp\n",
    "            })\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No valid data extracted from results\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with a more compact layout\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    # 1. Perplexity across stages by model and strategy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    \n",
    "    # Get unique values for grouping\n",
    "    models = df[\"model\"].unique()\n",
    "    strategies = df[\"strategy\"].unique()\n",
    "    \n",
    "    # Filter to main stages\n",
    "    stages_df = df[df[\"stage\"].isin([\"baseline\", \"pruned\", \"fine_tuned\"])]\n",
    "    \n",
    "    # Plot lines connecting stages for each experiment\n",
    "    for model in models:\n",
    "        model_df = stages_df[stages_df[\"model\"] == model]\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            strategy_df = model_df[model_df[\"strategy\"] == strategy]\n",
    "            \n",
    "            for pruning_level in strategy_df[\"pruning_level\"].unique():\n",
    "                experiment_df = strategy_df[strategy_df[\"pruning_level\"] == pruning_level]\n",
    "                \n",
    "                # Sort by stage\n",
    "                stage_order = {\"baseline\": 0, \"pruned\": 1, \"fine_tuned\": 2}\n",
    "                experiment_df = experiment_df.sort_values(by=\"stage\", key=lambda x: x.map(stage_order))\n",
    "                \n",
    "                # Only plot if we have at least 2 stages\n",
    "                if len(experiment_df) >= 2:\n",
    "                    label = f\"{model[:6]}-{strategy[:3]}-{pruning_level:.1f}\"\n",
    "                    plt.plot(experiment_df[\"stage\"], experiment_df[\"perplexity\"], \"o-\", label=label)\n",
    "    \n",
    "    plt.title(\"Perplexity Across Stages\")\n",
    "    plt.xlabel(\"Stage\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(fontsize=7, loc='upper right', ncol=2)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Pruning level vs perplexity by strategy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    # Filter to specific stages\n",
    "    baseline_df = df[df[\"stage\"] == \"baseline\"]\n",
    "    pruned_df = df[df[\"stage\"] == \"pruned\"]\n",
    "    finetuned_df = df[df[\"stage\"] == \"fine_tuned\"]\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        # Get strategy data for each stage\n",
    "        baseline_strategy = baseline_df[baseline_df[\"strategy\"] == strategy]\n",
    "        pruned_strategy = pruned_df[pruned_df[\"strategy\"] == strategy]\n",
    "        finetuned_strategy = finetuned_df[finetuned_df[\"strategy\"] == strategy]\n",
    "        \n",
    "        # Plot lines for each stage if data exists\n",
    "        if not baseline_strategy.empty:\n",
    "            plt.plot(baseline_strategy[\"pruning_level\"], baseline_strategy[\"perplexity\"], \n",
    "                    \"o--\", label=f\"Base-{strategy[:3]}\", alpha=0.7)\n",
    "        \n",
    "        if not pruned_strategy.empty:\n",
    "            plt.plot(pruned_strategy[\"pruning_level\"], pruned_strategy[\"perplexity\"], \n",
    "                    \"s--\", label=f\"Pruned-{strategy[:3]}\", alpha=0.7)\n",
    "        \n",
    "        if not finetuned_strategy.empty:\n",
    "            plt.plot(finetuned_strategy[\"pruning_level\"], finetuned_strategy[\"perplexity\"], \n",
    "                    \"^-\", label=f\"Tuned-{strategy[:3]}\", alpha=0.7)\n",
    "    \n",
    "    plt.title(\"Perplexity vs Pruning Level\")\n",
    "    plt.xlabel(\"Pruning Level\")\n",
    "    plt.ylabel(\"Perplexity\")\n",
    "    plt.legend(fontsize=7, loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Recovery/improvement percentages\n",
    "    plt.subplot(2, 2, 3)\n",
    "    \n",
    "    # Create dataframe with recovery metrics\n",
    "    recovery_df = finetuned_df.copy()\n",
    "    \n",
    "    if not recovery_df.empty:\n",
    "        # Create unified recovery column (negative means improvement)\n",
    "        recovery_df[\"recovery\"] = recovery_df[\"recovery_percentage\"]\n",
    "        # If recovery is NaN but improvement exists, use negative of improvement\n",
    "        mask = recovery_df[\"recovery\"].isna() & recovery_df[\"improvement_percentage\"].notna()\n",
    "        recovery_df.loc[mask, \"recovery\"] = -recovery_df.loc[mask, \"improvement_percentage\"]\n",
    "        \n",
    "        # Plot by strategy\n",
    "        for strategy in strategies:\n",
    "            strategy_recovery = recovery_df[recovery_df[\"strategy\"] == strategy]\n",
    "            if not strategy_recovery.empty:\n",
    "                # Sort by pruning level\n",
    "                strategy_recovery = strategy_recovery.sort_values(\"pruning_level\")\n",
    "                plt.plot(strategy_recovery[\"pruning_level\"], strategy_recovery[\"recovery\"], \n",
    "                        \"o-\", label=strategy)\n",
    "        \n",
    "        plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.axhline(y=100, color=\"g\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.text(0.01, 100, \"Full Recovery\", color=\"green\", ha=\"left\", va=\"bottom\", fontsize=8)\n",
    "        plt.text(0.01, -5, \"Improvement\", color=\"blue\", ha=\"left\", va=\"top\", fontsize=8)\n",
    "        \n",
    "        plt.title(\"Recovery/Improvement %\")\n",
    "        plt.xlabel(\"Pruning Level\")\n",
    "        plt.ylabel(\"% (negative = improvement)\")\n",
    "        plt.legend(fontsize=7, loc='best')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, \"No recovery data available yet\", \n",
    "                ha=\"center\", va=\"center\", fontsize=12)\n",
    "    \n",
    "    # 4. Status overview\n",
    "    plt.subplot(2, 2, 4)\n",
    "    \n",
    "    # Count experiments by status\n",
    "    total_exps = len(set([(r[\"model\"], r[\"strategy\"], r[\"pruning_level\"]) for r in results]))\n",
    "    completed_exps = len(finetuned_df)\n",
    "    pruned_only = len(set(pruned_df[\"timestamp\"])) - completed_exps\n",
    "    baseline_only = len(set(baseline_df[\"timestamp\"])) - pruned_only - completed_exps\n",
    "    \n",
    "    # Create status labels and counts\n",
    "    status_labels = [\"Completed\", \"Pruned\", \"Baseline\", \"Planned\"]\n",
    "    status_counts = [\n",
    "        completed_exps,\n",
    "        pruned_only,\n",
    "        baseline_only,\n",
    "        total_exps - completed_exps - pruned_only - baseline_only\n",
    "    ]\n",
    "    \n",
    "    # Create status bar chart\n",
    "    colors = [\"green\", \"orange\", \"blue\", \"gray\"]\n",
    "    plt.bar(status_labels, status_counts, color=colors)\n",
    "    \n",
    "    for i, count in enumerate(status_counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha=\"center\")\n",
    "    \n",
    "    plt.title(f\"Experiment Status (Total: {total_exps})\")\n",
    "    plt.xlabel(\"Status\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add timestamp\n",
    "    plt.figtext(0.5, 0.01, f\"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\", \n",
    "               ha=\"center\", fontsize=8)\n",
    "    \n",
    "    # Apply tight layout to reduce white space\n",
    "    plt.tight_layout(pad=1.5)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the visualization - you can run this cell repeatedly to refresh\n",
    "df = visualize_ongoing_experiments()\n",
    "\n",
    "# Display success count by strategy if we have data\n",
    "if df is not None and not df.empty and \"fine_tuned\" in df[\"stage\"].values:\n",
    "    finetuned = df[df[\"stage\"] == \"fine_tuned\"]\n",
    "    \n",
    "    # Calculate improvement status\n",
    "    finetuned[\"status\"] = \"No Change\"\n",
    "    finetuned.loc[finetuned[\"perplexity\"] < finetuned[\"perplexity\"], \"status\"] = \"Improved\"\n",
    "    finetuned.loc[finetuned[\"perplexity\"] > finetuned[\"perplexity\"], \"status\"] = \"Degraded\"\n",
    "    \n",
    "    # Count by strategy and status\n",
    "    status_by_strategy = pd.crosstab(finetuned[\"strategy\"], finetuned[\"status\"])\n",
    "    display(status_by_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head Importance Visualization\n",
    "\n",
    "The cell below can be used to visualize which heads are most important in your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention head importance for different pruning strategies\n",
    "# This can help identify which heads are most critical for model performance\n",
    "\n",
    "# Initialize the model\n",
    "model_name = \"gpt2\"  # Change to one of the models you're using\n",
    "pruning_module = PruningModule(model_name)\n",
    "if not pruning_module.load_model():\n",
    "    print(f\"Failed to load model {model_name}\")\n",
    "else:\n",
    "    # Get original parameters\n",
    "    original_params = pruning_module.original_params\n",
    "    \n",
    "    # Set up a sample prompt\n",
    "    prompt = \"Artificial intelligence will transform society by\"\n",
    "    \n",
    "    # Calculate importance for different strategies\n",
    "    strategies = {}\n",
    "    \n",
    "    try:\n",
    "        # Random strategy (baseline)\n",
    "        random_strategy = get_strategy(\"random\", pruning_module, prompt)\n",
    "        random_importance = random_strategy.get_head_importance(original_params)\n",
    "        strategies[\"random\"] = random_importance\n",
    "        \n",
    "        # Magnitude strategy\n",
    "        magnitude_strategy = get_strategy(\"magnitude\", pruning_module, prompt)\n",
    "        magnitude_importance = magnitude_strategy.get_head_importance(original_params)\n",
    "        strategies[\"magnitude\"] = magnitude_importance\n",
    "        \n",
    "        # Entropy strategy\n",
    "        entropy_strategy = get_strategy(\"entropy\", pruning_module, prompt)\n",
    "        entropy_importance = entropy_strategy.get_head_importance(original_params)\n",
    "        strategies[\"entropy\"] = entropy_importance\n",
    "        \n",
    "        # Set better plot styling\n",
    "        plt.rcParams.update({\n",
    "            'figure.figsize': (12, 1.5 * pruning_module.num_layers),\n",
    "            'figure.titlesize': 14,\n",
    "            'axes.titlesize': 12,\n",
    "            'axes.labelsize': 10,\n",
    "            'xtick.labelsize': 9,\n",
    "            'ytick.labelsize': 9,\n",
    "            'legend.fontsize': 8,\n",
    "            'font.family': 'sans-serif'\n",
    "        })\n",
    "        \n",
    "        # Now visualize the head importance scores\n",
    "        fig, axes = plt.subplots(pruning_module.num_layers, 3, figsize=(12, 1.5 * pruning_module.num_layers))\n",
    "        \n",
    "        # Create title\n",
    "        fig.suptitle(f\"Attention Head Importance by Strategy for {model_name}\", fontsize=16)\n",
    "        \n",
    "        # Set column titles\n",
    "        for i, strategy_name in enumerate([\"random\", \"magnitude\", \"entropy\"]):\n",
    "            axes[0, i].set_title(f\"{strategy_name.capitalize()} Strategy\")\n",
    "        \n",
    "        # Create a heatmap for each strategy showing head importance\n",
    "        for layer in range(pruning_module.num_layers):\n",
    "            for i, strategy_name in enumerate([\"random\", \"magnitude\", \"entropy\"]):\n",
    "                # Extract importance scores for this layer\n",
    "                layer_scores = [score for l, h, score in strategies[strategy_name] if l == layer]\n",
    "                \n",
    "                # Create array for visualization\n",
    "                scores_array = np.array(layer_scores).reshape(1, -1)\n",
    "                \n",
    "                # Create heatmap\n",
    "                cax = axes[layer, i].imshow(scores_array, cmap=\"viridis\", aspect=\"auto\")\n",
    "                \n",
    "                # Add labels\n",
    "                axes[layer, i].set_yticks([0])\n",
    "                axes[layer, i].set_yticklabels([f\"Layer {layer}\"])\n",
    "                axes[layer, i].set_xticks(range(pruning_module.num_heads))\n",
    "                axes[layer, i].set_xticklabels([f\"H{h}\" for h in range(pruning_module.num_heads)], \n",
    "                                              rotation=90 if pruning_module.num_heads > 8 else 0)\n",
    "                \n",
    "                # Add importance values as text\n",
    "                for h in range(pruning_module.num_heads):\n",
    "                    score = scores_array[0, h]\n",
    "                    if np.isnan(score):\n",
    "                        text_color = \"black\"\n",
    "                    else:\n",
    "                        text_color = \"white\" if score > 0.5 else \"black\"\n",
    "                    axes[layer, i].text(h, 0, f\"{score:.2f}\", ha=\"center\", va=\"center\", \n",
    "                                       color=text_color, fontsize=8)\n",
    "        \n",
    "        # Add a colorbar\n",
    "        fig.colorbar(cax, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  # Make room for the title\n",
    "        plt.show()\n",
    "        \n",
    "        # Now show the top 10 most and least important heads according to entropy\n",
    "        # (usually considered the most accurate measure)\n",
    "        sorted_entropy = sorted(entropy_importance, key=lambda x: x[2])\n",
    "        \n",
    "        print(\"Top 10 Least Important Heads (candidates for pruning):\")\n",
    "        for i, (layer, head, score) in enumerate(sorted_entropy[:10]):\n",
    "            print(f\"{i+1}. Layer {layer}, Head {head}: {score:.4f}\")\n",
    "            \n",
    "        print(\"\\nTop 10 Most Important Heads (preserved even with aggressive pruning):\")\n",
    "        for i, (layer, head, score) in enumerate(sorted_entropy[-10:]):\n",
    "            print(f\"{i+1}. Layer {layer}, Head {head}: {score:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating head importance: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Analysis\n",
    "\n",
    "After collecting results, run a comprehensive analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results with improved sizing\n",
    "experiment.plot_results(figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "if not experiment.results_df.empty:\n",
    "    # Get data for different stages\n",
    "    baseline_df = experiment.results_df[experiment.results_df[\"stage\"] == \"baseline\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    baseline_df = baseline_df.rename(columns={\"perplexity\": \"baseline_perplexity\"})\n",
    "    \n",
    "    pruned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"pruned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    pruned_df = pruned_df.rename(columns={\"perplexity\": \"pruned_perplexity\"})\n",
    "    \n",
    "    finetuned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"fine_tuned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    finetuned_df = finetuned_df.rename(columns={\"perplexity\": \"finetuned_perplexity\"})\n",
    "    \n",
    "    # Merge dataframes\n",
    "    summary = pd.merge(baseline_df, pruned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    summary = pd.merge(summary, finetuned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    \n",
    "    # Calculate changes\n",
    "    summary[\"pruning_effect\"] = summary[\"pruned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    summary[\"finetuning_effect\"] = summary[\"finetuned_perplexity\"] - summary[\"pruned_perplexity\"]\n",
    "    summary[\"net_change\"] = summary[\"finetuned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    \n",
    "    # Display summary\n",
    "    summary.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
