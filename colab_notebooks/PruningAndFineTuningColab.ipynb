{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcc03596",
   "metadata": {},
   "source": [
    "# Make a GPT-2 Model Smaller and More Powerful (v0.0.35)\n",
    "\n",
    "This notebook demonstrates how to make a GPT-2 model both smaller and more powerful by:\n",
    "1. Applying pruning to remove less important attention heads\n",
    "2. Fine-tuning the pruned model to recover and improve performance\n",
    "3. Showing clear metrics of improvement throughout the process\n",
    "\n",
    "We use real data (Wikitext) rather than synthetic data for realistic evaluation.\n",
    "\n",
    "Version History:\n",
    "- v0.0.34 (April 2025): Fixed undefined variable error, visualization issues and enhanced CUDA error handling\n",
    "- v0.0.33 (April 2025): Fixed visualization issues, improved model compatibility and enhanced error handling\n",
    "- v0.0.32 (April 2025): Added CUDA error handling for Colab compatibility and memory management\n",
    "- v0.0.31 (April 2025): Fixed get_strategy parameters issue and improved Colab compatibility\n",
    "- v0.0.30 (April 2025): Added OPT model support and chart improvements\n",
    "\n",
    "---\n",
    "**Note**: This notebook is part of the SentinelAI project. For detailed documentation, see `PruningAndFineTuningColab.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7230997e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by installing the required dependencies and configuring our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757a3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utility for Colab\n",
    "def display_available_memory():\n",
    "    \"\"\"Display available memory in Colab.\"\"\"\n",
    "    if IS_COLAB:\n",
    "        # Get GPU memory info\n",
    "        try:\n",
    "            !nvidia-smi --query-gpu=memory.total,memory.used --format=csv\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Get system memory info\n",
    "        !free -h\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers==4.38.0 datasets==2.17.0 torch matplotlib tqdm\n",
    "\n",
    "# Check if we're running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IS_COLAB = True\n",
    "    print(\"Running in Google Colab!\")\n",
    "    \n",
    "    # Add file download helper for Colab\n",
    "    from google.colab import files\n",
    "    \n",
    "    def download_files(file_paths):\n",
    "        \"\"\"Helper function to download files from Colab.\"\"\"\n",
    "        for file_path in file_paths:\n",
    "            if os.path.exists(file_path):\n",
    "                files.download(file_path)\n",
    "                print(f\"Downloaded: {file_path}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Free up memory for Colab\n",
    "    import gc\n",
    "    import torch\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Display memory status\n",
    "    display_available_memory()\n",
    "    \n",
    "except:\n",
    "    IS_COLAB = False\n",
    "    print(\"Not running in Google Colab\")\n",
    "    \n",
    "    # Dummy function for non-Colab environments\n",
    "    def download_files(file_paths):\n",
    "        print(\"File download only works in Google Colab\")\n",
    "        print(f\"Files would be downloaded: {file_paths}\")\n",
    "        \n",
    "    def display_available_memory():\n",
    "        print(\"Memory display not available outside Colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee6651e",
   "metadata": {},
   "source": [
    "## Imports and Configuration\n",
    "\n",
    "Import required libraries and set up the configuration for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3143299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    get_linear_schedule_with_warmup, \n",
    "    GPT2LMHeadModel\n",
    ")\n",
    "\n",
    "# Initialize plotting style\n",
    "plt.style.use('seaborn-v0_8-pastel')\n",
    "\n",
    "# Configure device and optimize for Colab environment\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Half-precision for GPU to reduce memory usage\n",
    "USE_FP16 = DEVICE == \"cuda\"\n",
    "\n",
    "# Handle TPU if available (Colab-specific optimization)\n",
    "if 'COLAB_TPU_ADDR' in os.environ:\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        DEVICE = xm.xla_device()\n",
    "        print(f\"TPU detected and configured!\")\n",
    "        USE_FP16 = False  # TPUs have their own optimization\n",
    "    except ImportError:\n",
    "        print(\"TPU environment detected but torch_xla not installed.\")\n",
    "\n",
    "# Set up directories\n",
    "OUTPUT_DIR = \"pruning_results\"\n",
    "MODEL_CACHE_DIR = \"model_cache\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Using FP16: {USE_FP16}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# CUDA memory management helper\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory to avoid CUDA out of memory errors.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(\"GPU memory cleared\")\n",
    "\n",
    "# Import garbage collector for memory management\n",
    "import gc\n",
    "\n",
    "# For better GPU memory management, we'll use a context manager\n",
    "try:\n",
    "    import contextlib\n",
    "    @contextlib.contextmanager\n",
    "    def autocast_if_available():\n",
    "        \"\"\"Use autocast if available for better memory efficiency.\"\"\"\n",
    "        if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast') and USE_FP16:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                yield\n",
    "        else:\n",
    "            yield\n",
    "except:\n",
    "    # Fallback if the import fails\n",
    "    @contextlib.contextmanager\n",
    "    def autocast_if_available():\n",
    "        yield"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f33db2",
   "metadata": {},
   "source": [
    "## Progress Tracking\n",
    "\n",
    "We'll create a class to track metrics and visualize progress throughout the pruning and fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMetrics:\n",
    "    \"\"\"Track metrics throughout the pruning and fine-tuning process.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"loss\": [],\n",
    "            \"perplexity\": [],\n",
    "            \"steps\": [],\n",
    "            \"pruning_level\": None,\n",
    "            \"strategy\": None,\n",
    "            \"pruned_heads\": [],\n",
    "            \"gate_values\": [],\n",
    "            \"head_importance\": [],\n",
    "            \"generation_samples\": []\n",
    "        }\n",
    "        \n",
    "        # Create visualizations\n",
    "        self.fig, self.axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        self.loss_line = None\n",
    "        self.ppl_line = None\n",
    "        \n",
    "    def update(self, step, loss, perplexity, head_info=None, gate_values=None, \n",
    "               generation_sample=None):\n",
    "        \"\"\"Update metrics with new values.\"\"\"\n",
    "        self.metrics[\"steps\"].append(step)\n",
    "        self.metrics[\"loss\"].append(loss)\n",
    "        self.metrics[\"perplexity\"].append(perplexity)\n",
    "        \n",
    "        if head_info is not None:\n",
    "            self.metrics[\"head_importance\"] = head_info\n",
    "            \n",
    "        if gate_values is not None:\n",
    "            self.metrics[\"gate_values\"] = gate_values\n",
    "            \n",
    "        if generation_sample is not None:\n",
    "            self.metrics[\"generation_samples\"].append({\n",
    "                \"step\": step,\n",
    "                \"text\": generation_sample\n",
    "            })\n",
    "        \n",
    "        # Update visualization\n",
    "        self._update_plots()\n",
    "        \n",
    "    def set_pruning_info(self, strategy, level, pruned_heads):\n",
    "        \"\"\"Set pruning information.\"\"\"\n",
    "        self.metrics[\"strategy\"] = strategy\n",
    "        self.metrics[\"pruning_level\"] = level\n",
    "        self.metrics[\"pruned_heads\"] = pruned_heads\n",
    "        \n",
    "    def _update_plots(self):\n",
    "        \"\"\"Update visualization plots.\"\"\"\n",
    "        steps = self.metrics[\"steps\"]\n",
    "        loss = self.metrics[\"loss\"]\n",
    "        ppl = self.metrics[\"perplexity\"]\n",
    "        \n",
    "        if not steps:\n",
    "            return\n",
    "            \n",
    "        # Clear previous plots\n",
    "        self.axes[0].clear()\n",
    "        self.axes[1].clear()\n",
    "        \n",
    "        # Plot loss\n",
    "        self.axes[0].plot(steps, loss, 'b-')\n",
    "        self.axes[0].set_title('Training Loss')\n",
    "        self.axes[0].set_xlabel('Step')\n",
    "        self.axes[0].set_ylabel('Loss')\n",
    "        self.axes[0].grid(True)\n",
    "        \n",
    "        # Plot perplexity\n",
    "        self.axes[1].plot(steps, ppl, 'r-')\n",
    "        self.axes[1].set_title('Perplexity (lower is better)')\n",
    "        self.axes[1].set_xlabel('Step')\n",
    "        self.axes[1].set_ylabel('Perplexity')\n",
    "        self.axes[1].grid(True)\n",
    "        \n",
    "        self.fig.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.001)\n",
    "        \n",
    "    def save_plots(self, path):\n",
    "        \"\"\"Save plots to file.\"\"\"\n",
    "        plt.savefig(path)\n",
    "        \n",
    "    def save_metrics(self, path):\n",
    "        \"\"\"Save metrics to file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "            \n",
    "    def get_summary(self):\n",
    "        \"\"\"Return a summary of key metrics.\"\"\"\n",
    "        if not self.metrics[\"perplexity\"] or len(self.metrics[\"perplexity\"]) <= 1:\n",
    "            return {\"error\": \"Not enough data points for summary\"}\n",
    "            \n",
    "        return {\n",
    "            \"strategy\": self.metrics[\"strategy\"],\n",
    "            \"pruning_level\": self.metrics[\"pruning_level\"],\n",
    "            \"pruned_heads_count\": len(self.metrics[\"pruned_heads\"]),\n",
    "            \"initial_loss\": self.metrics[\"loss\"][0],\n",
    "            \"final_loss\": self.metrics[\"loss\"][-1],\n",
    "            \"initial_perplexity\": self.metrics[\"perplexity\"][0],\n",
    "            \"final_perplexity\": self.metrics[\"perplexity\"][-1],\n",
    "            \"improvement_percent\": ((self.metrics[\"perplexity\"][0] - self.metrics[\"perplexity\"][-1]) / \n",
    "                                   self.metrics[\"perplexity\"][0] * 100)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f3d08",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We'll use the Wikitext-2 dataset for fine-tuning and evaluation, which provides real-world text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75961d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories():\n",
    "    \"\"\"Create necessary directories for outputs and data.\"\"\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(MODEL_CACHE_DIR, exist_ok=True)\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    return OUTPUT_DIR, MODEL_CACHE_DIR, DATA_DIR\n",
    "\n",
    "def download_wikitext():\n",
    "    \"\"\"Download Wikitext dataset if not already present.\"\"\"\n",
    "    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n",
    "    \n",
    "    if not os.path.exists(wikitext_file):\n",
    "        print(\"Downloading Wikitext-2 dataset...\")\n",
    "        try:\n",
    "            # Using HF datasets library\n",
    "            from datasets import load_dataset\n",
    "            dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "            \n",
    "            # Save validation text\n",
    "            with open(wikitext_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for item in tqdm(dataset[\"validation\"], desc=\"Saving dataset\"):\n",
    "                    if item[\"text\"].strip():\n",
    "                        f.write(item[\"text\"] + \"\\n\")\n",
    "                        \n",
    "            print(f\"Dataset saved to {wikitext_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading dataset: {e}\")\n",
    "            \n",
    "            # Fallback: download using requests\n",
    "            try:\n",
    "                import requests\n",
    "                url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\"\n",
    "                r = requests.get(url)\n",
    "                \n",
    "                # Save zip file\n",
    "                zip_path = os.path.join(DATA_DIR, \"wikitext-2-raw-v1.zip\")\n",
    "                with open(zip_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "                \n",
    "                # Extract\n",
    "                import zipfile\n",
    "                with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(DATA_DIR)\n",
    "                \n",
    "                print(f\"Dataset downloaded and extracted to {DATA_DIR}\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Fallback download also failed: {e2}\")\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def prepare_dataset(paragraphs, tokenizer, max_length, batch_size):\n",
    "    \"\"\"Tokenize and prepare paragraphs into a PyTorch dataset.\"\"\"\n",
    "    # Tokenize\n",
    "    tokenized = tokenizer(\n",
    "        paragraphs,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def load_wikitext_data(tokenizer, max_length=512, batch_size=4):\n",
    "    \"\"\"Load and prepare Wikitext data for fine-tuning and evaluation.\"\"\"\n",
    "    wikitext_file = os.path.join(DATA_DIR, \"wikitext-2-raw-v1-validation.txt\")\n",
    "    \n",
    "    if not os.path.exists(wikitext_file):\n",
    "        success = download_wikitext()\n",
    "        if not success:\n",
    "            print(\"Failed to download dataset\")\n",
    "            return None, None\n",
    "    \n",
    "    # Read the data\n",
    "    print(\"Loading Wikitext-2 data...\")\n",
    "    with open(wikitext_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Split into train and validation (80/20)\n",
    "    paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    \n",
    "    # Ensure we have at least 100 paragraphs of reasonable length\n",
    "    paragraphs = [p for p in paragraphs if len(p) > 100]\n",
    "    \n",
    "    if len(paragraphs) < 100:\n",
    "        # Fall back to splitting by newline if needed\n",
    "        paragraphs = [p for p in text.split(\"\\n\") if len(p.strip()) > 100]\n",
    "    \n",
    "    # Shuffle and split\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(paragraphs)\n",
    "    \n",
    "    split_idx = int(len(paragraphs) * 0.8)\n",
    "    train_paragraphs = paragraphs[:split_idx]\n",
    "    val_paragraphs = paragraphs[split_idx:]\n",
    "    \n",
    "    print(f\"Tokenizing {len(train_paragraphs)} training and {len(val_paragraphs)} validation paragraphs...\")\n",
    "    \n",
    "    # Tokenize and prepare datasets\n",
    "    train_data = prepare_dataset(train_paragraphs, tokenizer, max_length, batch_size)\n",
    "    val_data = prepare_dataset(val_paragraphs, tokenizer, max_length, batch_size)\n",
    "    \n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b1845e",
   "metadata": {},
   "source": [
    "## Model Loading\n",
    "\n",
    "Load the pre-trained model and prepare it for pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name, cache_dir=None):\n",
    "    \"\"\"Load pre-trained model and tokenizer.\"\"\"\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    \n",
    "    # Determine model type from name\n",
    "    if \"gpt2\" in model_name.lower():\n",
    "        model_type = \"gpt2\"\n",
    "    elif \"opt\" in model_name.lower() or \"facebook\" in model_name.lower():\n",
    "        model_type = \"opt\"\n",
    "    elif \"pythia\" in model_name.lower() or \"eleutherai\" in model_name.lower():\n",
    "        model_type = \"pythia\"\n",
    "    else:\n",
    "        model_type = \"gpt2\"  # Default to gpt2\n",
    "        \n",
    "    print(f\"Detected model type: {model_type}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Ensure padding token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with potential FP16 optimization\n",
    "    try:\n",
    "        if USE_FP16:\n",
    "            print(\"Using FP16 for model loading\")\n",
    "            # For FP16, we need to set torch_dtype\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                cache_dir=cache_dir,\n",
    "                torch_dtype=torch.float16\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model with AutoModelForCausalLM: {e}\")\n",
    "        print(\"Falling back to GPT2LMHeadModel\")\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Store model type for later use\n",
    "    model.model_type = model_type\n",
    "    \n",
    "    # Print model size information\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model loaded with {param_count/1e6:.2f}M parameters\")\n",
    "    \n",
    "    # Add head_count attribute if we can determine it\n",
    "    try:\n",
    "        # Count number of attention heads\n",
    "        if hasattr(model.config, \"n_head\"):\n",
    "            model.head_count = model.config.n_head\n",
    "        elif hasattr(model.config, \"num_attention_heads\"):\n",
    "            model.head_count = model.config.num_attention_heads\n",
    "        elif hasattr(model.config, \"num_heads\"):\n",
    "            model.head_count = model.config.num_heads\n",
    "        else:\n",
    "            model.head_count = 12  # Reasonable default\n",
    "        print(f\"Model has {model.head_count} attention heads per layer\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine head count: {e}\")\n",
    "        model.head_count = 12  # Reasonable default\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d790ab",
   "metadata": {},
   "source": [
    "## Attention Module Extraction\n",
    "\n",
    "Identify and extract attention modules from the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b02eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_modules(model):\n",
    "    \"\"\"Extract attention modules from model.\"\"\"\n",
    "    # Set default model type if not already set\n",
    "    if not hasattr(model, \"model_type\"):\n",
    "        model.model_type = \"gpt2\"\n",
    "    \n",
    "    attention_modules = []\n",
    "    \n",
    "    # Function to safely get attribute path\n",
    "    def get_nested_attr(obj, attr_path):\n",
    "        \"\"\"Safely get attribute path without raising AttributeError.\"\"\"\n",
    "        attrs = attr_path.split(\".\")\n",
    "        current = obj\n",
    "        for attr in attrs:\n",
    "            if hasattr(current, attr):\n",
    "                current = getattr(current, attr)\n",
    "            else:\n",
    "                return None\n",
    "        return current\n",
    "    \n",
    "    # Try different model architectures\n",
    "    if model.model_type == \"gpt2\":\n",
    "        # GPT-2 style models\n",
    "        transformer = get_nested_attr(model, \"transformer\")\n",
    "        if transformer:\n",
    "            blocks = get_nested_attr(transformer, \"h\")\n",
    "            if blocks:\n",
    "                for i, block in enumerate(blocks):\n",
    "                    attn = get_nested_attr(block, \"attn\")\n",
    "                    if attn:\n",
    "                        attention_modules.append((i, attn))\n",
    "    \n",
    "    elif model.model_type == \"opt\":\n",
    "        # OPT models\n",
    "        model_module = get_nested_attr(model, \"model\")\n",
    "        if model_module:\n",
    "            decoder = get_nested_attr(model_module, \"decoder\")\n",
    "            if decoder:\n",
    "                layers = get_nested_attr(decoder, \"layers\")\n",
    "                if layers:\n",
    "                    for i, layer in enumerate(layers):\n",
    "                        self_attn = get_nested_attr(layer, \"self_attn\")\n",
    "                        if self_attn:\n",
    "                            attention_modules.append((i, self_attn))\n",
    "    \n",
    "    elif model.model_type == \"pythia\":\n",
    "        # Pythia models (similar to GPT-2)\n",
    "        transformer = get_nested_attr(model, \"transformer\") or get_nested_attr(model, \"gpt_neox\")\n",
    "        if transformer:\n",
    "            blocks = get_nested_attr(transformer, \"h\") or get_nested_attr(transformer, \"layers\")\n",
    "            if blocks:\n",
    "                for i, block in enumerate(blocks):\n",
    "                    attn = get_nested_attr(block, \"attn\") or get_nested_attr(block, \"attention\")\n",
    "                    if attn:\n",
    "                        attention_modules.append((i, attn))\n",
    "    \n",
    "    # Generic fallback if nothing matched\n",
    "    if not attention_modules:\n",
    "        # Try common patterns across different architectures\n",
    "        candidate_paths = [\n",
    "            \"transformer.h\",              # GPT-2 style\n",
    "            \"model.decoder.layers\",       # OPT style\n",
    "            \"encoder.layers\",             # Encoder style models\n",
    "            \"decoder.layers\",             # Decoder style models\n",
    "            \"layers\",                     # Direct layers attribute\n",
    "            \"transformer.layers\",         # Some transformers\n",
    "            \"gpt_neox.layers\"             # Pythia/GPT-NeoX\n",
    "        ]\n",
    "        \n",
    "        for path in candidate_paths:\n",
    "            try:\n",
    "                blocks = get_nested_attr(model, path)\n",
    "                if blocks and isinstance(blocks, (list, tuple)) or hasattr(blocks, \"__getitem__\"):\n",
    "                    for i, block in enumerate(blocks):\n",
    "                        # Try common attention module names\n",
    "                        for attn_name in [\"attn\", \"attention\", \"self_attn\", \"self_attention\", \"mha\"]:\n",
    "                            attn = get_nested_attr(block, attn_name)\n",
    "                            if attn:\n",
    "                                attention_modules.append((i, attn))\n",
    "                                break\n",
    "                    \n",
    "                    if attention_modules:\n",
    "                        # Found some attention modules, can stop looking\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    if attention_modules:\n",
    "        print(f\"Found {len(attention_modules)} attention modules\")\n",
    "        \n",
    "        # Try to add head_size attribute if not present\n",
    "        for _, attn in attention_modules:\n",
    "            if not hasattr(attn, \"head_size\") and hasattr(model, \"head_count\"):\n",
    "                # Try to determine head size from attention module\n",
    "                if hasattr(attn, \"head_dim\"):\n",
    "                    attn.head_size = attn.head_dim\n",
    "                elif hasattr(model.config, \"hidden_size\"):\n",
    "                    attn.head_size = model.config.hidden_size // model.head_count\n",
    "                elif hasattr(attn, \"q_proj\") and hasattr(attn.q_proj, \"weight\"):\n",
    "                    # Common in models like OPT\n",
    "                    attn.head_size = attn.q_proj.weight.shape[0] // model.head_count\n",
    "                elif hasattr(attn, \"c_attn\") and hasattr(attn.c_attn, \"weight\"):\n",
    "                    # Common in GPT-2 models\n",
    "                    q_weight = attn.c_attn.weight\n",
    "                    attn.head_size = q_weight.shape[1] // (3 * model.head_count)\n",
    "            \n",
    "            # Add num_heads attribute if not present\n",
    "            if not hasattr(attn, \"num_heads\") and hasattr(model, \"head_count\"):\n",
    "                attn.num_heads = model.head_count\n",
    "    else:\n",
    "        print(\"Warning: Could not find attention modules. Unsupported model architecture.\")\n",
    "    \n",
    "    return attention_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a7aa43",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Define functions to evaluate model performance and generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9e4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"Evaluate model loss and perplexity on the provided dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Don't track gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        with autocast_if_available():\n",
    "            for batch_idx, (input_ids, attention_mask) in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "                try:\n",
    "                    # Move tensors to the correct device\n",
    "                    input_ids = input_ids.to(DEVICE)\n",
    "                    attention_mask = attention_mask.to(DEVICE)\n",
    "                    \n",
    "                    # Create labels (shift input_ids right)\n",
    "                    labels = input_ids.clone()\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    \n",
    "                    # Accumulate loss\n",
    "                    batch_tokens = torch.sum(attention_mask).item()\n",
    "                    total_loss += loss.item() * batch_tokens\n",
    "                    total_tokens += batch_tokens\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if DEVICE == \"cuda\" and \"CUDA\" in str(e):\n",
    "                        print(f\"CUDA error during evaluation at batch {batch_idx}: {e}\")\n",
    "                        print(\"Attempting to continue evaluation on CPU...\")\n",
    "                        \n",
    "                        # Transfer model to CPU for this batch\n",
    "                        model = model.cpu()\n",
    "                        DEVICE_BACKUP = \"cpu\"\n",
    "                        \n",
    "                        # Try again on CPU\n",
    "                        input_ids = input_ids.to(DEVICE_BACKUP)\n",
    "                        attention_mask = attention_mask.to(DEVICE_BACKUP)\n",
    "                        labels = labels.to(DEVICE_BACKUP)\n",
    "                        \n",
    "                        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        loss = outputs.loss\n",
    "                        \n",
    "                        batch_tokens = torch.sum(attention_mask).item()\n",
    "                        total_loss += loss.item() * batch_tokens\n",
    "                        total_tokens += batch_tokens\n",
    "                        \n",
    "                        # Move model back to GPU for next batches\n",
    "                        model = model.to(DEVICE)\n",
    "                    else:\n",
    "                        print(f\"Error during evaluation at batch {batch_idx}: {e}\")\n",
    "                        # Skip this batch\n",
    "                        continue\n",
    "    \n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
    "    \"\"\"Generate text from the model with the given prompt.\"\"\"\n",
    "    print(f\"Generating text with prompt: '{prompt}'\")\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    encoded_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoded_prompt[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = encoded_prompt[\"attention_mask\"].to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            with autocast_if_available():\n",
    "                # Generate text\n",
    "                output = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=max_length,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        if DEVICE == \"cuda\" and \"CUDA\" in str(e):\n",
    "            print(f\"CUDA error during text generation: {e}\")\n",
    "            print(\"Falling back to CPU for generation...\")\n",
    "            \n",
    "            # Free up memory\n",
    "            clear_gpu_memory()\n",
    "            \n",
    "            # Move to CPU and try again\n",
    "            cpu_model = model.cpu()\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "            \n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    output = cpu_model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=max_length,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                # Move model back to GPU\n",
    "                model.to(DEVICE)\n",
    "                \n",
    "                # Decode the generated text\n",
    "                generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "                return generated_text\n",
    "            except Exception as e2:\n",
    "                print(f\"Generation also failed on CPU: {e2}\")\n",
    "                return f\"[Generation failed: {str(e2)}]\"\n",
    "        else:\n",
    "            print(f\"Error during text generation: {e}\")\n",
    "            return f\"[Generation failed: {str(e)}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee5441a",
   "metadata": {},
   "source": [
    "## Head Importance Calculation\n",
    "\n",
    "Calculate the importance of each attention head using different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e1de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_attention_hooks(model, attention_modules):\n",
    "    \"\"\"Register hooks to collect attention patterns.\"\"\"\n",
    "    attention_patterns = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # For GPT-2, output[0] contains attention weights of shape [batch_size, num_heads, seq_len, seq_len]\n",
    "        # Store it for later analysis\n",
    "        attention_patterns.append(output[0].detach())\n",
    "    \n",
    "    hooks = []\n",
    "    for layer_idx, attn_module in attention_modules:\n",
    "        # Check if module has the expected attributes for attention weights\n",
    "        if hasattr(attn_module, \"_attn\") and callable(attn_module._attn):\n",
    "            # GPT-2 style hook\n",
    "            hook = attn_module._attn.__name__ if hasattr(attn_module._attn, \"__name__\") else \"hook\"\n",
    "            hooks.append(attn_module.register_forward_hook(hook_fn))\n",
    "        elif hasattr(attn_module, \"forward\") and callable(attn_module.forward):\n",
    "            # Generic attention hook\n",
    "            hook = attn_module.forward.__name__ if hasattr(attn_module.forward, \"__name__\") else \"hook\"\n",
    "            hooks.append(attn_module.register_forward_hook(hook_fn))\n",
    "    \n",
    "    return hooks, attention_patterns\n",
    "\n",
    "def compute_entropy(attention_weights):\n",
    "    \"\"\"Compute entropy of attention patterns.\"\"\"\n",
    "    # attention_weights shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "    # Clamp values to avoid log(0)\n",
    "    eps = 1e-8\n",
    "    attention_weights = torch.clamp(attention_weights, eps, 1.0)\n",
    "    \n",
    "    # Compute entropy per head: -sum(p * log(p))\n",
    "    entropy = -torch.sum(attention_weights * torch.log(attention_weights), dim=-1)  # [batch, num_heads, seq_len]\n",
    "    \n",
    "    # Average over sequence length and batch\n",
    "    entropy = entropy.mean(dim=-1).mean(dim=0)  # [num_heads]\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def gather_head_importance(model, dataloader, attention_modules, strategy=\"entropy\", num_batches=10):\n",
    "    \"\"\"Calculate importance of each attention head using the specified strategy.\"\"\"\n",
    "    # Prepare to gather head importance\n",
    "    model.eval()\n",
    "    num_layers = len(attention_modules)\n",
    "    num_heads = model.head_count if hasattr(model, \"head_count\") else 12\n",
    "    \n",
    "    # Initialize importance scores\n",
    "    if strategy == \"random\":\n",
    "        # Random importance\n",
    "        importance = torch.rand(num_layers, num_heads)\n",
    "        print(\"Using random importance scores\")\n",
    "        return importance\n",
    "        \n",
    "    # For magnitude or entropy, we need to compute them\n",
    "    importance = torch.zeros(num_layers, num_heads)\n",
    "    \n",
    "    if strategy == \"magnitude\":\n",
    "        # Compute L2 norm of weight matrices\n",
    "        print(\"Computing magnitude-based importance...\")\n",
    "        for layer_idx, attn_module in attention_modules:\n",
    "            if hasattr(attn_module, \"c_attn\") and hasattr(attn_module.c_attn, \"weight\"):\n",
    "                # GPT-2 style\n",
    "                weight = attn_module.c_attn.weight\n",
    "                head_size = weight.shape[1] // (3 * num_heads)\n",
    "                \n",
    "                # Compute importance for each head\n",
    "                for head_idx in range(num_heads):\n",
    "                    # Get query weight matrix for this head\n",
    "                    start_idx = head_idx * head_size\n",
    "                    end_idx = (head_idx + 1) * head_size\n",
    "                    head_weight = weight[:, start_idx:end_idx]\n",
    "                    \n",
    "                    # Compute L2 norm\n",
    "                    importance[layer_idx, head_idx] = torch.norm(head_weight)\n",
    "            elif hasattr(attn_module, \"q_proj\") and hasattr(attn_module.q_proj, \"weight\"):\n",
    "                # OPT style\n",
    "                weight = attn_module.q_proj.weight\n",
    "                head_size = weight.shape[0] // num_heads\n",
    "                \n",
    "                # Compute importance for each head\n",
    "                for head_idx in range(num_heads):\n",
    "                    # Get query weight matrix for this head\n",
    "                    start_idx = head_idx * head_size\n",
    "                    end_idx = (head_idx + 1) * head_size\n",
    "                    head_weight = weight[start_idx:end_idx, :]\n",
    "                    \n",
    "                    # Compute L2 norm\n",
    "                    importance[layer_idx, head_idx] = torch.norm(head_weight)\n",
    "            else:\n",
    "                # Fallback: random importance for this layer\n",
    "                importance[layer_idx, :] = torch.rand(num_heads)\n",
    "    \n",
    "    elif strategy == \"entropy\":\n",
    "        # Register hooks to capture attention patterns\n",
    "        hooks, attention_patterns = register_attention_hooks(model, attention_modules)\n",
    "        \n",
    "        try:\n",
    "            # Evaluate model on some batches to get attention patterns\n",
    "            print(\"Capturing attention patterns for entropy calculation...\")\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (input_ids, attention_mask) in enumerate(tqdm(dataloader, desc=\"Computing entropy\")):\n",
    "                    if batch_idx >= num_batches:\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        # Move to device\n",
    "                        input_ids = input_ids.to(DEVICE)\n",
    "                        attention_mask = attention_mask.to(DEVICE)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        with autocast_if_available():\n",
    "                            _ = model(input_ids, attention_mask=attention_mask)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error computing entropy for batch {batch_idx}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Process collected attention patterns\n",
    "            if attention_patterns:\n",
    "                for layer_idx, attn_pattern in enumerate(attention_patterns):\n",
    "                    if layer_idx < num_layers:\n",
    "                        # Compute entropy for each head\n",
    "                        entropy = compute_entropy(attn_pattern)\n",
    "                        importance[layer_idx, :num_heads] = entropy\n",
    "            else:\n",
    "                print(\"No attention patterns collected. Using random importance.\")\n",
    "                importance = torch.rand(num_layers, num_heads)\n",
    "        finally:\n",
    "            # Remove hooks\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    return importance\n",
    "\n",
    "def get_strategy(model_type, strategy_name):\n",
    "    \"\"\"Get the appropriate head importance strategy.\"\"\"\n",
    "    # For Pythia models, entropy computation can be unstable, use magnitude by default\n",
    "    if model_type == \"pythia\" and strategy_name == \"entropy\":\n",
    "        print(\"Warning: Entropy strategy may be unstable for Pythia models. Using magnitude instead.\")\n",
    "        return \"magnitude\"\n",
    "    return strategy_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041491b",
   "metadata": {},
   "source": [
    "## Attention Pruning\n",
    "\n",
    "Implement attention gating for pruning less important heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attention_gating(model, attention_modules):\n",
    "    \"\"\"Add attention gates to model by modifying the attention computation.\"\"\"\n",
    "    num_layers = len(attention_modules)\n",
    "    num_heads = model.head_count if hasattr(model, \"head_count\") else 12\n",
    "    \n",
    "    # Create gate parameters\n",
    "    gates = torch.ones(num_layers, num_heads, requires_grad=True)\n",
    "    model.attention_gates = torch.nn.Parameter(gates)\n",
    "    \n",
    "    # Function to apply gating to attention\n",
    "    def apply_gating_to_attention(attn_module, layer_idx):\n",
    "        \"\"\"Apply gating to an attention module.\"\"\"\n",
    "        original_attn = None\n",
    "        \n",
    "        # GPT-2 style gating\n",
    "        if hasattr(attn_module, \"_attn\") and callable(attn_module._attn):\n",
    "            original_attn = attn_module._attn\n",
    "            \n",
    "            def gated_attention(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "                \"\"\"Gated version of the attention function.\"\"\"\n",
    "                # Call original attention\n",
    "                attn_output = original_attn(query, key, value, attention_mask, head_mask)\n",
    "                \n",
    "                # Apply gating (attn_output has shape [batch, num_heads, seq_len, head_dim])\n",
    "                gates = model.attention_gates[layer_idx].view(1, -1, 1, 1)\n",
    "                gated_output = attn_output * gates\n",
    "                \n",
    "                return gated_output\n",
    "            \n",
    "            # Replace attention function\n",
    "            attn_module._attn = types.MethodType(gated_attention, attn_module)\n",
    "        \n",
    "        # OPT style gating (apply to bmm operation)\n",
    "        elif hasattr(attn_module, \"forward\") and callable(attn_module.forward):\n",
    "            original_forward = attn_module.forward\n",
    "            \n",
    "            def gated_forward(self, *args, **kwargs):\n",
    "                \"\"\"Gated version of the forward function.\"\"\"\n",
    "                # Call original forward\n",
    "                output = original_forward(*args, **kwargs)\n",
    "                \n",
    "                # Check if we have multiple outputs\n",
    "                if isinstance(output, tuple):\n",
    "                    attn_output = output[0]\n",
    "                    \n",
    "                    # Reshape gates to match attention output\n",
    "                    gates = model.attention_gates[layer_idx].view(1, -1, 1, 1)\n",
    "                    \n",
    "                    # Apply gating\n",
    "                    try:\n",
    "                        # Handle different output formats\n",
    "                        if attn_output.dim() == 4 and attn_output.shape[1] == num_heads:\n",
    "                            # Standard shape [batch, num_heads, seq_len, head_dim]\n",
    "                            gated_output = attn_output * gates\n",
    "                            return (gated_output,) + output[1:]\n",
    "                        else:\n",
    "                            # Unknown format, try not to break things\n",
    "                            print(f\"Warning: Couldn't apply gating to output with shape {attn_output.shape}\")\n",
    "                            return output\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error applying gates: {e}\")\n",
    "                        return output\n",
    "                else:\n",
    "                    # Single output\n",
    "                    attn_output = output\n",
    "                    \n",
    "                    # Reshape gates to match attention output\n",
    "                    gates = model.attention_gates[layer_idx].view(1, -1, 1, 1)\n",
    "                    \n",
    "                    # Apply gating\n",
    "                    try:\n",
    "                        if attn_output.dim() == 4 and attn_output.shape[1] == num_heads:\n",
    "                            gated_output = attn_output * gates\n",
    "                            return gated_output\n",
    "                        else:\n",
    "                            print(f\"Warning: Couldn't apply gating to output with shape {attn_output.shape}\")\n",
    "                            return output\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error applying gates: {e}\")\n",
    "                        return output\n",
    "            \n",
    "            # Replace forward function\n",
    "            attn_module.forward = types.MethodType(gated_forward, attn_module)\n",
    "        \n",
    "        return original_attn is not None\n",
    "    \n",
    "    # Import types module for MethodType\n",
    "    import types\n",
    "    \n",
    "    # Apply gating to all attention modules\n",
    "    modified_count = 0\n",
    "    for layer_idx, attn_module in attention_modules:\n",
    "        if apply_gating_to_attention(attn_module, layer_idx):\n",
    "            modified_count += 1\n",
    "    \n",
    "    print(f\"Added attention gating to {modified_count}/{len(attention_modules)} modules\")\n",
    "    return modified_count > 0\n",
    "\n",
    "def apply_head_pruning(model, importance, pruning_level, max_display_items=40):\n",
    "    \"\"\"Apply pruning to less important heads.\"\"\"\n",
    "    # Flatten importance to get global ranking\n",
    "    flat_importance = importance.view(-1)\n",
    "    num_heads_total = flat_importance.shape[0]\n",
    "    \n",
    "    # Determine heads to prune\n",
    "    k = int(num_heads_total * pruning_level)\n",
    "    if k <= 0:\n",
    "        print(\"Pruning level too low, no heads will be pruned\")\n",
    "        return []\n",
    "    \n",
    "    # Get heads with lowest importance values\n",
    "    _, indices = torch.topk(flat_importance, k, largest=False)\n",
    "    heads_to_prune = [(idx // importance.shape[1], idx % importance.shape[1]) for idx in indices]\n",
    "    \n",
    "    # Sort by layer then head for better visualization\n",
    "    heads_to_prune.sort()\n",
    "    \n",
    "    # Apply pruning by setting gates to zero\n",
    "    for layer_idx, head_idx in heads_to_prune:\n",
    "        model.attention_gates[layer_idx, head_idx] = 0.0\n",
    "    \n",
    "    # Display pruned heads\n",
    "    print(f\"Pruned {len(heads_to_prune)} attention heads ({pruning_level*100:.1f}% of {num_heads_total} total heads)\")\n",
    "    \n",
    "    # Visually show which heads were pruned with limited display\n",
    "    if len(heads_to_prune) > max_display_items:\n",
    "        print(f\"Showing a subset of pruned heads (displaying {max_display_items} out of {len(heads_to_prune)} heads)\")\n",
    "        # Always show some heads from the beginning, middle, and end\n",
    "        display_heads = heads_to_prune[:max_display_items//3] + heads_to_prune[len(heads_to_prune)//2-max_display_items//6:len(heads_to_prune)//2+max_display_items//6] + heads_to_prune[-max_display_items//3:]\n",
    "    else:\n",
    "        display_heads = heads_to_prune\n",
    "    \n",
    "    # Show pruned heads in a grid\n",
    "    num_layers = importance.shape[0]\n",
    "    num_heads = importance.shape[1]\n",
    "    grid = []\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        row = []\n",
    "        for head_idx in range(num_heads):\n",
    "            if (layer_idx, head_idx) in heads_to_prune:\n",
    "                if (layer_idx, head_idx) in display_heads:\n",
    "                    row.append(\"\")  # Red circle for pruned heads in display set\n",
    "                else:\n",
    "                    row.append(\"\")   # Small dot for pruned heads not in display set\n",
    "            else:\n",
    "                row.append(\"\")      # White circle for kept heads\n",
    "        grid.append(\"\".join(row))\n",
    "    \n",
    "    # Print the grid with layer numbers\n",
    "    for layer_idx, row in enumerate(grid):\n",
    "        print(f\"Layer {layer_idx:2d}: {row}\")\n",
    "    \n",
    "    return heads_to_prune\n",
    "\n",
    "def visualize_head_importance(importance, pruned_heads=None, max_display_items=40):\n",
    "    \"\"\"Visualize the importance of attention heads.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Get dimensions\n",
    "    num_layers, num_heads = importance.shape\n",
    "    \n",
    "    # Convert to numpy\n",
    "    importance_np = importance.cpu().numpy()\n",
    "    \n",
    "    # Create a heatmap\n",
    "    im = ax.imshow(importance_np, cmap=\"viridis\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    plt.colorbar(im, ax=ax, label=\"Importance\")\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xlabel(\"Head\")\n",
    "    ax.set_ylabel(\"Layer\")\n",
    "    ax.set_title(\"Attention Head Importance\")\n",
    "    \n",
    "    # Set ticks\n",
    "    if num_heads <= 20:\n",
    "        ax.set_xticks(np.arange(num_heads))\n",
    "        ax.set_xticklabels([str(i) for i in range(num_heads)])\n",
    "    else:\n",
    "        # Show fewer ticks for readability\n",
    "        ax.set_xticks(np.arange(0, num_heads, 2))\n",
    "        ax.set_xticklabels([str(i) for i in range(0, num_heads, 2)])\n",
    "    \n",
    "    if num_layers <= 12:\n",
    "        ax.set_yticks(np.arange(num_layers))\n",
    "        ax.set_yticklabels([str(i) for i in range(num_layers)])\n",
    "    else:\n",
    "        # Show fewer ticks for readability\n",
    "        ax.set_yticks(np.arange(0, num_layers, 2))\n",
    "        ax.set_yticklabels([str(i) for i in range(0, num_layers, 2)])\n",
    "    \n",
    "    # Rotate x labels for better readability\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Mark pruned heads if provided\n",
    "    if pruned_heads:\n",
    "        # If we have a lot of pruned heads, only plot a subset\n",
    "        if len(pruned_heads) > max_display_items:\n",
    "            # Prioritize variety - sample across layers\n",
    "            subset_indices = np.linspace(0, len(pruned_heads)-1, max_display_items).astype(int)\n",
    "            display_heads = [pruned_heads[i] for i in subset_indices]\n",
    "            print(f\"Showing {max_display_items} out of {len(pruned_heads)} pruned heads in the visualization\")\n",
    "        else:\n",
    "            display_heads = pruned_heads\n",
    "        \n",
    "        # Plot pruned heads as red squares\n",
    "        for layer_idx, head_idx in display_heads:\n",
    "            rect = plt.Rectangle((head_idx - 0.5, layer_idx - 0.5), 1, 1, fill=False, \n",
    "                                 edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d4c8c",
   "metadata": {},
   "source": [
    "## Fine-tuning Implementation\n",
    "\n",
    "Fine-tune the pruned model to recover performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(model, train_dataloader, val_dataloader, optimizer, scheduler, metrics, num_epochs=3):\n",
    "    \"\"\"Fine-tune the model and track metrics.\"\"\"\n",
    "    print(f\"Starting fine-tuning for {num_epochs} epochs\")\n",
    "    \n",
    "    step = 0\n",
    "    total_steps = len(train_dataloader) * num_epochs\n",
    "    evaluation_freq = max(1, len(train_dataloader) // 5)  # Evaluate 5 times per epoch\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (input_ids, attention_mask) in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                input_ids = input_ids.to(DEVICE)\n",
    "                attention_mask = attention_mask.to(DEVICE)\n",
    "                \n",
    "                # Create labels by shifting input_ids right\n",
    "                labels = input_ids.clone()\n",
    "                \n",
    "                # Clear previous gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with autocast_if_available():\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update learning rate\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Evaluate periodically\n",
    "                if batch_idx % evaluation_freq == 0 or batch_idx == len(train_dataloader) - 1:\n",
    "                    # Generate text sample periodically\n",
    "                    if batch_idx % (evaluation_freq * 2) == 0:\n",
    "                        prompt = \"The quick brown fox\"\n",
    "                        sample = generate_text(model, tokenizer, prompt)\n",
    "                    else:\n",
    "                        sample = None\n",
    "                    \n",
    "                    # Evaluate model\n",
    "                    val_loss, val_ppl = evaluate_model(model, val_dataloader)\n",
    "                    print(f\"Step {step+1}/{total_steps} | Loss: {loss.item():.4f} | Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.2f}\")\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    metrics.update(step, val_loss, val_ppl, generation_sample=sample)\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if (epoch == num_epochs - 1) and (batch_idx == len(train_dataloader) - 1):\n",
    "                        checkpoint_path = os.path.join(OUTPUT_DIR, \"pruned_finetuned_model.pt\")\n",
    "                        torch.save({\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'scheduler_state_dict': scheduler.state_dict(),\n",
    "                            'step': step,\n",
    "                            'loss': loss.item(),\n",
    "                            'val_loss': val_loss,\n",
    "                            'val_ppl': val_ppl\n",
    "                        }, checkpoint_path)\n",
    "                        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "                \n",
    "                # Increment step\n",
    "                step += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                if DEVICE == \"cuda\" and \"CUDA\" in str(e):\n",
    "                    print(f\"CUDA error during training at batch {batch_idx}, epoch {epoch+1}: {e}\")\n",
    "                    print(\"Attempting to continue training on CPU...\")\n",
    "                    \n",
    "                    # Clear GPU memory\n",
    "                    clear_gpu_memory()\n",
    "                    \n",
    "                    # Try again on CPU\n",
    "                    try:\n",
    "                        # Move to CPU\n",
    "                        model = model.cpu()\n",
    "                        input_ids = input_ids.cpu()\n",
    "                        attention_mask = attention_mask.cpu()\n",
    "                        labels = labels.cpu()\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                        cpu_loss = outputs.loss\n",
    "                        \n",
    "                        # Backward pass\n",
    "                        cpu_loss.backward()\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        \n",
    "                        # Evaluate\n",
    "                        val_loss, val_ppl = evaluate_model(model, val_dataloader)\n",
    "                        print(f\"CPU Step {step+1}/{total_steps} | Loss: {cpu_loss.item():.4f} | Val Loss: {val_loss:.4f} | Val PPL: {val_ppl:.2f}\")\n",
    "                        \n",
    "                        # Update metrics\n",
    "                        metrics.update(step, val_loss, val_ppl)\n",
    "                        \n",
    "                        # Move back to GPU if possible\n",
    "                        if torch.cuda.is_available():\n",
    "                            model = model.to(DEVICE)\n",
    "                        \n",
    "                        step += 1\n",
    "                    except Exception as e2:\n",
    "                        print(f\"Training also failed on CPU: {e2}\")\n",
    "                else:\n",
    "                    print(f\"Error during training at batch {batch_idx}, epoch {epoch+1}: {e}\")\n",
    "                \n",
    "                # Skip to next batch\n",
    "                continue\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_loss, final_ppl = evaluate_model(model, val_dataloader)\n",
    "    print(f\"Final evaluation - Loss: {final_loss:.4f}, Perplexity: {final_ppl:.2f}\")\n",
    "    \n",
    "    return final_loss, final_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aa5fb9",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Execute the full pruning and fine-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_name=\"gpt2\", \n",
    "                   pruning_strategy=\"entropy\", \n",
    "                   pruning_level=0.3, \n",
    "                   fine_tuning_epochs=3, \n",
    "                   learning_rate=5e-5,\n",
    "                   batch_size=4,\n",
    "                   prompt=\"The quick brown fox jumps over the lazy dog. In recent years,\"):\n",
    "    \"\"\"Run the complete pruning and fine-tuning experiment.\"\"\"\n",
    "    # Step 1: Initialize and setup\n",
    "    print(f\"=== Running Pruning and Fine-tuning Experiment ===\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Pruning strategy: {pruning_strategy}\")\n",
    "    print(f\"Pruning level: {pruning_level}\")\n",
    "    print(f\"Fine-tuning epochs: {fine_tuning_epochs}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    \n",
    "    # Initialize metrics tracker\n",
    "    metrics = ProgressMetrics()\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, cache_dir=MODEL_CACHE_DIR)\n",
    "    \n",
    "    # Step 3: Load data\n",
    "    train_dataloader, val_dataloader = load_wikitext_data(tokenizer, batch_size=batch_size)\n",
    "    if train_dataloader is None or val_dataloader is None:\n",
    "        print(\"Failed to load data. Aborting experiment.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 4: Evaluate initial performance\n",
    "    print(\"\\nEvaluating initial model performance...\")\n",
    "    initial_loss, initial_ppl = evaluate_model(model, val_dataloader)\n",
    "    print(f\"Initial performance - Loss: {initial_loss:.4f}, Perplexity: {initial_ppl:.2f}\")\n",
    "    \n",
    "    # Track initial metrics\n",
    "    metrics.update(0, initial_loss, initial_ppl)\n",
    "    \n",
    "    # Step 5: Generate initial text sample\n",
    "    print(\"\\nGenerating initial text sample...\")\n",
    "    initial_generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"Initial generation:\\n{initial_generation}\")\n",
    "    \n",
    "    # Step 6: Extract attention modules\n",
    "    attention_modules = get_attention_modules(model)\n",
    "    if not attention_modules:\n",
    "        print(\"Failed to extract attention modules. Aborting experiment.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 7: Add attention gating\n",
    "    success = add_attention_gating(model, attention_modules)\n",
    "    if not success:\n",
    "        print(\"Failed to add attention gating. Aborting experiment.\")\n",
    "        return None\n",
    "    \n",
    "    # Step 8: Calculate head importance\n",
    "    print(\"\\nCalculating head importance...\")\n",
    "    strategy = get_strategy(model.model_type, pruning_strategy)\n",
    "    importance = gather_head_importance(model, val_dataloader, attention_modules, strategy=strategy)\n",
    "    \n",
    "    # Step 9: Apply pruning\n",
    "    print(\"\\nApplying pruning...\")\n",
    "    pruned_heads = apply_head_pruning(model, importance, pruning_level)\n",
    "    \n",
    "    # Update metrics with pruning info\n",
    "    metrics.set_pruning_info(strategy, pruning_level, pruned_heads)\n",
    "    \n",
    "    # Visualize head importance\n",
    "    print(\"\\nVisualizing head importance...\")\n",
    "    fig = visualize_head_importance(importance, pruned_heads)\n",
    "    \n",
    "    # Step 10: Evaluate pruned model\n",
    "    print(\"\\nEvaluating pruned model performance...\")\n",
    "    pruned_loss, pruned_ppl = evaluate_model(model, val_dataloader)\n",
    "    print(f\"After pruning: loss: {pruned_loss:.4f}, perplexity: {pruned_ppl:.2f}\")\n",
    "    \n",
    "    # Step 11: Generate example text with pruned model\n",
    "    pruned_generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"Generation after pruning:\\n{pruned_generation}\")\n",
    "    \n",
    "    # Update metrics with pruned model performance\n",
    "    metrics.update(1, pruned_loss, pruned_ppl, \n",
    "                  head_info=importance.cpu().numpy().tolist(), \n",
    "                  generation_sample=pruned_generation)\n",
    "    \n",
    "    # Step 12: Set up optimizer and scheduler for fine-tuning\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create scheduler with warmup\n",
    "    num_training_steps = len(train_dataloader) * fine_tuning_epochs\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=num_warmup_steps, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    # Step 13: Fine-tune the pruned model\n",
    "    print(\"\\nFine-tuning pruned model...\")\n",
    "    final_loss, final_ppl = fine_tune_model(\n",
    "        model, \n",
    "        train_dataloader, \n",
    "        val_dataloader, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        metrics, \n",
    "        num_epochs=fine_tuning_epochs\n",
    "    )\n",
    "    \n",
    "    # Step 14: Generate final text sample\n",
    "    final_generation = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"Final generation after fine-tuning:\\n{final_generation}\")\n",
    "    \n",
    "    # Step 15: Save final metrics and plots\n",
    "    metrics_path = os.path.join(OUTPUT_DIR, \"pruning_finetuning_metrics.json\")\n",
    "    metrics.save_metrics(metrics_path)\n",
    "    \n",
    "    plots_path = os.path.join(OUTPUT_DIR, \"pruning_finetuning_plots.png\")\n",
    "    metrics.save_plots(plots_path)\n",
    "    \n",
    "    # Step 16: Print summary\n",
    "    summary = metrics.get_summary()\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Pruning strategy: {summary.get('strategy', strategy)}\")\n",
    "    print(f\"Pruning level: {summary.get('pruning_level', pruning_level)}\")\n",
    "    print(f\"Pruned heads: {summary.get('pruned_heads_count', len(pruned_heads))}\")\n",
    "    print(f\"Initial perplexity: {summary.get('initial_perplexity', initial_ppl):.2f}\")\n",
    "    print(f\"After pruning perplexity: {pruned_ppl:.2f}\")\n",
    "    print(f\"Final perplexity: {summary.get('final_perplexity', final_ppl):.2f}\")\n",
    "    print(f\"Improvement: {summary.get('improvement_percent', ((initial_ppl - final_ppl) / initial_ppl) * 100):.2f}%\")\n",
    "    \n",
    "    # If in Colab, offer to download results\n",
    "    if IS_COLAB:\n",
    "        print(\"\\nDownloading result files...\")\n",
    "        try:\n",
    "            download_files([metrics_path, plots_path])\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading files: {e}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef4df",
   "metadata": {},
   "source": [
    "## User Interface\n",
    "\n",
    "Run the experiment with customizable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f00d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment with the specified parameters\n",
    "# You can customize these parameters\n",
    "MODEL_NAME = \"distilgpt2\"  # Smaller GPT-2 model for faster demonstration\n",
    "PRUNING_STRATEGY = \"entropy\"  # Options: \"random\", \"magnitude\", \"entropy\"\n",
    "PRUNING_LEVEL = 0.3  # Percentage of heads to prune (0.0 to 1.0)\n",
    "FINE_TUNING_EPOCHS = 3  # Number of epochs for fine-tuning\n",
    "LEARNING_RATE = 5e-5  # Learning rate for fine-tuning\n",
    "BATCH_SIZE = 4  # Batch size for training and evaluation\n",
    "PROMPT = \"The quick brown fox jumps over the lazy dog. In recent years,\"  # Prompt for text generation\n",
    "\n",
    "# Run the experiment\n",
    "experiment_metrics = run_experiment(\n",
    "    model_name=MODEL_NAME,\n",
    "    pruning_strategy=PRUNING_STRATEGY,\n",
    "    pruning_level=PRUNING_LEVEL,\n",
    "    fine_tuning_epochs=FINE_TUNING_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    prompt=PROMPT\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PruningAndFineTuningColab",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
