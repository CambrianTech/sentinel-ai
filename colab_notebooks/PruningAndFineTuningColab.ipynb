{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (critical to do this before anything else)\n",
    "print(\"\ud83d\udd27 Installing dependencies...\")\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy pandas seaborn tqdm optax\n",
    "# Install datasets explicitly with required version to ensure compatibility\n",
    "!pip install -q 'datasets>=2.0.0' multiprocess\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1731672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from HuggingFace datasets and our data_modules without conflicts\n",
    "# Our internal module was renamed from 'datasets' to 'data_modules' to avoid namespace collisions\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "print(f\"Using HuggingFace datasets from: {datasets.__file__}\")\n",
    "\n",
    "# Import rest of the libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "# Add the current directory to path and import our modules\n",
    "# Prioritize local imports for our own modules, but keep system modules first\n",
    "# This ensures we use our utils package but system's datasets\n",
    "system_paths = [p for p in sys.path if '/usr/local' in p or 'python3' in p or 'site-packages' in p]\n",
    "local_paths = [\".\"]  # Current directory first\n",
    "other_paths = [p for p in sys.path if p not in system_paths and p != \".\"]\n",
    "sys.path = system_paths + local_paths + other_paths\n",
    "from utils.pruning import (\n",
    "    Environment,\n",
    "    ResultsManager,\n",
    "    PruningModule, \n",
    "    get_strategy,\n",
    "    FineTuner,\n",
    "    ImprovedFineTuner\n",
    ")\n",
    "from utils.pruning.stability import patch_fine_tuner, optimize_fine_tuner\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6cfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()\n",
    "\n",
    "# Check JAX capabilities\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom PruningFineTuningExperiment class for the Colab notebook\n# Note: This is different from the one in utils.pruning.experiment to support additional parameters\n",
    "class PruningFineTuningExperiment:\n",
    "    \"\"\"Manages the pruning + fine-tuning experiment\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 results_dir=\"pruning_finetuning_results\",\n",
    "                 use_improved_fine_tuner=True,\n",
    "                 detect_environment=True,\n",
    "                 optimize_memory=True,\n",
    "                 batch_size=None,\n",
    "                 sequence_length=None,\n",
    "                 stability_level=None,\n",
    "                 stability_level=None):\n",
    "        self.results_dir = Path(results_dir)\n",
    "        self.results_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.results = []\n",
    "        self.current_experiment = {}\n",
    "        \n",
    "        # Store configuration parameters\n",
    "        self.use_improved_fine_tuner = use_improved_fine_tuner\n",
    "        self.optimize_memory = optimize_memory\n",
    "        self.batch_size = batch_size\n",
    "        # Get sequence length from environment if set\n",
    "        sequence_length = getattr(self.env, \"seq_length\", 64) if hasattr(self.env, \"seq_length\") else 64\n",
    "        # Use ImprovedFineTuner with stability level if provided\n",
    "        stability_level = getattr(self.env, \"stability_level\", 2) if hasattr(self.env, \"stability_level\") else 2\n",
    "        \n",
    "        # Initialize environment\n",
    "        self.env = Environment()\n",
    "        \n",
    "        # Add has_high_ram attribute if not present in Environment class\n",
    "        # This ensures backward compatibility while also providing better memory detection\n",
    "        if not hasattr(self.env, 'has_high_ram'):\n",
    "            self.env.has_high_ram = False\n",
    "            try:\n",
    "                import psutil\n",
    "                total_ram = psutil.virtual_memory().total / (1024**3)  # Convert to GB\n",
    "                self.env.has_high_ram = total_ram > 12\n",
    "                print(f\"Detected {total_ram:.1f}GB RAM, high RAM: {self.env.has_high_ram}\")\n",
    "            except:\n",
    "                print(\"Could not detect RAM, assuming standard memory\")\n",
    "                \n",
    "        # Detect GPU memory if possible\n",
    "        self.gpu_memory_gb = 0\n",
    "        if self.env.has_gpu:\n",
    "            try:\n",
    "                import torch\n",
    "                gpu_props = torch.cuda.get_device_properties(0)\n",
    "                self.gpu_memory_gb = gpu_props.total_memory / (1024**3)\n",
    "                print(f\"Detected GPU with {self.gpu_memory_gb:.1f}GB VRAM\")\n",
    "            except:\n",
    "                try:\n",
    "                    # Alternative method for JAX\n",
    "                    device = jax.devices()[0]\n",
    "                    if hasattr(device, 'memory_stats') and callable(device.memory_stats):\n",
    "                        memory_stats = device.memory_stats()\n",
    "                        if 'bytes_limit' in memory_stats:\n",
    "                            self.gpu_memory_gb = memory_stats['bytes_limit'] / (1024**3)\n",
    "                            print(f\"Detected GPU with approximately {self.gpu_memory_gb:.1f}GB VRAM\")\n",
    "                except:\n",
    "                    # Estimate based on common GPU types\n",
    "                    if self.env.in_colab:\n",
    "                        # T4 in Colab typically has 16GB\n",
    "                        self.gpu_memory_gb = 16\n",
    "                        print(f\"Estimating Colab GPU with {self.gpu_memory_gb}GB VRAM\")\n",
    "        \n",
    "        # Get suitable models for this environment\n",
    "        self.available_models = self.env.get_suitable_models()\n",
    "        print(f\"Models available: {', '.join(self.available_models)}\")\n",
    "        \n",
    "        # Apply manual configuration overrides if provided\n",
    "        if self.batch_size is not None or self.sequence_length is not None or self.stability_level is not None:\n",
    "            print(f\"Using custom optimization parameters:\")\n",
    "            if self.batch_size is not None:\n",
    "                print(f\"  - Batch size: {self.batch_size}\")\n",
    "                self.env.batch_size = self.batch_size\n",
    "            if self.sequence_length is not None:\n",
    "                print(f\"  - Sequence length: {self.sequence_length}\")\n",
    "                self.env.seq_length = self.sequence_length\n",
    "            if self.stability_level is not None:\n",
    "                print(f\"  - Stability level: {self.stability_level}\")\n",
    "                self.env.stability_level = self.stability_level\n",
    "        \n",
    "        # Model size limits based on environment - adapt based on detected resources\n",
    "        self.model_size_limits = {\n",
    "            \"gpt2\": 1.0,  # Always allow GPT-2 (124M params)\n",
    "            \"gpt2-medium\": 1.0,  # Always allow GPT-2 Medium (355M params)\n",
    "            \"opt-350m\": 1.0,  # Always allow OPT-350M\n",
    "            \"opt-125m\": 1.0,  # Always allow OPT-125M\n",
    "            \"facebook/opt-125m\": 1.0,  # Always allow OPT-125M\n",
    "            \"facebook/opt-350m\": 1.0,  # Always allow OPT-350M\n",
    "            \"EleutherAI/pythia-160m\": 1.0,  # Always allow Pythia-160M\n",
    "            \"EleutherAI/pythia-410m\": 1.0,  # Always allow Pythia-410M\n",
    "        }\n",
    "        \n",
    "        # Add larger models only if we have enough resources\n",
    "        if self.env.has_gpu and self.gpu_memory_gb >= 8:\n",
    "            # If we have a GPU with 8+ GB, allow medium-sized models\n",
    "            self.model_size_limits.update({\n",
    "                \"gpt2-large\": 1.0,  # Allow GPT-2 Large (774M params) with sufficient GPU\n",
    "                \"EleutherAI/pythia-1b\": 0.5,  # Allow Pythia-1B with pruning\n",
    "                \"facebook/opt-1.3b\": 0.3,  # Allow OPT-1.3B with significant pruning only\n",
    "            })\n",
    "            \n",
    "            if self.gpu_memory_gb >= 16:\n",
    "                # If we have 16+ GB VRAM, allow larger models\n",
    "                self.model_size_limits.update({\n",
    "                    \"gpt2-xl\": 0.3,  # Allow GPT-2 XL with pruning\n",
    "                    \"facebook/opt-1.3b\": 0.5,  # Allow OPT-1.3B with moderate pruning\n",
    "                    \"facebook/opt-2.7b\": 0.2,  # Allow OPT-2.7B with heavy pruning\n",
    "                })\n",
    "        \n",
    "        # Setup Results Manager\n",
    "        self.results_manager = ResultsManager(str(self.results_dir))\n",
    "        self.results_df = pd.DataFrame()\n",
    "    \n",
    "    def run_experiment(self, strategies, pruning_levels, prompt, fine_tuning_epochs=1, max_runtime=3600):\n",
    "        \"\"\"Run the full experiment\"\"\"\n",
    "        if not self.available_models:\n",
    "            print(\"No suitable models found for this environment\")\n",
    "            return\n",
    "        \n",
    "        # Start time for runtime tracking\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Generate all experiment combinations\n",
    "        experiments = []\n",
    "        for model in self.available_models:\n",
    "            for strategy in strategies:\n",
    "                for level in pruning_levels:\n",
    "                    experiments.append({\n",
    "                        \"model\": model,\n",
    "                        \"strategy\": strategy,\n",
    "                        \"pruning_level\": level,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"fine_tuning_epochs\": fine_tuning_epochs\n",
    "                    })\n",
    "        \n",
    "        # Shuffle to get more diverse results early\n",
    "        random.shuffle(experiments)\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=len(experiments), desc=\"Running experiments\")\n",
    "        \n",
    "        # Run experiments\n",
    "        for i, exp in enumerate(experiments):\n",
    "            # Check if we've exceeded the runtime limit\n",
    "            current_runtime = time.time() - start_time\n",
    "            if max_runtime is not None and current_runtime > max_runtime:\n",
    "                print(f\"\\nReached maximum runtime of {max_runtime/3600:.1f} hours\")\n",
    "                break\n",
    "                \n",
    "            # Update progress bar\n",
    "            pbar.set_description(f\"Testing {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}\")\n",
    "            \n",
    "            # Run experiment\n",
    "            try:\n",
    "                result = self.run_single_experiment(**exp)\n",
    "                if result is not None:\n",
    "                    self.results.append(result)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Plot intermediate results every few experiments\n",
    "                if (i + 1) % 1 == 0 or i == len(experiments) - 1:\n",
    "                    self.plot_results()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in experiment {exp['model']}, {exp['strategy']}, {exp['pruning_level']:.2f}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                # Still update progress bar\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Close progress bar\n",
    "        pbar.close()\n",
    "        \n",
    "        # Final results\n",
    "        print(f\"\\nCompleted {len(self.results)} experiments out of {len(experiments)} attempted\")\n",
    "        runtime = time.time() - start_time\n",
    "        print(f\"Total runtime: {runtime/3600:.2f} hours ({runtime/60:.2f} minutes)\")\n",
    "        \n",
    "        # Plot final results\n",
    "        self.plot_results()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def run_single_experiment(self, model, strategy, pruning_level, prompt, fine_tuning_epochs=1):\n",
    "        \"\"\"Run a single experiment with pruning and fine-tuning\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Experiment: {model}, {strategy} strategy, {pruning_level:.2f} pruning level\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Initialize pruning module\n",
    "        pruning_module = PruningModule(model)\n",
    "        if not pruning_module.load_model():\n",
    "            print(f\"Failed to load model {model}\")\n",
    "            return None\n",
    "        \n",
    "        # Store model name for architecture detection\n",
    "        pruning_module.model_name = model\n",
    "        \n",
    "        # Setup experiment record\n",
    "        self.current_experiment = {\n",
    "            \"model\": model,\n",
    "            \"strategy\": strategy,\n",
    "            \"pruning_level\": pruning_level,\n",
    "            \"prompt\": prompt,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"stages\": {}\n",
    "        }\n",
    "        \n",
    "        # 1. Evaluate baseline model\n",
    "        print(\"\\n>> Stage 1: Evaluating baseline model\")\n",
    "        original_params = pruning_module.original_params\n",
    "        \n",
    "        # Evaluate perplexity and generation\n",
    "        perplexity_baseline = pruning_module.evaluate_perplexity(original_params, prompt)\n",
    "        print(f\"Baseline perplexity: {perplexity_baseline:.4f}\")\n",
    "        \n",
    "        generated_baseline = pruning_module.generate_text(original_params, prompt)\n",
    "        print(f\"Baseline generated: {generated_baseline}\")\n",
    "        \n",
    "        # Record baseline results\n",
    "        self.current_experiment[\"stages\"][\"baseline\"] = {\n",
    "            \"perplexity\": float(perplexity_baseline),\n",
    "            \"generated_text\": generated_baseline\n",
    "        }\n",
    "        \n",
    "        # 2. Apply pruning\n",
    "        print(\"\\n>> Stage 2: Applying pruning\")\n",
    "        pruning_strat = get_strategy(strategy, pruning_module, prompt)\n",
    "        \n",
    "        # Calculate importance scores\n",
    "        print(\"Calculating head importance...\")\n",
    "        all_head_importance = pruning_strat.get_head_importance(original_params)\n",
    "        \n",
    "        # Sort by importance (ascending)\n",
    "        all_head_importance.sort(key=lambda x: x[2])\n",
    "        \n",
    "        # Determine number of heads to prune\n",
    "        total_heads = pruning_module.num_layers * pruning_module.num_heads\n",
    "        heads_to_prune = int(total_heads * pruning_level)\n",
    "        print(f\"Pruning {heads_to_prune} out of {total_heads} heads\")\n",
    "        \n",
    "        # Get head indices to prune (least important first)\n",
    "        head_indices = [(l, h) for l, h, _ in all_head_importance[:heads_to_prune]]\n",
    "        \n",
    "        # Prune heads\n",
    "        print(\"Pruning heads...\")\n",
    "        pruned_params = pruning_strat.prune_heads(original_params, head_indices)\n",
    "        \n",
    "        # Evaluate after pruning\n",
    "        perplexity_pruned = pruning_module.evaluate_perplexity(pruned_params, prompt)\n",
    "        print(f\"Pruned perplexity: {perplexity_pruned:.4f}\")\n",
    "        \n",
    "        generated_pruned = pruning_module.generate_text(pruned_params, prompt)\n",
    "        print(f\"Pruned generated: {generated_pruned}\")\n",
    "        \n",
    "        # Record pruning results\n",
    "        self.current_experiment[\"stages\"][\"pruned\"] = {\n",
    "            \"perplexity\": float(perplexity_pruned),\n",
    "            \"perplexity_change\": float(perplexity_pruned - perplexity_baseline),\n",
    "            \"generated_text\": generated_pruned,\n",
    "            \"pruned_heads\": heads_to_prune,\n",
    "            \"total_heads\": total_heads,\n",
    "            \"head_indices\": head_indices\n",
    "        }\n",
    "        \n",
    "        # 3. Fine-tune the pruned model - with improved stability\n",
    "        print(\"\\n>> Stage 3: Fine-tuning the pruned model\")\n",
    "        \n",
    "        # Create fine-tuner with dataset config\n",
    "        dataset_name = \"wikitext\"\n",
    "        dataset_config = \"wikitext-2-v1\"\n",
    "        \n",
    "        # Use custom parameters if provided, otherwise auto-detect\n",
    "        if hasattr(self.env, 'batch_size') and self.env.batch_size is not None:\n",
    "            batch_size = self.env.batch_size\n",
    "            print(f\"Using configured batch size: {batch_size}\")\n",
    "        else:\n",
    "            # Determine batch size based on model size and environment\n",
    "            if self.env.in_colab and self.env.has_tpu:\n",
    "                # TPUs can handle larger batch sizes\n",
    "                batch_size = 16\n",
    "                # But still reduce for large models\n",
    "                if \"1.3b\" in model.lower() or \"large\" in model.lower():\n",
    "                    batch_size = 8\n",
    "            elif self.env.in_colab and self.env.has_gpu:\n",
    "                batch_size = 8\n",
    "                # Reduce batch size for larger models\n",
    "                if \"1.3b\" in model.lower() or \"large\" in model.lower():\n",
    "                    batch_size = 4\n",
    "                elif \"2.7b\" in model.lower() or \"xl\" in model.lower():\n",
    "                    batch_size = 2\n",
    "            else:\n",
    "                # CPU-only case\n",
    "                batch_size = 4\n",
    "                # Even smaller for large models on CPU\n",
    "                if \"1.3b\" in model.lower() or \"large\" in model.lower():\n",
    "                    batch_size = 2\n",
    "                elif \"2.7b\" in model.lower() or \"xl\" in model.lower():\n",
    "                    batch_size = 1\n",
    "        \n",
    "        # Get sequence length from environment if set\n",
    "        sequence_length = getattr(self.env, 'seq_length', 64) if hasattr(self.env, 'seq_length') else 64\n",
    "        \n",
    "        # Use ImprovedFineTuner with stability level if provided\n",
    "        stability_level = getattr(self.env, 'stability_level', 2) if hasattr(self.env, 'stability_level') else 2\n",
    "        print(f\"Using ImprovedFineTuner (stability level: {stability_level})\")\n",
    "        print(f\"Training parameters: batch_size={batch_size}, sequence_length={sequence_length}\")\n",
    "        \n",
    "        fine_tuner = ImprovedFineTuner(\n",
    "            pruning_module, \n",
    "            dataset_name=dataset_name,\n",
    "            dataset_config=dataset_config,\n",
    "            batch_size=batch_size,\n",
    "            sequence_length=sequence_length,\n",
    "            stability_level=stability_level\n",
    "        )\n",
    "        \n",
    "        # Adjust learning rate based on model size\n",
    "        model_name = model.lower()\n",
    "        is_large_model = any(x in model_name for x in ['opt', '1.3b', 'large', 'bloom', '2.7b', 'xl'])\n",
    "        learning_rate = 1e-5 if is_large_model else 5e-5\n",
    "        \n",
    "        # Fine-tune model\n",
    "        try:\n",
    "            # Install a emergency NaN detection system\n",
    "            def emergency_fix_loss_fn(params, batch):\n",
    "                \"\"\"Safe loss function that prevents NaN values\"\"\"\n",
    "                model = pruning_module.model\n",
    "                \n",
    "                # Extract labels from batch but don't pass them to the model\n",
    "                labels = batch.pop(\"labels\", None)\n",
    "                \n",
    "                # Check for NaNs in input\n",
    "                for k, v in batch.items():\n",
    "                    if jnp.isnan(v).any() or jnp.isinf(v).any():\n",
    "                        # Replace NaNs with zeros\n",
    "                        batch[k] = jnp.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                        print(f\"Warning: Found NaN in {k}, replaced with zeros\")\n",
    "                \n",
    "                try:\n",
    "                    # Handle different model architectures\n",
    "                    is_opt_model = 'opt' in model_name\n",
    "                    \n",
    "                    if is_opt_model:\n",
    "                        # OPT models don't accept 'train' parameter\n",
    "                        outputs = model(**batch, params=params)\n",
    "                    else:\n",
    "                        # Other models might need the 'train' parameter\n",
    "                        outputs = model(**batch, params=params, train=True)\n",
    "                        \n",
    "                    logits = outputs.logits\n",
    "                    \n",
    "                    # Add labels back to batch\n",
    "                    batch[\"labels\"] = labels\n",
    "                    \n",
    "                    # Create mask for padding tokens\n",
    "                    pad_token_id = pruning_module.tokenizer.pad_token_id  \n",
    "                    loss_mask = (labels != pad_token_id)\n",
    "                    \n",
    "                    # Shift logits and labels\n",
    "                    shift_logits = logits[:, :-1]\n",
    "                    shift_labels = labels[:, 1:]\n",
    "                    shift_mask = loss_mask[:, 1:]\n",
    "                    \n",
    "                    # Check for NaNs in logits\n",
    "                    if jnp.isnan(shift_logits).any() or jnp.isinf(shift_logits).any():\n",
    "                        print(\"Warning: Found NaN/Inf in logits - replacing with finite values\")\n",
    "                        shift_logits = jnp.nan_to_num(shift_logits, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "                    \n",
    "                    # Safe cross entropy computation\n",
    "                    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "                        shift_logits, shift_labels\n",
    "                    )\n",
    "                    \n",
    "                    # Check and fix NaNs in loss\n",
    "                    if jnp.isnan(loss).any() or jnp.isinf(loss).any():\n",
    "                        print(\"Warning: Found NaN/Inf in loss - replacing with finite values\")\n",
    "                        loss = jnp.nan_to_num(loss, nan=1.0, posinf=1.0, neginf=1.0)\n",
    "                    \n",
    "                    # Safe mean calculation\n",
    "                    masked_loss = loss * shift_mask\n",
    "                    mask_sum = shift_mask.sum()\n",
    "                    \n",
    "                    # Safe division\n",
    "                    computed_loss = jnp.where(\n",
    "                        mask_sum > 0,\n",
    "                        masked_loss.sum() / mask_sum,\n",
    "                        jnp.array(0.0, dtype=loss.dtype)\n",
    "                    )\n",
    "                    \n",
    "                    if jnp.isnan(computed_loss) or jnp.isinf(computed_loss):\n",
    "                        print(\"CRITICAL: NaN/Inf in final loss - using fallback value\")\n",
    "                        return jnp.array(1.0, dtype=loss.dtype)\n",
    "                        \n",
    "                    return computed_loss\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in loss function: {e}\")\n",
    "                    # Return a safe default value\n",
    "                    batch[\"labels\"] = labels  # Restore labels\n",
    "                    return jnp.array(1.0)  # Safe fallback\n",
    "            \n",
    "            # Patch the fine tuner with our stability utilities\n",
    "            print(\"\u26a0\ufe0f Installing NaN-safe loss function\")\n",
    "            fine_tuner = patch_fine_tuner(fine_tuner, model_name=model)\n",
    "            \n",
    "            # Optimize memory usage based on model size and available GPU memory\n",
    "            print(f\"\ud83e\udde0 Optimizing memory usage for {model}\")\n",
    "            fine_tuner = optimize_fine_tuner(fine_tuner, model_name=model, gpu_memory_gb=self.gpu_memory_gb)\n",
    "            \n",
    "            # Proceed with fine-tuning\n",
    "            tuned_params, metrics = fine_tuner.fine_tune(\n",
    "                pruned_params, \n",
    "                num_epochs=fine_tuning_epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                evaluate_interval=5\n",
    "            )\n",
    "            \n",
    "            # Plot training progress\n",
    "            fine_tuner.plot_training_progress()\n",
    "            \n",
    "            # Evaluate fine-tuned model\n",
    "            perplexity_tuned = pruning_module.evaluate_perplexity(tuned_params, prompt)\n",
    "            print(f\"Fine-tuned perplexity: {perplexity_tuned:.4f}\")\n",
    "            \n",
    "            generated_tuned = pruning_module.generate_text(tuned_params, prompt)\n",
    "            print(f\"Fine-tuned generated: {generated_tuned}\")\n",
    "            \n",
    "            # Record fine-tuning results\n",
    "            self.current_experiment[\"stages\"][\"fine_tuned\"] = {\n",
    "                \"perplexity\": float(perplexity_tuned),\n",
    "                \"perplexity_change_from_baseline\": float(perplexity_tuned - perplexity_baseline),\n",
    "                \"perplexity_change_from_pruned\": float(perplexity_tuned - perplexity_pruned),\n",
    "                \"generated_text\": generated_tuned,\n",
    "                \"training_epochs\": fine_tuning_epochs,\n",
    "                \"training_metrics\": metrics\n",
    "            }\n",
    "            \n",
    "            # Compute recovery percentage\n",
    "            if perplexity_pruned > perplexity_baseline:\n",
    "                # Calculate how much of the perplexity increase was recovered\n",
    "                perplexity_increase = perplexity_pruned - perplexity_baseline\n",
    "                perplexity_recovery = perplexity_pruned - perplexity_tuned\n",
    "                recovery_percentage = (perplexity_recovery / perplexity_increase) * 100 if perplexity_increase > 0 else 0\n",
    "                \n",
    "                self.current_experiment[\"stages\"][\"fine_tuned\"][\"recovery_percentage\"] = float(recovery_percentage)\n",
    "                print(f\"Recovery percentage: {recovery_percentage:.2f}%\")\n",
    "            else:\n",
    "                # Pruning improved perplexity, so we measure improvement from baseline\n",
    "                improvement_percentage = ((perplexity_baseline - perplexity_tuned) / perplexity_baseline) * 100\n",
    "                \n",
    "                self.current_experiment[\"stages\"][\"fine_tuned\"][\"improvement_percentage\"] = float(improvement_percentage)\n",
    "                print(f\"Improvement percentage: {improvement_percentage:.2f}%\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error during fine-tuning: {e}\")\n",
    "            # Continue with partial results\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        # 4. Save results\n",
    "        print(\"\\n>> Stage 4: Saving results\")\n",
    "        \n",
    "        # Save to disk\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        result_filename = f\"{model.replace('/', '_')}_{strategy}_{pruning_level:.2f}_{timestamp}.json\"\n",
    "        result_path = self.results_dir / result_filename\n",
    "        \n",
    "        import json\n",
    "        with open(result_path, \"w\") as f:\n",
    "            json.dump(self.current_experiment, f, indent=2)\n",
    "            \n",
    "        print(f\"Results saved to {result_path}\")\n",
    "        \n",
    "        # Update DataFrame for plotting\n",
    "        self._update_dataframe()\n",
    "        \n",
    "        return self.current_experiment\n",
    "    \n",
    "    def _update_dataframe(self):\n",
    "        \"\"\"Update DataFrame for visualization\"\"\"\n",
    "        # Extract data for DataFrame\n",
    "        data = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            # Extract model and strategy info\n",
    "            model = result[\"model\"]\n",
    "            strategy = result[\"strategy\"]\n",
    "            pruning_level = result[\"pruning_level\"]\n",
    "            \n",
    "            # Add baseline stage\n",
    "            if \"baseline\" in result[\"stages\"]:\n",
    "                baseline = result[\"stages\"][\"baseline\"]\n",
    "                data.append({\n",
    "                    \"model\": model,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"pruning_level\": pruning_level,\n",
    "                    \"stage\": \"baseline\",\n",
    "                    \"perplexity\": baseline[\"perplexity\"]\n",
    "                })\n",
    "            \n",
    "            # Add pruned stage\n",
    "            if \"pruned\" in result[\"stages\"]:\n",
    "                pruned = result[\"stages\"][\"pruned\"]\n",
    "                data.append({\n",
    "                    \"model\": model,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"pruning_level\": pruning_level,\n",
    "                    \"stage\": \"pruned\",\n",
    "                    \"perplexity\": pruned[\"perplexity\"],\n",
    "                    \"perplexity_change\": pruned.get(\"perplexity_change\", 0)\n",
    "                })\n",
    "                \n",
    "            # Add fine-tuned stage\n",
    "            if \"fine_tuned\" in result[\"stages\"]:\n",
    "                fine_tuned = result[\"stages\"][\"fine_tuned\"]\n",
    "                data.append({\n",
    "                    \"model\": model,\n",
    "                    \"strategy\": strategy,\n",
    "                    \"pruning_level\": pruning_level,\n",
    "                    \"stage\": \"fine_tuned\",\n",
    "                    \"perplexity\": fine_tuned[\"perplexity\"],\n",
    "                    \"perplexity_change_from_baseline\": fine_tuned.get(\"perplexity_change_from_baseline\", 0),\n",
    "                    \"perplexity_change_from_pruned\": fine_tuned.get(\"perplexity_change_from_pruned\", 0),\n",
    "                    \"recovery_percentage\": fine_tuned.get(\"recovery_percentage\", None),\n",
    "                    \"improvement_percentage\": fine_tuned.get(\"improvement_percentage\", None)\n",
    "                })\n",
    "        \n",
    "        self.results_df = pd.DataFrame(data)\n",
    "    \n",
    "    def plot_results(self, figsize=(15, 12)):\n",
    "        \"\"\"Plot comprehensive experiment results\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available yet\")\n",
    "            return\n",
    "            \n",
    "        # Update DataFrame\n",
    "        self._update_dataframe()\n",
    "            \n",
    "        if self.results_df.empty:\n",
    "            print(\"No data available for plotting\")\n",
    "            return\n",
    "        \n",
    "        # Create figure\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        \n",
    "        # 1. Perplexity across stages by model and strategy\n",
    "        plt.subplot(2, 2, 1)\n",
    "        \n",
    "        # Get unique models and strategies\n",
    "        models = self.results_df[\"model\"].unique()\n",
    "        strategies = self.results_df[\"strategy\"].unique()\n",
    "        \n",
    "        # Filter to main stages\n",
    "        stages_df = self.results_df[self.results_df[\"stage\"].isin([\"baseline\", \"pruned\", \"fine_tuned\"])]\n",
    "        \n",
    "        # Plot lines connecting stages for each experiment\n",
    "        for model in models:\n",
    "            model_df = stages_df[stages_df[\"model\"] == model]\n",
    "            \n",
    "            for strategy in strategies:\n",
    "                strategy_df = model_df[model_df[\"strategy\"] == strategy]\n",
    "                \n",
    "                for pruning_level in strategy_df[\"pruning_level\"].unique():\n",
    "                    experiment_df = strategy_df[strategy_df[\"pruning_level\"] == pruning_level]\n",
    "                    \n",
    "                    # Sort by stage to ensure correct order\n",
    "                    stage_order = {\"baseline\": 0, \"pruned\": 1, \"fine_tuned\": 2}\n",
    "                    experiment_df = experiment_df.sort_values(by=\"stage\", key=lambda x: x.map(stage_order))\n",
    "                    \n",
    "                    # Plot if we have all stages\n",
    "                    if len(experiment_df) >= 2:\n",
    "                        label = f\"{model}, {strategy}, {pruning_level:.2f}\"\n",
    "                        plt.plot(experiment_df[\"stage\"], experiment_df[\"perplexity\"], \"o-\", label=label)\n",
    "        \n",
    "        plt.title(\"Perplexity Across Stages\")\n",
    "        plt.xlabel(\"Stage\")\n",
    "        plt.ylabel(\"Perplexity\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # 2. Recovery percentage vs pruning level\n",
    "        plt.subplot(2, 2, 2)\n",
    "        \n",
    "        # Get data with recovery information\n",
    "        recovery_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n",
    "        \n",
    "        if not recovery_df.empty:\n",
    "            # Create recovery column (combining both metrics)\n",
    "            recovery_df[\"recovery\"] = recovery_df[\"recovery_percentage\"]\n",
    "            # If improvement percentage exists and recovery is NaN, use negative of improvement\n",
    "            mask = recovery_df[\"recovery\"].isna() & recovery_df[\"improvement_percentage\"].notna()\n",
    "            recovery_df.loc[mask, \"recovery\"] = -recovery_df.loc[mask, \"improvement_percentage\"]\n",
    "            \n",
    "            # Plot by strategy\n",
    "            for strategy in strategies:\n",
    "                strategy_df = recovery_df[recovery_df[\"strategy\"] == strategy]\n",
    "                if not strategy_df.empty:\n",
    "                    for model in models:\n",
    "                        model_strategy_df = strategy_df[strategy_df[\"model\"] == model]\n",
    "                        if not model_strategy_df.empty:\n",
    "                            # Sort by pruning level\n",
    "                            model_strategy_df = model_strategy_df.sort_values(\"pruning_level\")\n",
    "                            plt.plot(model_strategy_df[\"pruning_level\"], model_strategy_df[\"recovery\"], \n",
    "                                    \"o-\", label=f\"{model}, {strategy}\")\n",
    "            \n",
    "            plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "            plt.axhline(y=100, color=\"g\", linestyle=\"--\", alpha=0.3)\n",
    "            plt.text(0.01, 100, \"Full Recovery\", color=\"green\", ha=\"left\", va=\"bottom\")\n",
    "            plt.text(0.01, -5, \"Improvement\", color=\"blue\", ha=\"left\", va=\"top\")\n",
    "            \n",
    "            plt.title(\"Recovery Percentage by Pruning Level\")\n",
    "            plt.xlabel(\"Pruning Level\")\n",
    "            plt.ylabel(\"Recovery % (negative means improvement)\")\n",
    "            plt.legend(fontsize=8)\n",
    "            plt.grid(True)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, \"No recovery data available yet\", \n",
    "                    ha=\"center\", va=\"center\", fontsize=12)\n",
    "        \n",
    "        # 3. Perplexity change: pruning vs fine-tuning effect\n",
    "        plt.subplot(2, 2, 3)\n",
    "        \n",
    "        if \"perplexity_change\" in self.results_df.columns and \"perplexity_change_from_pruned\" in self.results_df.columns:\n",
    "            # Get pruning change\n",
    "            pruned_df = self.results_df[self.results_df[\"stage\"] == \"pruned\"].copy()\n",
    "            pruned_df = pruned_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity_change\"]]\n",
    "            \n",
    "            # Get fine-tuning change\n",
    "            finetuned_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n",
    "            finetuned_df = finetuned_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity_change_from_pruned\"]]\n",
    "            \n",
    "            # Merge\n",
    "            effects_df = pd.merge(\n",
    "                pruned_df, finetuned_df,\n",
    "                on=[\"model\", \"strategy\", \"pruning_level\"],\n",
    "                suffixes=(\"_pruning\", \"_finetuning\")\n",
    "            )\n",
    "            \n",
    "            if not effects_df.empty:\n",
    "                # Plot scatter with size based on pruning level\n",
    "                for strategy in strategies:\n",
    "                    strategy_df = effects_df[effects_df[\"strategy\"] == strategy]\n",
    "                    if not strategy_df.empty:\n",
    "                        for model in models:\n",
    "                            model_df = strategy_df[strategy_df[\"model\"] == model]\n",
    "                            if not model_df.empty:\n",
    "                                plt.scatter(\n",
    "                                    model_df[\"perplexity_change\"], \n",
    "                                    model_df[\"perplexity_change_from_pruned\"],\n",
    "                                    s=model_df[\"pruning_level\"] * 500,  # Size based on pruning level\n",
    "                                    label=f\"{model}, {strategy}\",\n",
    "                                    alpha=0.7\n",
    "                                )\n",
    "                \n",
    "                plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "                plt.axvline(x=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "                \n",
    "                # Add quadrant labels\n",
    "                plt.text(-5, -5, \"Both improved\", fontsize=10, ha=\"center\", va=\"center\",\n",
    "                        bbox=dict(facecolor=\"lightgreen\", alpha=0.5))\n",
    "                plt.text(5, -5, \"Pruning hurt,\\nFine-tuning fixed\", fontsize=10, ha=\"center\", va=\"center\",\n",
    "                        bbox=dict(facecolor=\"lightblue\", alpha=0.5))\n",
    "                plt.text(-5, 5, \"Pruning helped,\\nFine-tuning hurt\", fontsize=10, ha=\"center\", va=\"center\",\n",
    "                        bbox=dict(facecolor=\"lightyellow\", alpha=0.5))\n",
    "                plt.text(5, 5, \"Both hurt\", fontsize=10, ha=\"center\", va=\"center\",\n",
    "                        bbox=dict(facecolor=\"lightcoral\", alpha=0.5))\n",
    "                \n",
    "                plt.title(\"Effect of Pruning vs. Fine-tuning\")\n",
    "                plt.xlabel(\"Perplexity Change from Pruning\")\n",
    "                plt.ylabel(\"Perplexity Change from Fine-tuning\")\n",
    "                plt.legend(fontsize=8)\n",
    "                plt.grid(True)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"No effect data available yet\", \n",
    "                        ha=\"center\", va=\"center\", fontsize=12)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, \"No effect data available yet\", \n",
    "                    ha=\"center\", va=\"center\", fontsize=12)\n",
    "        \n",
    "        # 4. Final results: perplexity reduction by pruning level and strategy\n",
    "        plt.subplot(2, 2, 4)\n",
    "        \n",
    "        if \"perplexity_change_from_baseline\" in self.results_df.columns:\n",
    "            # Get baseline and final results\n",
    "            baseline_df = self.results_df[self.results_df[\"stage\"] == \"baseline\"].copy()\n",
    "            baseline_df = baseline_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "            baseline_df = baseline_df.rename(columns={\"perplexity\": \"baseline_perplexity\"})\n",
    "            \n",
    "            final_df = self.results_df[self.results_df[\"stage\"] == \"fine_tuned\"].copy()\n",
    "            final_df = final_df[[\"model\", \"strategy\", \"pruning_level\", \"perplexity\", \"perplexity_change_from_baseline\"]]\n",
    "            final_df = final_df.rename(columns={\"perplexity\": \"final_perplexity\"})\n",
    "            \n",
    "            # Merge\n",
    "            final_results = pd.merge(\n",
    "                baseline_df, final_df,\n",
    "                on=[\"model\", \"strategy\", \"pruning_level\"]\n",
    "            )\n",
    "            \n",
    "            if not final_results.empty:\n",
    "                # Plot as bar chart\n",
    "                # Group by pruning level and strategy\n",
    "                grouped = final_results.groupby([\"pruning_level\", \"strategy\"])[\"perplexity_change_from_baseline\"].mean().reset_index()\n",
    "                \n",
    "                # Pivot for grouped bar chart\n",
    "                pivot_df = grouped.pivot(index=\"pruning_level\", columns=\"strategy\", values=\"perplexity_change_from_baseline\")\n",
    "                \n",
    "                # Plot\n",
    "                pivot_df.plot(kind=\"bar\", ax=plt.gca())\n",
    "                \n",
    "                plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "                plt.title(\"Final Perplexity Change from Baseline\")\n",
    "                plt.xlabel(\"Pruning Level\")\n",
    "                plt.ylabel(\"Perplexity Change\")\n",
    "                plt.legend(title=\"Strategy\")\n",
    "                plt.grid(True, axis=\"y\")\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, \"No final results available yet\", \n",
    "                        ha=\"center\", va=\"center\", fontsize=12)\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, \"No final results available yet\", \n",
    "                    ha=\"center\", va=\"center\", fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb100e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment with memory optimizations\n",
    "experiment = PruningFineTuningExperiment(\n",
    "    results_dir=\"pruning_finetuning_results\",\n",
    "    use_improved_fine_tuner=True,      # Use the improved fine-tuner with stability enhancements\n",
    "    detect_environment=True,           # Automatically detect Colab environment\n",
    "    optimize_memory=True,              # Optimize memory usage based on detected hardware\n",
    "    batch_size=2,                      # Override batch size for fine-tuning\n",
    "    sequence_length=64,                # Override sequence length for fine-tuning\n",
    "    stability_level=2                  # Use enhanced stability measures (1-3, where 3 is max stability)\n",
    ")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef469269",
   "metadata": {},
   "source": [
    "# ## Longer Overnight Run\n",
    "# \n",
    "# For an extended overnight run, uncomment and run this cell:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ca23e",
   "metadata": {},
   "source": [
    "# ## Optimization Recommendations\n",
    "# \n",
    "# Based on our extensive profiling and research, we have the following recommendations:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e22b13",
   "metadata": {},
   "source": [
    "# ## Comprehensive Analysis\n",
    "# \n",
    "# After collecting results, run a comprehensive analysis:\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17bc276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "if not experiment.results_df.empty:\n",
    "    # Get data for different stages\n",
    "    baseline_df = experiment.results_df[experiment.results_df[\"stage\"] == \"baseline\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    baseline_df = baseline_df.rename(columns={\"perplexity\": \"baseline_perplexity\"})\n",
    "    \n",
    "    pruned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"pruned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    pruned_df = pruned_df.rename(columns={\"perplexity\": \"pruned_perplexity\"})\n",
    "    \n",
    "    finetuned_df = experiment.results_df[experiment.results_df[\"stage\"] == \"fine_tuned\"][[\"model\", \"strategy\", \"pruning_level\", \"perplexity\"]]\n",
    "    finetuned_df = finetuned_df.rename(columns={\"perplexity\": \"finetuned_perplexity\"})\n",
    "    \n",
    "    # Merge dataframes\n",
    "    summary = pd.merge(baseline_df, pruned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    summary = pd.merge(summary, finetuned_df, on=[\"model\", \"strategy\", \"pruning_level\"])\n",
    "    \n",
    "    # Calculate changes\n",
    "    summary[\"pruning_effect\"] = summary[\"pruned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    summary[\"finetuning_effect\"] = summary[\"finetuned_perplexity\"] - summary[\"pruned_perplexity\"]\n",
    "    summary[\"net_change\"] = summary[\"finetuned_perplexity\"] - summary[\"baseline_perplexity\"]\n",
    "    \n",
    "    # Display summary\n",
    "    summary.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}