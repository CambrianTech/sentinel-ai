{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Make a GPT-2 Model Smaller and More Powerful (v0.0.43-updated)\n\nThis notebook demonstrates how to make a GPT-2 model both smaller and more powerful by:\n1. Applying pruning to remove less important attention heads\n2. Fine-tuning the pruned model to recover and improve performance\n3. Showing clear metrics of improvement throughout the process\n\nWe use real data (Wikitext) rather than synthetic data for realistic evaluation.\n\nVersion History:\n- v0.0.43-updated (April 2025): FIXED COLAB REPOSITORY URL AND BRANCH SELECTION\n- v0.0.43 (April 2025): Fixed entropy pruning implementation to handle API availability gracefully\n- v0.0.42 (April 2025): Added super_simple test mode and improved error handling\n- v0.0.41 (April 2025): Modularized code using sentinel.pruning package\n- v0.0.40 (April 2025): Improve robustness for different model architectures\n- v0.0.39 (April 2025): Fix TypeError in run_experiment function call\n- v0.0.38 (April 2025): Fix ValueError in generate_text function\n- v0.0.37 (April 2025): Complete rewrite with minimal dependencies for reliability\n- v0.0.36 (April 2025): Simplified pruning implementation for better reliability\n- v0.0.35 (April 2025): Fixed in-place operation error in apply_head_pruning function\n- v0.0.34 (April 2025): Fixed undefined variable error, visualization issues and enhanced CUDA error handling\n- v0.0.33 (April 2025): Fixed visualization issues, improved model compatibility and enhanced error handling\n- v0.0.32 (April 2025): Added CUDA error handling for Colab compatibility and memory management\n- v0.0.31 (April 2025): Fixed get_strategy parameters issue and improved Colab compatibility\n- v0.0.30 (April 2025): Added OPT model support and chart improvements\n\n---\n**Note**: This notebook is part of the SentinelAI project. For detailed documentation, see `PruningAndFineTuningColab.md`.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of this Notebook\n",
    "\n",
    "This notebook demonstrates how pruning transformer models can make them both smaller and more powerful. Pruning is the process of removing less important components (in this case, attention heads) to create a more efficient model.\n",
    "\n",
    "The steps in this experiment are:\n",
    "\n",
    "1. **Initial Evaluation**: Measure the starting performance of the model\n",
    "2. **Pruning**: Remove less important attention heads using one of several strategies\n",
    "3. **Fine-tuning**: Train the pruned model to recover and potentially exceed its original performance\n",
    "4. **Evaluation**: Compare model performance before and after pruning and fine-tuning\n",
    "\n",
    "The metrics we track:\n",
    "- **Loss**: The training loss (lower is better)\n",
    "- **Perplexity**: A measure of how well the model predicts the next token (lower is better)\n",
    "\n",
    "The experiment shows how a properly pruned and fine-tuned model can be both smaller and more powerful than the original model.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "1. **Run all cells sequentially** from top to bottom\n",
    "2. You can adjust parameters like model size, pruning percentage, and training epochs\n",
    "3. The final cell allows you to interactively generate text with your pruned and fine-tuned model\n",
    "\n",
    "For GPT-2 models, this notebook works best with:\n",
    "- distilgpt2 (82M parameters)\n",
    "- gpt2 (124M parameters) \n",
    "- gpt2-medium (355M parameters)\n",
    "\n",
    "Other model architectures (OPT, Pythia, etc.) may require additional modifications."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q transformers==4.38.0 datasets==2.17.0 torch matplotlib tqdm\n\n# Import necessary libraries\nimport os\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport json\n\n# Import sys for path manipulation\nimport sys\n# Add the project root to the path so we can import our API modules\nif not any(p.endswith('sentinel-ai') for p in sys.path):\n    # For Google Colab - handle case where the notebook is running in a different directory\n    if os.path.exists('/content'):\n        # Clone the repo if running in Colab and not already cloned\n        if not os.path.exists('/content/sentinel-ai'):\n            # Use the correct repository URL and branch\n            !git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git /content/sentinel-ai\n        sys.path.append('/content/sentinel-ai')\n    else:\n        # Add parent directory to path if running locally\n        sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n\n# Print the Python path for debugging\nprint(\"Python path:\", sys.path)\n\n# Check if the expected directory structure exists\nif os.path.exists('/content/sentinel-ai'):\n    print(\"Repository contents:\")\n    !ls -la /content/sentinel-ai\n    print(\"\\nBranch information:\")\n    !cd /content/sentinel-ai && git branch -v\n\n# Import the modular sentinel.pruning API with better error handling\ntry:\n    from sentinel.pruning.experiment_runner import run_experiment, ExperimentConfig\n    from sentinel.pruning.text_generator import interactive_generate\n    print(\"Successfully imported sentinel.pruning modules\")\nexcept ImportError as e:\n    print(f\"Failed to import sentinel.pruning modules: {e}\")\n    print(\"This notebook requires the modular sentinel.pruning package.\")\n    print(\"Make sure you've pulled the latest code from the repository.\")\n    print(\"Falling back to direct API imports...\")\n    \n    # Fall back to the old API imports if sentinel.pruning is not available\n    try:\n        # Try importing from utils in the sentinel-ai repository\n        from utils.pruning.api.pruning import compute_head_importance, prune_heads, fine_tune, evaluate_model\n        from utils.pruning.api.data import load_wikitext, prepare_data, prepare_test_data\n        print(\"Successfully imported from utils.pruning.api\")\n    except ImportError as e2:\n        print(f\"Failed to import utils.pruning.api: {e2}\")\n        print(\"Installing the necessary packages via pip...\")\n        !pip install -q git+https://github.com/CambrianTech/sentinel-ai.git@feature/implement-adaptive-plasticity\n        print(\"Trying imports again after installation...\")\n        try:\n            from sentinel.pruning.experiment_runner import run_experiment, ExperimentConfig\n            from sentinel.pruning.text_generator import interactive_generate\n            print(\"Successfully imported sentinel.pruning modules after pip install\")\n        except ImportError as e3:\n            print(f\"Still failing after pip install: {e3}\")\n            print(\"Please check the repository structure or run locally.\")\n\n# Set up device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create directories for saving results\nos.makedirs(\"pruning_results\", exist_ok=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Pruning Works\n",
    "\n",
    "Pruning in transformer models involves identifying and removing less important components. In this notebook, we focus on pruning attention heads.\n",
    "\n",
    "### Why Prune Attention Heads?\n",
    "\n",
    "1. **Efficiency**: Fewer attention heads means less computation and memory usage\n",
    "2. **Specialization**: Removing redundant heads can force the model to learn more efficiently\n",
    "3. **Performance**: Properly pruned and fine-tuned models can actually perform better than the original\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. **Importance Calculation**: We use various metrics to determine which heads are important\n",
    "   - Random (baseline)\n",
    "   - Magnitude (based on weight norms)\n",
    "   - Entropy (based on attention patterns)\n",
    "\n",
    "2. **Pruning**: We remove heads with the lowest importance scores\n",
    "   - This is done by masking their output rather than actually removing parameters\n",
    "   \n",
    "3. **Fine-tuning**: We train the pruned model to recover and improve performance \n",
    "   - The model learns to compensate for the missing heads\n",
    "   - Remaining heads become more specialized and effective\n",
    "\n",
    "The result is a smaller, faster model that can match or exceed the original model's performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment parameters\n",
    "MODEL_NAME = \"distilgpt2\"  # Smaller GPT-2 model for faster demonstration\n",
    "PRUNING_PERCENT = 0.3  # Percentage of heads to prune (0-1)\n",
    "NUM_EPOCHS = 3  # Number of fine-tuning epochs \n",
    "BATCH_SIZE = 4  # Batch size for training and evaluation\n",
    "\n",
    "# Create experiment config\n",
    "experiment_config = ExperimentConfig(\n",
    "    model_name=MODEL_NAME,\n",
    "    pruning_percent=PRUNING_PERCENT,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    output_dir=\"pruning_results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Experiment\n",
    "\n",
    "Now we'll run the full pruning and fine-tuning experiment using our modular API. This process will:\n",
    "\n",
    "1. Load the specified model and tokenizer\n",
    "2. Evaluate the initial model performance\n",
    "3. Compute head importance using entropy-based metrics\n",
    "4. Prune the least important heads\n",
    "5. Evaluate the pruned model\n",
    "6. Fine-tune the pruned model\n",
    "7. Evaluate the final model and generate a summary\n",
    "\n",
    "All of this functionality is now neatly encapsulated in the `run_experiment` function from our modular API."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Run the experiment\nprint(\"\\nRunning experiment with modular API and improved error handling...\")\nprint(\"Note: Entropy pruning will gracefully fall back to alternative methods if needed\")\ntry:\n    # Create a simpler test config for troubleshooting if needed\n    test_config = experiment_config\n    test_config.use_test_data = True  # Use test data to ensure it works\n    \n    # Try running with test data first for verification\n    print(\"First running with test data to verify functionality...\")\n    model, tokenizer, summary = run_experiment(test_config)\n    \n    # If that works, run with the real config\n    if not experiment_config.use_test_data:\n        print(\"\\nNow running with actual configuration...\")\n        model, tokenizer, summary = run_experiment(experiment_config)\n        \nexcept Exception as e:\n    print(f\"\\nError in experiment: {e}\")\n    import traceback\n    traceback.print_exc()\n    \n    if \"collect_attention_distributions\" in str(e) or \"entropy_based_pruning\" in str(e):\n        print(\"\\nNOTE: If you're seeing an error with entropy pruning functions, make sure\")\n        print(\"you're using the latest version of the benchmark_with_metrics.py script that\")\n        print(\"has the fix for handling different API availability scenarios.\")\n    \n    # Try manual setup as a fallback\n    print(\"\\nAttempting manual setup as fallback...\")\n    try:\n        from transformers import AutoModelForCausalLM, AutoTokenizer\n        print(\"\\nLoading model directly for demonstration purposes...\")\n        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n        \n        # Add some basic pruning functionality for demo purposes\n        print(\"\\nSetting up basic pruning demo...\")\n        \n        # Create a very simple pruning function\n        def simple_pruning_demo(model, tokenizer, text=\"Once upon a time\"):\n            print(f\"Running simple pruning demo on '{text}'\")\n            # Generate text before pruning\n            inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n            with torch.no_grad():\n                outputs_before = model.generate(\n                    inputs[\"input_ids\"], \n                    max_length=50, \n                    do_sample=True, \n                    temperature=0.7,\n                    num_return_sequences=1\n                )\n            \n            before_text = tokenizer.decode(outputs_before[0], skip_special_tokens=True)\n            print(f\"\\nBefore pruning: {before_text}\")\n            \n            # Apply simple mask to attention (simulating pruning)\n            print(\"\\nApplying simple attention masking (simulating pruning)...\")\n            num_modified = 0\n            \n            # Find attention modules and apply masks\n            for name, module in model.named_modules():\n                if \"attention\" in name.lower() and hasattr(module, \"dropout\"):\n                    # Set dropout higher to simulate pruning\n                    if hasattr(module.dropout, \"p\"):\n                        old_dropout = module.dropout.p\n                        module.dropout.p = min(0.5, old_dropout + 0.2)\n                        num_modified += 1\n            \n            print(f\"Modified {num_modified} attention modules\")\n            \n            # Generate text after \"pruning\"\n            with torch.no_grad():\n                outputs_after = model.generate(\n                    inputs[\"input_ids\"], \n                    max_length=50, \n                    do_sample=True, \n                    temperature=0.7,\n                    num_return_sequences=1\n                )\n            \n            after_text = tokenizer.decode(outputs_after[0], skip_special_tokens=True)\n            print(f\"\\nAfter pruning: {after_text}\")\n            \n            return model, tokenizer\n        \n        # Run the demo\n        model, tokenizer = simple_pruning_demo(model, tokenizer)\n        \n    except Exception as e2:\n        print(f\"Fallback also failed: {e2}\")\n        print(\"Please run this notebook in a local environment where you have the repository properly set up.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Text Generation\n",
    "\n",
    "Now that we have a pruned and fine-tuned model, let's try generating some text with it interactively. You can provide your own prompts and see how the model responds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for interactive text generation \n",
    "def generate_interactive(prompt=None, max_length=100):\n",
    "    \"\"\"Generate text from the fine-tuned model interactively\"\"\"\n",
    "    return interactive_generate(model, tokenizer, prompt, max_length)\n",
    "\n",
    "# Generate text interactively from the fine-tuned model\n",
    "generate_interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}