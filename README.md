# ğŸ‘¾ Sentinel-AI â€” Adaptive Transformer with ANN Controller

Welcome to **Sentinel-AI**, a research framework for adaptive transformer models that restructure themselves in real-time. This architecture introduces:

- **Sentinel Gating**: Dynamic head pruning and regrowth  
- **ANN Controller**: Guides adaptivity using gradient and entropy signals  
- **Plug-and-Play Flexibility**: Load and upgrade pretrained models like `GPT2`, `DistilGPT2`, and others

<p align="center">
  <img src="./docs/assets/architecture_full_diagram.png" width="1000"/>
</p>

ğŸ‘¾ How U-Net Works in Our Transformer
In Sentinel-AI, we borrow from U-Netâ€™s idea of hierarchical skip pathways to stabilize regrowth of pruned attention heads and preserve earlier semantic features. Here's how:

ğŸ”„ U-Net Adaptivity in Transformer:
Skip Paths: Low-level gate activations or embeddings from earlier layers are forwarded to later layers.

Controller Memory: The ANN controller can receive both local and skip-connected signals (e.g., early entropy values).

Reinforcement Signal: When heads regrow, they can be initialized or influenced by past behavior â€” like how U-Net reuses encoder features for decoder guidance.

This is especially valuable when a head is pruned and later re-activated â€” we want the new head to resume useful behavior, not start from scratch.


ğŸ”¬ Our work builds on the premise that models should be able to **grow**, **shrink**, and **adapt** to task complexity on demand â€” ideal for edge deployment, low-resource training, and continual learning.

ğŸ“„ **[Read our Paper](./paper/adaptive_transformer_with_controller.md)**  
ğŸ§ª **[Explore the Interactive Notebooks](./notebooks/)**

---

## ğŸš€ Key Features

- ğŸ” **Dynamic Adaptivity** â€” Models automatically prune or re-enable attention heads during training  
- ğŸ›ï¸ **Controller-Driven Learning** â€” An ANN learns to adjust gates using runtime statistics  
- ğŸªœ **U-Net Style Growth** â€” Temporal skip connections stabilize regrowth of attention units  
- âš¡ **Colab & Low-resource Ready** â€” Optimized for T4-class GPUs and RAM-constrained settings  
- ğŸ§© **Compatible with Hugging Face** â€” Import pretrained `GPT2`, `DistilGPT2`, and others for rapid experimentation  

---


## ğŸ—‚ï¸ Repository Structure

```bash
sentinel-ai/
â”œâ”€â”€ models/                # Core model + adapters
â”œâ”€â”€ controller/            # ANN Controller for head gating
â”œâ”€â”€ datasets/              # Tokenization, batching, evaluation
â”œâ”€â”€ utils/                 # Logging, training logic, wrappers
â”œâ”€â”€ notebooks/             # Exploratory analysis and visualization
â”œâ”€â”€ paper/                 # Research paper in Markdown
â”œâ”€â”€ scripts/               # Colab-optimized training/eval
â”œâ”€â”€ train.py               # CLI for training
â”œâ”€â”€ main.py                # CLI for inference
â””â”€â”€ requirements.txt       # Environment dependencies
```

---

## ğŸ§ª Getting Started

### Installation

```bash
pip install -r requirements.txt
```

### Train

```bash
python train.py
```

This launches adaptive training using GPT-style weights and your selected dataset. Head activations are tracked and updated dynamically using controller signals.

### Inference

```bash
python main.py
# or with a different model
MODEL_NAME=gpt2 python main.py
```

### Use on Google Colab

```python
# In Colab
!git clone https://github.com/your-username/sentinel-ai.git
%cd sentinel-ai
!pip install -r requirements.txt
```

You can then open any notebook in `/notebooks/` or run `train_colab.py`.

---

## ğŸ“Š Visual Analysis & Evaluation Notebooks

Weâ€™ve included a rich suite of notebooks that explore different aspects of the architecture:

| Notebook | Description |
|----------|-------------|
| **AdaptiveTransformerNotebook** | Full Colab-ready training + benchmarking notebook |
| **Proof of Adaptivity** | Demonstrates head growth and pruning over time |
| **UNet Adaptivity** | Shows skip-residual inspired head adaptation |
| **Controller Dynamics** | Tracks controller gate logits and updates |
| **Attention Heatmaps** | Compare attention layers side-by-side |
| **Checkpoint Resumption** | Verifies full recovery from saved state |
| **Low Resource Adaptivity** | Validates self-slimming behavior under compute constraints |
| **Model Scaling Test** | Evaluate across GPT2 variants (`distilgpt2`, `gpt2`, etc.) |

ğŸ“ [See full list in `notebooks/`](./notebooks/README.md)

---

## ğŸ§  How It Works (Architecture)

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Pretrained GPT-like Transformer    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Sentinel Gates (Per-Head) â”€â”€â”
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                          â–²                  â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    ANN Controller    â”‚  â”‚ Attention  â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              (Dynamic activation / pruning)
```

---

## ğŸ§  Datasets Supported

- ğŸ“ **Tiny Shakespeare**
- ğŸ“š **WikiText-2**
- ğŸŒ **OpenWebText**

(Selected in `dataset_loader.py` or via notebook configuration.)

---

## âœ… Checkpointing

```python
from utils.checkpoint import save_checkpoint, load_checkpoint

# Save
save_checkpoint("checkpoint.pth", model, optimizer, head_lr_multipliers, epoch, step)

# Load
load_checkpoint("checkpoint.pth", model, optimizer)
```

---

## ğŸ“Œ Future Directions

- ğŸ¤– Expand controller to use gradient-based attention attribution
- ğŸ§¬ Enable continual learning across tasks and domains
- ğŸ”€ Experiment with LoRA and Adapter integration for rapid task adaptation
- ğŸ“¡ Federated scaling of adaptive models across edge devices

---

## ğŸ‘¥ Contributing

We welcome contributions!  
- Found a bug? File an issue.  
- Want to extend the controller or improve training? Submit a PR.  
- Interested in building new visual notebooks? We'd love that too.

---

ğŸ§ª Built with care by researchers exploring model self-adaptation, efficient inference, and plasticity in deep networks.
