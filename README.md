# ğŸ‘¾ Sentinel-AI â€” Dynamic Transformer with Learnable Attention Pruning and Regrowth

Welcome to **Sentinel-AI**, a modular research framework for transformers that can **prune**, **regrow**, and **restructure** themselves during training and inference. This architecture introduces:

- **Sentinel Gating** â€“ Learnable gating per attention head enabling pruning and selective reactivation  
- **ANN Controller** â€“ Neural network controller trained to monitor usage and adapt architecture  
- **U-Net Inspired Regrowth** â€“ Skip pathways and memory for reactivating previously pruned units without starting from scratch  
- **Plug-and-Play Loading** â€“ Easily imports pretrained models like `GPT2`, `DistilGPT2`, and others

> ğŸ”¬ This system evolves from compact models into large, expressive ones by **dynamically growing** its structure in response to data complexity â€” ideal for edge devices, progressive scaling, and long-term continual learning.

-<p align="center">
  <img src="./docs/assets/architecture_full_diagram.png" width="1000"/>
</p>

### ğŸ§­ Why Sentinel-AI?

Unlike traditional fixed-size transformers, Sentinel-AI is:

- Designed to **start small and grow** intelligently  
- Capable of **pruning and regrowing attention heads**, guided by data signals  
- Modular enough to wrap existing models with adaptive functionality  
- Efficient for training and inference across **low-resource** and **scalable** environments

ğŸ‘¾ **How Our Transformer Grows and Prunes Its Own Architecture**  
Sentinel-AI adopts a U-Net-inspired mechanism to **regrow pruned attention heads** without losing prior knowledge. This hierarchical structure preserves key semantics even as the model dynamically restructures itself.

**ğŸ”„ U-Net Adaptivity in Transformers:**
- **Skip Paths** â€” Early-layer gate activations or embeddings are forwarded to later layers during regrowth.
- **Controller Memory** â€” The ANN controller leverages both local signals and skip-connected context (e.g., entropy, gradients).
- **Reinforcement Signal** â€” Reactivated heads resume useful behavior by inheriting past characteristics, similar to how U-Net reuses encoder features in its decoder.

This enables seamless architectural evolution â€” pruning for efficiency, regrowing for capability â€” all without starting from zero.

---

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)
[![Colab Notebooks](https://img.shields.io/badge/Notebook-Colab-yellow.svg)](./notebooks/)


## ğŸ§  Why Adaptive Transformers?

Large language models are powerful but inefficient â€” many attention heads contribute little to output. **Sentinel-AI** dynamically prunes underutilized heads and later regrows them based on task complexity, entropy, and gradient feedback. This architecture:

- Saves memory and compute during training and inference
- Enables real-time architectural evolution
- Is ideal for edge devices, continual learning, and low-resource environments

---

## ğŸ“„ What Is Sentinel-AI?

Sentinel-AI is a research framework for adaptive transformer models that restructure themselves in real time. This architecture introduces:

- **Sentinel Gating** â€” Per-head gating values learned and optionally adjusted using runtime metrics
- **ANN Controller** â€” Learns to activate or deactivate heads based on entropy and gradient norms
- **U-Net Adaptivity** â€” Skip connections help reactivate heads gracefully without losing prior signal
- **Model Loading** â€” Easily wrap Hugging Face models (`GPT2`, `DistilGPT2`, etc.) and apply adaptivity on top

ğŸ“„ **[Read the Paper](./paper/adaptive_transformer_with_controller.md)**  
ğŸ§ª **[Explore the Notebooks](./notebooks/)**

---

## ğŸ§© Key Features

- ğŸ” **Dynamic Adaptivity** â€” Grows and prunes transformer heads in real-time
- ğŸ›ï¸ **Controller-Driven Optimization** â€” Entropy/gradient-based ANN controller adjusts gate values
- ğŸªœ **U-Net Style Growth** â€” Skip connections stabilize regrowth and knowledge reuse
- âš¡ **Colab-Ready** â€” Trains on T4 and other low-end GPUs with minimal memory
- ğŸ§  **Compatible with Pretrained Transformers** â€” Easily load and adapt `GPT2`, `DistilGPT2`, etc.

---

## ğŸ—‚ï¸ Repository Structure

```bash
sentinel-ai/
â”œâ”€â”€ models/                # Core model + adapters
â”œâ”€â”€ controller/            # ANN Controller for head gating
â”œâ”€â”€ datasets/              # Tokenization, batching, evaluation
â”œâ”€â”€ utils/                 # Logging, training logic, wrappers
â”œâ”€â”€ notebooks/             # Exploratory analysis and visualization
â”œâ”€â”€ paper/                 # Research paper in Markdown
â”œâ”€â”€ scripts/               # Colab-optimized training/eval
â”œâ”€â”€ train.py               # CLI for training
â”œâ”€â”€ main.py                # CLI for inference
â””â”€â”€ requirements.txt       # Environment dependencies
```

---

## ğŸš€ Getting Started

### Installation

```bash
pip install -r requirements.txt
```

### Training

```bash
python train.py
```

Train on `distilgpt2`, `gpt2`, or other Hugging Face models. The ANN controller and Sentinel gates activate dynamically during training.

### Inference

```bash
python main.py
# Or specify a different model
MODEL_NAME=gpt2 python main.py
```

### Google Colab Setup

```python
!git clone https://github.com/your-username/sentinel-ai.git
%cd sentinel-ai
!pip install -r requirements.txt
```

Then open any notebook in `/notebooks/` or run `scripts/train_colab.py`.

---

## ğŸ“Š Interactive Notebooks

| Notebook | Description |
|----------|-------------|
| **AdaptiveTransformerNotebook** | Full training + benchmarking notebook |
| **Proof of Adaptivity** | Shows dynamic pruning and regrowth in action |
| **UNet Adaptivity** | Demonstrates skip-based reinitialization for heads |
| **Controller Dynamics** | Tracks ANN logits and gating patterns |
| **Attention Heatmaps** | Side-by-side attention comparisons |
| **Checkpoint Resumption** | Tests that training resumes with gates intact |
| **Low Resource Adaptivity** | Confirms pruning under low-compute conditions |
| **Model Scaling Test** | Compare performance across model sizes |

ğŸ“ [Browse all notebooks](./notebooks/README.md)

---

## ğŸ§  How It Works (Overview)

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Pretrained Transformer    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Sentinel Gates       â”‚â—„â”€â”€â”€â”€â”
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                          â”‚                  â”‚
                          â–¼                  â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
               â”‚ Attention & FFN      â”‚      â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                          â–²                  â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
                â”‚  ANN Controller       â”€â”€â”€â”€â”€â”˜
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸ“ Also see:
- [`AdaptiveTransformer_Proof_of_Adaptivity.ipynb`](./notebooks/AdaptiveTransformer_Proof_of_Adaptivity.ipynb)
- [`ControllerDynamics.ipynb`](./notebooks/ControllerDynamics.ipynb)

---

## âœ… Checkpointing

```python
from utils.checkpoint import save_checkpoint, load_checkpoint

# Save training state
save_checkpoint("checkpoint.pth", model, optimizer, head_lr_multipliers, epoch, step)

# Resume training
load_checkpoint("checkpoint.pth", model, optimizer)
```

---

## ğŸ§¬ Supported Datasets

- ğŸ“ **Tiny Shakespeare**
- ğŸ“š **WikiText-2**
- ğŸŒ **OpenWebText**

Choose from notebook UI or set manually in `dataset_loader.py`.

---

## ğŸ“Œ Future Work

- ğŸ¤– Expand controller to use gradient attribution
- ğŸ§¬ Enable lifelong task adaptation
- ğŸª„ Plug in LoRA, Adapters, or QLoRA support
- ğŸŒ Enable federated adaptive learning across edge devices

---

## ğŸ‘¥ Contributing

Pull requests welcome! Whether itâ€™s:
- A new controller strategy
- A cleaner training loop
- Visualization notebooks
- Docs or diagrams

â€¦ weâ€™re excited to build this together.

---

ğŸ§ª Built with care by researchers exploring dynamic architectures, efficient inference, and model plasticity.
```

---
