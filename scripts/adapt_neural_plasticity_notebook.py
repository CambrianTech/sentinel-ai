#!/usr/bin/env python
"""
Adapt Neural Plasticity Notebook

This script creates a new version of the NeuralPlasticityDemo notebook that
uses the modular API while maintaining the original structure and flow.

Version: v0.0.60 (2025-04-20)
"""

import os
import sys
import json
import nbformat
from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell
from datetime import datetime
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

def create_adapted_notebook(short_version=False, output_path=None):
    """
    Create an adapted notebook that uses the modular neural plasticity API
    while preserving the original structure and flow of NeuralPlasticityDemo.ipynb.
    
    Args:
        short_version: Whether to create a shorter version for quick testing
        output_path: Path to save the notebook (default: neural_plasticity_adapted.ipynb)
    
    Returns:
        Path to the created notebook
    """
    # Default output path
    if output_path is None:
        output_path = os.path.join(project_root, "neural_plasticity_adapted.ipynb")
    
    # Get current timestamp
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    version = "0.0.60"
    
    # Create notebook
    notebook = new_notebook()
    
    # Title and introduction - match original notebook structure
    title_cell = new_markdown_cell(
        f"# Neural Plasticity in Transformer Models (v{version} ({current_time}))\n\n"
        "### Changes in v0.0.60:\n"
        "- Created fully modular architecture via utils/neural_plasticity package\n"
        "- Enhanced Apple Silicon compatibility with improved tensor handling\n"
        "- Added cross-platform visualization with device-aware tensor conversion\n"
        "- Added workarounds for PyTorch/BLAS crashes on M1/M2/M3 chips\n"
        "- Improved environment detection for Colab/local execution\n"
        "This notebook demonstrates Sentinel AI's neural plasticity system, which allows transformer models to dynamically prune and regrow attention heads during training based on utility metrics.\n\n"
        "## What is Neural Plasticity?\n\n"
        "Neural plasticity is the ability of neural networks to adapt their structure over time through pruning (removing unused connections) and regrowth (restoring useful connections). This mimics how biological brains form efficient neural pathways.\n\n"
        "In this demo, we:\n"
        "1. Track the entropy and gradient patterns of each attention head\n"
        "2. Dynamically prune high-entropy, low-gradient heads (unfocused, less useful)\n"
        "3. Selectively revive low-entropy, higher-gradient heads (potentially useful)\n"
        "4. Visualize the \"brain dynamics\" over time\n\n"
        "This allows models to form more efficient neural structures during training.\n\n"
        "## Environment Compatibility\n\n"
        "This notebook automatically detects your execution environment and applies the appropriate optimizations:\n\n"
        "- **Colab:** Uses GPU acceleration when available for maximum performance\n"
        "- **Apple Silicon:** Applies safeguards against BLAS/libtorch crashes that commonly occur on M1/M2/M3 Macs\n"
        "- **Standard Hardware:** Operates normally with GPU acceleration when available\n\n"
        "No manual configuration is required - just run the cells and the notebook will optimize for your environment."
    )
    notebook.cells.append(title_cell)
    
    # Colab system dependencies cell - same as original
    notebook.cells.append(new_code_cell(
        "# This cell is only needed in Colab\n"
        "if 'google.colab' in str(get_ipython()):\n"
        "    # Install system dependencies in Colab\n"
        "    !apt-get update -qq > /dev/null\n"
        "    !apt-get install -qq libopenblas-dev > /dev/null  # For better performance\n"
        "    print(\"Installed system dependencies for Colab\")\n"
        "else:\n"
        "    print(\"Running locally - skipping Colab-specific system dependencies\")"
    ))
    
    # Colab setup cell - same as original
    notebook.cells.append(new_code_cell(
        "# This cell is only needed in Colab - has no effect when running locally\n"
        "import os\n"
        "\n"
        "# Check if we're running in Colab\n"
        "IN_COLAB = 'google.colab' in str(get_ipython())\n"
        "\n"
        "if IN_COLAB:\n"
        "    # Install required packages in Colab\n"
        "    !pip install -q torch transformers datasets matplotlib seaborn\n"
        "    \n"
        "    # Check for GPU availability in Colab\n"
        "    import torch\n"
        "    if torch.cuda.is_available():\n"
        "        print(\"âœ… CUDA GPU detected in Colab! Using GPU acceleration.\")\n"
        "        !nvidia-smi  # Show GPU info\n"
        "    else:\n"
        "        print(\"âš ï¸ No GPU detected in Colab. Consider enabling GPU in Runtime > Change runtime type.\")\n"
        "    \n"
        "    # Clone the repository in Colab\n"
        "    !git clone -b feature/implement-adaptive-plasticity https://github.com/CambrianTech/sentinel-ai.git\n"
        "    %cd sentinel-ai\n"
        "    \n"
        "    # Add repository to path\n"
        "    import sys\n"
        "    sys.path.append('.')\n"
        "else:\n"
        "    # When running locally, we're already in the repository\n"
        "    print(\"Running locally - no need to clone repository\")\n"
        "    \n"
        "    # Make sure all required packages are installed\n"
        "    import importlib\n"
        "    \n"
        "    required_packages = ['torch', 'transformers', 'datasets', 'matplotlib', 'seaborn']\n"
        "    missing_packages = []\n"
        "    \n"
        "    for package in required_packages:\n"
        "        try:\n"
        "            importlib.import_module(package)\n"
        "        except ImportError:\n"
        "            missing_packages.append(package)\n"
        "    \n"
        "    if missing_packages:\n"
        "        print(f\"Warning: The following packages are missing and should be installed: {', '.join(missing_packages)}\")\n"
        "    else:\n"
        "        print(\"All required packages are installed\")"
    ))
    
    # Dataset fix cell - same as original
    notebook.cells.append(new_code_cell(
        "# Fix dataset import conflicts\n"
        "import sys\n"
        "import types\n"
        "\n"
        "# Check if datasets is already imported\n"
        "if 'datasets' not in sys.modules:\n"
        "    print(\"Creating datasets module to prevent import conflicts...\")\n"
        "    # Create mock module\n"
        "    mock_datasets = types.ModuleType('datasets')\n"
        "    mock_datasets.__path__ = []\n"
        "    \n"
        "    # Add required attributes\n"
        "    mock_datasets.ArrowBasedBuilder = type('ArrowBasedBuilder', (), {})\n"
        "    mock_datasets.GeneratorBasedBuilder = type('GeneratorBasedBuilder', (), {})\n"
        "    mock_datasets.Value = lambda *args, **kwargs: None\n"
        "    mock_datasets.Features = lambda *args, **kwargs: {}\n"
        "    \n"
        "    # Install the mock module\n"
        "    sys.modules['datasets'] = mock_datasets\n"
        "    \n"
        "    # Try to import the real load_dataset function\n"
        "    try:\n"
        "        from datasets.load import load_dataset\n"
        "        mock_datasets.load_dataset = load_dataset\n"
        "        print(\"Successfully added load_dataset to datasets module\")\n"
        "    except ImportError as e:\n"
        "        print(f\"Failed to import load_dataset: {e}\")\n"
        "        \n"
        "        # Try importing from our custom module\n"
        "        try:\n"
        "            from sdata import load_dataset\n"
        "            mock_datasets.load_dataset = load_dataset\n"
        "            print(\"Using sdata.load_dataset as fallback\")\n"
        "        except ImportError:\n"
        "            try:\n"
        "                from sentinel_data import load_dataset\n"
        "                mock_datasets.load_dataset = load_dataset\n"
        "                print(\"Using sentinel_data.load_dataset as fallback\")\n"
        "            except ImportError:\n"
        "                print(\"WARNING: Could not import any dataset loading function\")\n"
        "else:\n"
        "    print(\"datasets module already imported\")"
    ))

    # Environment detection cell - using modular API
    notebook.cells.append(new_code_cell(
        "# Enhanced environment detection and optimization\n"
        "import sys\n"
        "import platform\n"
        "import os\n"
        "from typing import Dict, Tuple, Optional, Union, List, Any\n"
        "\n"
        "# Import the modular neural plasticity API\n"
        "from utils.neural_plasticity import NeuralPlasticity\n"
        "from utils.neural_plasticity import (\n"
        "    # Core functions\n"
        "    calculate_head_entropy,\n"
        "    calculate_head_gradients,\n"
        "    generate_pruning_mask,\n"
        "    apply_pruning_mask,\n"
        "    evaluate_model,\n"
        "    \n"
        "    # Visualization functions\n"
        "    visualize_head_entropy,\n"
        "    visualize_head_gradients,\n"
        "    visualize_pruning_decisions,\n"
        "    visualize_attention_patterns,\n"
        "    \n"
        "    # Environment constants\n"
        "    IS_APPLE_SILICON,\n"
        "    IS_COLAB,\n"
        "    HAS_GPU\n"
        ")\n"
        "\n"
        "# Get environment information\n"
        "env_info = NeuralPlasticity.get_environment_info()\n"
        "\n"
        "# Detect Apple Silicon\n"
        "if IS_APPLE_SILICON:\n"
        "    print(\"ðŸŽ Apple Silicon detected - enabling basic optimizations\")\n"
        "    print(\"â„¹ï¸ When running on Apple Silicon, model operations will be forced to CPU\")\n"
        "    print(\"   This prevents BLAS/libtorch crashes that commonly occur on M1/M2/M3 chips\")\n"
        "\n"
        "# Check for GPU in Colab\n"
        "if IS_COLAB and HAS_GPU:\n"
        "    print(f\"âœ… CUDA GPU detected in Colab: {torch.cuda.get_device_name(0)}\")\n"
        "    print(f\"ðŸš€ Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
        "\n"
        "# Safe tensor conversion function from the modular API\n"
        "def safe_tensor_to_numpy(tensor):\n"
        "    \"\"\"Safely convert a tensor to numpy array with proper device handling\"\"\"\n"
        "    import torch\n"
        "    import numpy as np\n"
        "    \n"
        "    if isinstance(tensor, np.ndarray):\n"
        "        return tensor\n"
        "        \n"
        "    if not isinstance(tensor, torch.Tensor):\n"
        "        raise TypeError(f\"Expected torch.Tensor, got {type(tensor)}\")\n"
        "    \n"
        "    # Handle based on environment\n"
        "    if IS_APPLE_SILICON:\n"
        "        # Always detach and move to CPU on Apple Silicon\n"
        "        if tensor.requires_grad:\n"
        "            tensor = tensor.detach()\n"
        "        if tensor.is_cuda:\n"
        "            tensor = tensor.cpu()\n"
        "        if not tensor.is_contiguous():\n"
        "            tensor = tensor.contiguous()\n"
        "    else:\n"
        "        # Standard handling\n"
        "        if tensor.requires_grad:\n"
        "            tensor = tensor.detach()\n"
        "        if tensor.is_cuda:\n"
        "            tensor = tensor.cpu()\n"
        "    \n"
        "    # Convert to numpy safely\n"
        "    try:\n"
        "        return tensor.numpy()\n"
        "    except Exception as e:\n"
        "        print(f\"Error converting tensor to numpy: {e}\")\n"
        "        # Fall back to a zeros array with the same shape\n"
        "        return np.zeros(tensor.shape)\n"
        "\n"
        "# Define unique ID for cache busting\n"
        "import hashlib\n"
        "unique_id = hashlib.md5(f\"{datetime.now().strftime('%Y%m%d%H%M%S')}\".encode()).hexdigest()[:8]\n"
        "print(f\"Environment setup complete [ID: {unique_id}]\")"
    ))
    
    # Configuration section header - same as original
    notebook.cells.append(new_markdown_cell(
        "# Configure the Experiment\n\n"
        "Let's set up our configuration for the neural plasticity experiment"
    ))
    
    # Configuration cell - similar to original but using modular API imports
    notebook.cells.append(new_code_cell(
        "# Configure experiment\n"
        "MODEL_NAME = \"distilgpt2\"  # Small GPT-2 model for faster demonstration\n"
        "DATASET = \"wikitext\"\n"
        "DATASET_CONFIG = \"wikitext-2-raw-v1\"\n"
        "MAX_LENGTH = 128\n"
        "BATCH_SIZE = 4\n"
        "NUM_EPOCHS = 100      # Run for many epochs if needed\n"
        "LEARNING_RATE = 5e-5\n"
        "WARMUP_STEPS = 100\n"
        "WARMUP_MAX_EPOCHS = 1     # Maximum number of warmup epochs (will stop earlier if loss stabilizes)\n"
        "EVAL_INTERVAL = 50    # Evaluate every 50 steps\n"
        "VISUALIZATION_INTERVAL = 100  # Show visuals every 100 steps\n"
        "INFERENCE_INTERVAL = 500      # Run inference every 500 steps\n"
        "CHECKPOINT_INTERVAL = 500    # Save checkpoint more frequently (was 1000)\n"
        "MAX_STEPS_PER_EPOCH = None    # Set to a number to limit steps per epoch, or None for unlimited\n"
        "\n"
        "# Set to True to enable continuous training for long periods\n"
        "ENABLE_LONG_TRAINING = False  # Set to False for demo purposes to avoid memory/runtime issues\n"
        "\n"
        "# If ENABLE_LONG_TRAINING is True, run with unlimited steps per epoch\n"
        "# If ENABLE_LONG_TRAINING is False, override to a reasonable limit for demo purposes\n"
        "if not ENABLE_LONG_TRAINING:\n"
        "    MAX_STEPS_PER_EPOCH = 200 # Limit steps per epoch for demo purposes\n"
        "    NUM_EPOCHS = 3            # Limit epochs for demo purposes\n"
        "\n"
        "# Import pruning enums from the modular API\n"
        "from utils.neural_plasticity import PruningStrategy, PruningMode\n"
        "\n"
        "# Set pruning mode (ADAPTIVE allows recovery, COMPRESSED prevents recovery)\n"
        "PRUNING_MODE = PruningMode.ADAPTIVE  # Change to PruningMode.COMPRESSED for permanent pruning\n"
        "\n"
        "# Configure statistical-based pruning strategy\n"
        "# Instead of fixed thresholds, we'll use percentile-based thresholds\n"
        "ENTROPY_PERCENTILE = 70  # Heads with entropy above the 70th percentile are candidates for pruning\n"
        "GRADIENT_PERCENTILE = 30  # Heads with gradient below the 30th percentile are candidates for pruning\n"
        "PRUNE_PERCENT = 0.1      # Target to prune approximately 10% of heads in each step\n"
        "MIN_ZERO_EPOCHS = 1      # Minimum epochs a head should remain pruned"
    ))
    
    # Model loading section header - same as original
    notebook.cells.append(new_markdown_cell(
        "# Load Model and Dataset\n\n"
        "Now we'll load the model and prepare the dataset for training"
    ))
    
    # Model loading cell - using modular API imports
    notebook.cells.append(new_code_cell(
        "%matplotlib inline\n"
        "import torch\n"
        "import numpy as np\n"
        "import matplotlib.pyplot as plt\n"
        "import seaborn as sns\n"
        "from transformers import (\n"
        "    AutoModelForCausalLM, \n"
        "    AutoTokenizer, \n"
        "    default_data_collator,\n"
        "    get_linear_schedule_with_warmup\n"
        ")\n"
        "from torch.utils.data import DataLoader\n"
        "\n"
        "# Fix dataset imports with proper error handling\n"
        "try:\n"
        "    # Try using the datasets module directly if imports were fixed\n"
        "    from datasets import load_dataset\n"
        "    print(\"Using HuggingFace datasets module\")\n"
        "except (ImportError, AttributeError) as e:\n"
        "    try:\n"
        "        # Fall back to our custom module\n"
        "        from sdata import load_dataset\n"
        "        print(\"Using sdata module (Sentinel's dataset wrapper)\")\n"
        "    except ImportError:\n"
        "        # Last resort - check if datasets was fixed earlier\n"
        "        if 'datasets' in sys.modules:\n"
        "            datasets = sys.modules['datasets']\n"
        "            load_dataset = datasets.load_dataset\n"
        "            print(\"Using datasets module from sys.modules\")\n"
        "        else:\n"
        "            print(\"WARNING: Could not import datasets module. Will attempt to use it anyway.\")\n"
        "            # Create a minimal datasets module\n"
        "            import types\n"
        "            datasets = types.ModuleType('datasets')\n"
        "            sys.modules['datasets'] = datasets\n"
        "            \n"
        "            # Try to import from sentinel_data as last resort\n"
        "            try:\n"
        "                from sentinel_data import load_dataset\n"
        "                datasets.load_dataset = load_dataset\n"
        "                print(\"Using sentinel_data module as fallback\")\n"
        "            except ImportError:\n"
        "                print(\"CRITICAL: No dataset module available\")\n"
        "\n"
        "# Import visualization utilities\n"
        "from utils.colab.helpers import safe_tensor_imshow\n"
        "\n"
        "# Set device - force CPU on Apple Silicon regardless of CUDA availability\n"
        "device = torch.device(\"cpu\") if IS_APPLE_SILICON else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
        "print(f\"Using device: {device}\")\n"
        "\n"
        "# Load model and tokenizer\n"
        "print(f\"Loading model: {MODEL_NAME}\")\n"
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n"
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
        "\n"
        "# Set pad token if needed\n"
        "if tokenizer.pad_token is None:\n"
        "    tokenizer.pad_token = tokenizer.eos_token\n"
        "\n"
        "# Load datasets\n"
        "print(f\"Loading dataset: {DATASET}/{DATASET_CONFIG}\")\n"
        "train_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"train\")\n"
        "validation_dataset = load_dataset(DATASET, DATASET_CONFIG, split=\"validation\")\n"
        "\n"
        "# Define tokenization function\n"
        "def tokenize_function(examples):\n"
        "    return tokenizer(\n"
        "        examples[\"text\"], \n"
        "        padding=\"max_length\", \n"
        "        truncation=True, \n"
        "        max_length=MAX_LENGTH\n"
        "    )\n"
        "\n"
        "# Tokenize datasets\n"
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
        "validation_dataset = validation_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n"
        "\n"
        "# Add labels for language modeling\n"
        "def add_labels(examples):\n"
        "    examples[\"labels\"] = examples[\"input_ids\"].copy()\n"
        "    return examples\n"
        "\n"
        "train_dataset = train_dataset.map(add_labels)\n"
        "validation_dataset = validation_dataset.map(add_labels)\n"
        "\n"
        "# Set format\n"
        "train_dataset = train_dataset.with_format(\"torch\")\n"
        "validation_dataset = validation_dataset.with_format(\"torch\")\n"
        "\n"
        "# Create dataloaders\n"
        "train_dataloader = DataLoader(\n"
        "    train_dataset, \n"
        "    batch_size=BATCH_SIZE, \n"
        "    shuffle=True, \n"
        "    collate_fn=default_data_collator\n"
        ")\n"
        "\n"
        "validation_dataloader = DataLoader(\n"
        "    validation_dataset, \n"
        "    batch_size=BATCH_SIZE, \n"
        "    collate_fn=default_data_collator\n"
        ")\n"
        "\n"
        "print(f\"Train dataset size: {len(train_dataset)} examples\")\n"
        "print(f\"Validation dataset size: {len(validation_dataset)} examples\")\n"
        "\n"
        "# Define unique ID for cache busting\n"
        "from datetime import datetime\n"
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M\")\n"
        "unique_id = f\"{hash(TIMESTAMP) % 10000000:08x}\"\n"
        "print(f\"Running modularized neural plasticity code [ID: {unique_id}]\")"
    ))

    # Add evaluation function cells - match original
    notebook.cells.append(new_markdown_cell(
        "# Define Evaluation Function\n\n"
        "Let's define a function to evaluate our model's performance"
    ))
    
    notebook.cells.append(new_code_cell(
        "def evaluate_model_performance(model, dataloader, device):\n"
        "    # Evaluate model on the provided dataloader\n"
        "    model.eval()\n"
        "    total_loss = 0.0\n"
        "    total_steps = 0\n"
        "    \n"
        "    with torch.no_grad():\n"
        "        for batch in dataloader:\n"
        "            # Move batch to device\n"
        "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n"
        "            \n"
        "            # Forward pass\n"
        "            outputs = model(**batch)\n"
        "            loss = outputs.loss\n"
        "            \n"
        "            total_loss += loss.item()\n"
        "            total_steps += 1\n"
        "            \n"
        "            # Limit evaluation to 10 steps for speed\n"
        "            if total_steps >= 10:\n"
        "                break\n"
        "    \n"
        "    avg_loss = total_loss / total_steps if total_steps > 0 else float(\"inf\")\n"
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n"
        "    \n"
        "    return avg_loss, perplexity\n"
        "\n"
        "def generate_text(model, tokenizer, prompt, device, max_length=100):\n"
        "    # Generate text from the model\n"
        "    # Set model to evaluation mode\n"
        "    model.eval()\n"
        "    \n"
        "    # Encode prompt\n"
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n"
        "    \n"
        "    # Generate text\n"
        "    with torch.no_grad():\n"
        "        output = model.generate(\n"
        "            input_ids=input_ids,\n"
        "            max_length=max_length,\n"
        "            temperature=0.7,\n"
        "            do_sample=True,\n"
        "            top_k=50,\n"
        "            top_p=0.95,\n"
        "            pad_token_id=tokenizer.eos_token_id\n"
        "        )\n"
        "    \n"
        "    # Decode and return text\n"
        "    return tokenizer.decode(output[0], skip_special_tokens=True)"
    ))

    # Only include warm-up cell if full version is requested
    if not short_version:
        notebook.cells.append(new_markdown_cell(
            "## Run Model Warm-up\n\n"
            "Before measuring baseline performance and applying neural plasticity, we'll run a brief warm-up phase to get initial attention patterns and stabilize metrics."
        ))
        
        notebook.cells.append(new_code_cell(
            "# Initialize optimizer and scheduler for warm-up\n"
            "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
            "total_steps = len(train_dataloader) * WARMUP_MAX_EPOCHS\n"
            "scheduler = get_linear_schedule_with_warmup(\n"
            "    optimizer, \n"
            "    num_warmup_steps=WARMUP_STEPS, \n"
            "    num_training_steps=total_steps\n"
            ")\n"
            "\n"
            "print(f\"Running warm-up until loss stabilizes (max {WARMUP_MAX_EPOCHS} epochs)...\")\n"
            "\n"
            "# Warm-up training loop\n"
            "model.train()\n"
            "warmup_losses = []\n"
            "warmup_step_losses = []\n"
            "last_loss_decrease = 0\n"
            "patience = 15      # Number of steps with no decrease to consider stabilized\n"
            "min_warmup_steps = 50  # Minimum number of warm-up steps\n"
            "max_warmup_steps = 150  # Maximum number of warm-up steps per epoch\n"
            "\n"
            "# Helper function to calculate if loss has stabilized \n"
            "def is_loss_stabilized(losses, min_steps, patience_steps, window_size=5):\n"
            "    # Check if loss has stabilized\n"
            "    # Not enough steps yet\n"
            "    if len(losses) < min_steps:\n"
            "        return False, 0\n"
            "\n"
            "    # Not enough steps since last decrease\n"
            "    steps_since_decrease = len(losses) - last_loss_decrease\n"
            "    if steps_since_decrease < patience_steps:\n"
            "        return False, steps_since_decrease\n"
            "    \n"
            "    # Check if recent trend is flat or increasing using rolling average\n"
            "    if len(losses) >= window_size * 2:\n"
            "        recent_window = sum(losses[-window_size:]) / window_size\n"
            "        previous_window = sum(losses[-(window_size*2):-window_size]) / window_size\n"
            "        # If recent average is lower than previous, we're still decreasing\n"
            "        if recent_window < previous_window * 0.99:  # Allow 1% variation\n"
            "            return False, steps_since_decrease\n"
            "            \n"
            "    return True, steps_since_decrease\n"
            "\n"
            "try:\n"
            "    for epoch in range(WARMUP_MAX_EPOCHS):\n"
            "        epoch_loss = 0.0\n"
            "        epoch_steps = 0\n"
            "        \n"
            "        for step, batch in enumerate(train_dataloader):\n"
            "            # Move batch to device\n"
            "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n"
            "            \n"
            "            # Forward pass\n"
            "            outputs = model(**batch)\n"
            "            loss = outputs.loss\n"
            "            \n"
            "            # Backward pass\n"
            "            loss.backward()\n"
            "            \n"
            "            # Update weights\n"
            "            optimizer.step()\n"
            "            scheduler.step()\n"
            "            optimizer.zero_grad()\n"
            "            \n"
            "            # Track loss\n"
            "            loss_val = loss.item()\n"
            "            epoch_loss += loss_val\n"
            "            epoch_steps += 1\n"
            "            warmup_losses.append(loss_val)\n"
            "            \n"
            "            # Check if we've met the minimum steps and loss has stabilized\n"
            "            if len(warmup_losses) > 1:\n"
            "                # Track non-increasing steps\n"
            "                if loss_val <= warmup_losses[-2]:\n"
            "                    last_loss_decrease = len(warmup_losses)\n"
            "                \n"
            "                # For visualization, track a smoothed version (rolling average of 5)\n"
            "                if len(warmup_losses) % 5 == 0:\n"
            "                    avg_loss = sum(warmup_losses[-5:]) / 5\n"
            "                    warmup_step_losses.append(avg_loss)\n"
            "            \n"
            "            # Print progress every 5 steps\n"
            "            if step % 5 == 0:\n"
            "                print(f\"Warm-up Epoch {epoch+1}, Step {step}: Loss = {loss_val:.4f}\", end='')\n"
            "            \n"
            "            # Check if loss has stabilized\n"
            "            is_stable, steps_without_decrease = is_loss_stabilized(\n"
            "                warmup_losses, min_warmup_steps, patience\n"
            "            )\n"
            "            \n"
            "            if is_stable:\n"
            "                print(f\"\\nWarm-up loss stabilized after {len(warmup_losses)} steps\")\n"
            "                print(f\"Loss has been non-decreasing for {steps_without_decrease} steps\")\n"
            "                break\n"
            "                \n"
            "            # Stop after max_warmup_steps for faster execution in demo\n"
            "            if step >= max_warmup_steps:\n"
            "                print(f\"\\nReached maximum warm-up steps per epoch ({max_warmup_steps})\")\n"
            "                break\n"
            "        \n"
            "        print(f\"\\nWarm-up Epoch {epoch+1} completed: Average Loss = {epoch_loss / epoch_steps:.4f}\")\n"
            "        \n"
            "        # Check if loss has stabilized across epochs\n"
            "        is_stable, steps_without_decrease = is_loss_stabilized(\n"
            "            warmup_losses, min_warmup_steps, patience\n"
            "        )\n"
            "        \n"
            "        if is_stable:\n"
            "            print(f\"Loss has stabilized with {steps_without_decrease} steps without significant decrease.\")\n"
            "            print(f\"Ending warm-up early after {epoch+1} epochs.\")\n"
            "            break\n"
            "    \n"
            "    # Plot warm-up loss\n"
            "    plt.figure(figsize=(12, 8))\n"
            "    \n"
            "    # Raw loss\n"
            "    plt.subplot(2, 1, 1)\n"
            "    plt.plot(warmup_losses)\n"
            "    plt.title(\"Warm-up Loss (Raw)\")\n"
            "    plt.xlabel(\"Step\")\n"
            "    plt.ylabel(\"Loss\")\n"
            "    plt.grid(True)\n"
            "    \n"
            "    # Smoothed loss if we have enough data\n"
            "    if len(warmup_step_losses) > 1:\n"
            "        plt.subplot(2, 1, 2)\n"
            "        plt.plot(range(0, len(warmup_step_losses)*5, 5), warmup_step_losses)\n"
            "        plt.title(\"Warm-up Loss (5-step Rolling Average)\")\n"
            "        plt.xlabel(\"Step\")\n"
            "        plt.ylabel(\"Loss\")\n"
            "        plt.grid(True)\n"
            "        \n"
            "        # Add trend line to smoothed plot\n"
            "        from scipy.stats import linregress\n"
            "        x = range(0, len(warmup_step_losses)*5, 5)\n"
            "        slope, intercept, r_value, p_value, std_err = linregress(x, warmup_step_losses)\n"
            "        plt.plot(x, [slope*xi + intercept for xi in x], 'r--', \n"
            "                 label=f'Trend: slope={slope:.6f}, RÂ²={r_value**2:.2f}')\n"
            "        plt.legend()\n"
            "    \n"
            "    plt.tight_layout()\n"
            "    plt.show()\n"
            "    \n"
            "    # Segment analysis - compare first third vs last third of training\n"
            "    if len(warmup_losses) > 6:\n"
            "        segment_size = len(warmup_losses) // 3\n"
            "        first_segment = warmup_losses[:segment_size]\n"
            "        last_segment = warmup_losses[-segment_size:]\n"
            "        first_avg = sum(first_segment) / len(first_segment)\n"
            "        last_avg = sum(last_segment) / len(last_segment)\n"
            "        \n"
            "        print(f\"\\nWarm-up Segment Analysis:\")\n"
            "        print(f\"First {segment_size} steps average loss: {first_avg:.4f}\")\n"
            "        print(f\"Last {segment_size} steps average loss: {last_avg:.4f}\")\n"
            "        print(f\"Improvement during warm-up: {(1 - last_avg/first_avg)*100:.1f}%\")\n"
            "        \n"
            "        # Calculate if still improving significantly\n"
            "        still_improving = (first_avg - last_avg) / first_avg > 0.01  # More than 1% improvement\n"
            "        print(f\"Is model still significantly improving? {'Yes' if still_improving else 'No'}\")\n"
            "    \n"
            "    # Print warm-up summary\n"
            "    print(f\"\\nWarm-up completed with {len(warmup_losses)} steps across {epoch+1} epochs\")\n"
            "    print(f\"Initial loss: {warmup_losses[0]:.4f}\")\n"
            "    print(f\"Final loss: {warmup_losses[-1]:.4f}\")\n"
            "    print(f\"Overall loss reduction: {(1 - warmup_losses[-1]/warmup_losses[0])*100:.1f}%\")\n"
            "\n"
            "except Exception as e:\n"
            "    print(f\"\\nError during training: {e}\")"
        ))
    
    # Baseline evaluation section - match original
    notebook.cells.append(new_markdown_cell(
        "# Evaluate Baseline Model\n\n"
        "Now let's measure the baseline performance after warm-up"
    ))
    
    notebook.cells.append(new_code_cell(
        "# Evaluate baseline model after warm-up\n"
        "baseline_loss, baseline_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n"
        "print(f\"Baseline evaluation after warm-up: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n"
        "\n"
        "# Generate text with baseline model\n"
        "prompt = \"Once upon a time\"\n"
        "baseline_text = generate_text(model, tokenizer, prompt, device)\n"
        "print(f\"\\nPrompt: {prompt}\")\n"
        "print(f\"Generated text:\\n{baseline_text}\")"
    ))
    
    # Analyze attention patterns section - using modular API
    notebook.cells.append(new_markdown_cell(
        "## Analyze Attention Patterns\n\n"
        "Let's look at the attention patterns in the model to understand which heads we might want to prune."
    ))
    
    notebook.cells.append(new_code_cell(
        "# Get a batch of data\n"
        "batch = next(iter(validation_dataloader))\n"
        "input_ids = batch[\"input_ids\"].to(device)\n"
        "attention_mask = batch[\"attention_mask\"].to(device)\n"
        "\n"
        "# Analyze attention patterns using the modular API\n"
        "attention_data = NeuralPlasticity.analyze_attention_patterns(\n"
        "    model=model,\n"
        "    input_ids=input_ids,\n"
        "    attention_mask=attention_mask\n"
        ")\n"
        "\n"
        "# Extract attention tensors\n"
        "attention_tensors = attention_data[\"attention_tensors\"]\n"
        "\n"
        "# Calculate entropy for each attention head\n"
        "entropy_values = attention_data[\"entropy_values\"]\n"
        "\n"
        "# Visualize the entropy heatmap\n"
        "entropy_fig = visualize_head_entropy(\n"
        "    entropy_values=entropy_values,\n"
        "    title=\"Attention Entropy Heatmap\",\n"
        "    min_value=0.0,\n"
        "    annotate=True,\n"
        "    figsize=(10, 6)\n"
        ")\n"
        "plt.show()"
    ))
    
    # Calculate head gradients section - using modular API
    notebook.cells.append(new_markdown_cell(
        "## Calculate Head Gradients\n\n"
        "Now we'll calculate the gradient norms for each attention head to identify which ones have the least impact on the model's outputs."
    ))
    
    notebook.cells.append(new_code_cell(
        "# Calculate gradient norms for each head\n"
        "grad_norm_values = calculate_head_gradients(\n"
        "    model=model,\n"
        "    dataloader=train_dataloader,\n"
        "    num_batches=2,\n"
        "    device=device\n"
        ")\n"
        "\n"
        "# Visualize gradient norms\n"
        "grad_fig = visualize_head_gradients(\n"
        "    grad_norm_values=grad_norm_values,\n"
        "    title=\"Head Gradient Norms\",\n"
        "    figsize=(10, 5)\n"
        ")\n"
        "plt.show()"
    ))
    
    # Pruning mask section - using modular API with enums
    notebook.cells.append(new_markdown_cell(
        "## Generate Pruning Mask\n\n"
        "Based on entropy and gradient values, we'll create a pruning mask to identify which heads to prune."
    ))
    
    notebook.cells.append(new_code_cell(
        "# Define pruning strategy\n"
        "PRUNING_STRATEGY = PruningStrategy.COMBINED  # Use modular API enum\n"
        "\n"
        "# Generate pruning mask\n"
        "pruning_mask = generate_pruning_mask(\n"
        "    grad_norm_values=grad_norm_values,\n"
        "    entropy_values=entropy_values[0],  # Use first layer's entropy as reference\n"
        "    prune_percent=PRUNE_PERCENT,\n"
        "    strategy=PRUNING_STRATEGY\n"
        ")\n"
        "\n"
        "# Visualize pruning mask\n"
        "mask_fig = visualize_pruning_decisions(\n"
        "    grad_norm_values=grad_norm_values,\n"
        "    pruning_mask=pruning_mask,\n"
        "    title=f\"Pruning Decisions ({PRUNING_STRATEGY} strategy, {PRUNE_PERCENT*100:.0f}%)\"\n"
        ")\n"
        "plt.show()\n"
        "\n"
        "# Count pruned heads\n"
        "total_heads = pruning_mask.numel()\n"
        "pruned_count = pruning_mask.sum().item()\n"
        "print(f\"Pruning {pruned_count} out of {total_heads} heads ({pruned_count/total_heads*100:.1f}%)\")"
    ))
    
    # Apply pruning section - using modular API
    notebook.cells.append(new_markdown_cell(
        "## Apply Pruning to Model\n\n"
        "Now we'll apply the pruning mask to the model, zeroing out the weights of the selected heads."
    ))
    
    notebook.cells.append(new_code_cell(
        "# Apply pruning to the model\n"
        "pruned_heads = apply_pruning_mask(\n"
        "    model=model,\n"
        "    pruning_mask=pruning_mask,\n"
        "    mode=\"zero_weights\"\n"
        ")\n"
        "\n"
        "print(f\"Pruned {len(pruned_heads)} heads:\")\n"
        "for layer, head in pruned_heads[:10]:\n"
        "    print(f\"  Layer {layer}, Head {head}\")\n"
        "    \n"
        "if len(pruned_heads) > 10:\n"
        "    print(f\"  ... and {len(pruned_heads) - 10} more\")\n"
        "\n"
        "# Evaluate pruned model\n"
        "pruned_loss, pruned_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n"
        "print(f\"\\nPruned model evaluation: Loss = {pruned_loss:.4f}, Perplexity = {pruned_perplexity:.2f}\")\n"
        "print(f\"Baseline:               Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n"
        "print(f\"Difference:             {((pruned_loss - baseline_loss) / baseline_loss * 100):+.2f}%\")\n"
        "\n"
        "# Generate text with pruned model\n"
        "pruned_text = generate_text(model, tokenizer, prompt, device)\n"
        "print(f\"\\nPrompt: {prompt}\")\n"
        "print(f\"Generated text with pruned model:\\n{pruned_text}\")"
    ))
    
    # Fine-tuning section - match original
    notebook.cells.append(new_markdown_cell(
        "## Fine-tune the Pruned Model\n\n"
        "Now let's fine-tune the pruned model to adapt to the reduced structure."
    ))
    
    # Shortened or full fine-tuning cell
    fine_tuning_steps = "50" if short_version else "200"
    notebook.cells.append(new_code_cell(
        "# Initialize optimizer and scheduler for fine-tuning\n"
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n"
        "total_steps = len(train_dataloader) * NUM_EPOCHS\n"
        "scheduler = get_linear_schedule_with_warmup(\n"
        "    optimizer, \n"
        "    num_warmup_steps=WARMUP_STEPS, \n"
        "    num_training_steps=total_steps\n"
        ")\n"
        "\n"
        "# Training loop\n"
        "model.train()\n"
        "global_step = 0\n"
        "training_metrics = {\n"
        "    \"train_loss\": [],\n"
        "    \"eval_loss\": [],\n"
        "    \"perplexity\": [],\n"
        "    \"steps\": []\n"
        "}\n"
        "\n"
        f"# Number of fine-tuning steps (keep short for demonstration)\n"
        f"fine_tuning_steps = {fine_tuning_steps}\n"
        "eval_every = 40\n"
        "\n"
        "try:\n"
        "    print(f\"Fine-tuning for {fine_tuning_steps} steps...\")\n"
        "    steps_completed = 0\n"
        "    \n"
        "    while steps_completed < fine_tuning_steps:\n"
        "        for batch in train_dataloader:\n"
        "            if steps_completed >= fine_tuning_steps:\n"
        "                break\n"
        "                \n"
        "            # Move batch to device\n"
        "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n"
        "            \n"
        "            # Forward pass\n"
        "            outputs = model(**batch)\n"
        "            loss = outputs.loss\n"
        "            \n"
        "            # Backward pass\n"
        "            loss.backward()\n"
        "            \n"
        "            # Update weights\n"
        "            optimizer.step()\n"
        "            scheduler.step()\n"
        "            optimizer.zero_grad()\n"
        "            \n"
        "            # Track training metrics\n"
        "            steps_completed += 1\n"
        "            global_step += 1\n"
        "            \n"
        "            # Print progress\n"
        "            if steps_completed % 10 == 0:\n"
        "                print(f\"Step {steps_completed}/{fine_tuning_steps}, Loss: {loss.item():.4f}\", end='')\n"
        "            \n"
        "            # Evaluate\n"
        "            if steps_completed % eval_every == 0:\n"
        "                # Evaluation\n"
        "                model.eval()\n"
        "                eval_loss, eval_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n"
        "                \n"
        "                # Update metrics\n"
        "                training_metrics[\"train_loss\"].append(loss.item())\n"
        "                training_metrics[\"eval_loss\"].append(eval_loss)\n"
        "                training_metrics[\"perplexity\"].append(eval_perplexity)\n"
        "                training_metrics[\"steps\"].append(steps_completed)\n"
        "                \n"
        "                print(f\"\\nStep {steps_completed} - Eval loss: {eval_loss:.4f}, Perplexity: {eval_perplexity:.2f}\")\n"
        "                \n"
        "                # Back to training\n"
        "                model.train()\n"
        "    \n"
        "    print(f\"\\nFine-tuning completed: {steps_completed} steps\")\n"
        "    \n"
        "    # Plot training metrics\n"
        "    plt.figure(figsize=(12, 8))\n"
        "    \n"
        "    # Loss plot\n"
        "    plt.subplot(2, 1, 1)\n"
        "    plt.plot(training_metrics[\"steps\"], training_metrics[\"train_loss\"], label=\"Train Loss\")\n"
        "    plt.plot(training_metrics[\"steps\"], training_metrics[\"eval_loss\"], label=\"Eval Loss\")\n"
        "    plt.title(\"Loss During Fine-tuning\")\n"
        "    plt.xlabel(\"Steps\")\n"
        "    plt.ylabel(\"Loss\")\n"
        "    plt.legend()\n"
        "    plt.grid(True, alpha=0.3)\n"
        "    \n"
        "    # Perplexity plot\n"
        "    plt.subplot(2, 1, 2)\n"
        "    plt.plot(training_metrics[\"steps\"], training_metrics[\"perplexity\"], label=\"Perplexity\", color=\"green\")\n"
        "    plt.title(\"Perplexity During Fine-tuning\")\n"
        "    plt.xlabel(\"Steps\")\n"
        "    plt.ylabel(\"Perplexity\")\n"
        "    plt.legend()\n"
        "    plt.grid(True, alpha=0.3)\n"
        "    \n"
        "    plt.tight_layout()\n"
        "    plt.show()\n"
        "\n"
        "except Exception as e:\n"
        "    print(f\"\\nError during fine-tuning: {e}\")\n"
        "\n"
        "# Evaluate the fine-tuned pruned model\n"
        "model.eval()\n"
        "final_loss, final_perplexity = evaluate_model_performance(model, validation_dataloader, device)\n"
        "print(f\"\\nFinal evaluation:\")\n"
        "print(f\"Baseline: Loss = {baseline_loss:.4f}, Perplexity = {baseline_perplexity:.2f}\")\n"
        "print(f\"Pruned:   Loss = {pruned_loss:.4f}, Perplexity = {pruned_perplexity:.2f} ({((pruned_loss - baseline_loss) / baseline_loss * 100):+.2f}%)\")\n"
        "print(f\"Fine-tuned: Loss = {final_loss:.4f}, Perplexity = {final_perplexity:.2f} ({((final_loss - baseline_loss) / baseline_loss * 100):+.2f}%)\")"
    ))
    
    # Text generation section - match original
    notebook.cells.append(new_markdown_cell(
        "## Generate Text with Fine-tuned Model\n\n"
        "Let's generate text with our fine-tuned pruned model to see the results."
    ))
    
    notebook.cells.append(new_code_cell(
        "# Generate text with various prompts\n"
        "prompts = [\n"
        "    \"Once upon a time\",\n"
        "    \"The meaning of life is\",\n"
        "    \"In a world where AI\",\n"
        "    \"Scientists recently discovered\"\n"
        "]\n"
        "\n"
        "for prompt in prompts:\n"
        "    finetuned_text = generate_text(model, tokenizer, prompt, device)\n"
        "    print(f\"Prompt: {prompt}\")\n"
        "    print(f\"Generated text:\\n{finetuned_text}\")\n"
        "    print(\"-\" * 80)"
    ))
    
    # Summary section - match original
    notebook.cells.append(new_markdown_cell(
        "## Neural Plasticity Summary\n\n"
        "Our experiment showed how transformer models can be made more efficient through neural plasticity:\n\n"
        "1. We identified and pruned heads with low gradient impact and high entropy\n"
        "2. After pruning, there was a small initial performance drop\n"
        "3. With fine-tuning, the model adapted to its new structure, recovering most of the performance\n"
        "4. The final model is more efficient, using fewer parameters without significant quality loss\n\n"
        "This demonstrates that transformer models contain redundancy that can be eliminated, and the pruned model can adapt to function with fewer heads.\n\n"
        "Key metrics:\n"
        "- Baseline perplexity: baseline_perplexity\n"
        "- After pruning: pruned_perplexity (change%)\n"
        "- After fine-tuning: final_perplexity (change%)\n"
        "- Heads pruned: pruned_count out of total_heads (percentage%)\n\n"
        "This neural plasticity cycle mimics how biological brains optimize their neural pathways, making it an important step toward more efficient and adaptable AI systems."
    ))
    
    # Save the notebook
    with open(output_path, 'w', encoding='utf-8') as f:
        nbformat.write(notebook, f)
    
    print(f"âœ… Created adapted notebook: {output_path}")
    print("You can now run this notebook to test the neural plasticity functionality.")
    
    if short_version:
        print("Note: This is a shorter version optimized for quick testing.")
    
    return output_path

def main():
    parser = argparse.ArgumentParser(description="Create an adapted neural plasticity notebook")
    parser.add_argument("--short", action="store_true", help="Create a shorter version for quick testing")
    parser.add_argument("--output", type=str, default=None, help="Output path for the notebook")
    parser.add_argument("--original", action="store_true", help="Use structure from original notebook")
    
    args = parser.parse_args()
    
    created_path = create_adapted_notebook(short_version=args.short, output_path=args.output)
    
    # Print how to run the notebook
    print("\nTo run the notebook:")
    print(f"jupyter notebook {created_path}")

if __name__ == "__main__":
    main()