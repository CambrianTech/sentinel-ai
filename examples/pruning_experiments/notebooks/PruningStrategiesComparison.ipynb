{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pruning Strategies Comparison with Modular Experiment Framework (v0.1.0)\n\nThis notebook demonstrates how to run experiments comparing different pruning strategies using the modular experiment framework. It's designed for use in Colab, but works in any environment.\n\n## Overview\n\nIn this notebook we'll:\n1. Set up the environment and clone the repository\n2. Compare different pruning strategies (random, magnitude, entropy) with the same model\n3. Visualize and analyze the results\n4. Compare recovery rates across strategies\n\nThe notebook uses the `PruningExperiment` and `PruningFineTuningExperiment` classes from the modular experiment framework.",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll install the required dependencies and clone the repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q jax jaxlib flax transformers matplotlib numpy pandas seaborn tqdm optax\n",
    "!pip install -q 'datasets>=2.0.0' scikit-learn hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone the repository - using the refactor/modular-experiment branch\n!git clone -b refactor/modular-experiment https://github.com/CambrianTech/sentinel-ai.git\n# Don't cd into it yet"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import huggingface datasets directly before changing directory\n",
    "# This ensures we're using the system package\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "print(f\"Using datasets from: {datasets.__file__}\")\n",
    "\n",
    "# Now safely change to the repository directory\n",
    "%cd sentinel-ai\n",
    "\n",
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import JAX/Flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "# Import Hugging Face libraries\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n",
    "\n",
    "# Add the current directory to path and import our modules\n",
    "sys.path.append(\".\")\n",
    "from utils.pruning import (\n",
    "    Environment,\n",
    "    PruningModule, \n",
    "    get_strategy,\n",
    "    PruningExperiment\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Detection\n",
    "\n",
    "Next, we'll detect our environment capabilities to optimize experiments for the available hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and detect capabilities\n",
    "env = Environment()\n",
    "env.print_info()\n",
    "\n",
    "# Check JAX capabilities\n",
    "print(f\"\\nJAX version: {jax.__version__}\")\n",
    "print(f\"Devices: {jax.devices()}\")\n",
    "print(f\"Default backend: {jax.default_backend()}\")\n",
    "\n",
    "# Get suitable models based on hardware\n",
    "suitable_models = env.get_suitable_models()\n",
    "print(f\"\\nSuitable models for this environment: {', '.join(suitable_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Comparison Experiment\n",
    "\n",
    "Now, we'll set up an experiment to compare different pruning strategies on the same model. We'll use a small model (distilgpt2) and run multiple experiments with different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for experiment results\n",
    "results_dir = \"strategy_comparison_results\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Select a model that's suitable for our environment\n",
    "# We'll prefer smaller models for faster experiments\n",
    "preferred_models = [\"distilgpt2\", \"gpt2\", \"facebook/opt-125m\", \"EleutherAI/pythia-160m\"]\n",
    "model_to_use = None\n",
    "\n",
    "for model in preferred_models:\n",
    "    if model in suitable_models:\n",
    "        model_to_use = model\n",
    "        break\n",
    "        \n",
    "if not model_to_use and suitable_models:\n",
    "    # If none of our preferred models are available, use the first suitable one\n",
    "    model_to_use = suitable_models[0]\n",
    "    \n",
    "if not model_to_use:\n",
    "    # Fallback to a small model if no detection was possible\n",
    "    model_to_use = \"distilgpt2\"\n",
    "    \n",
    "print(f\"Selected model for experiments: {model_to_use}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the experiment using our modular experiment framework\n# This provides a consistent, reusable approach to running pruning experiments\nexperiment = PruningExperiment(\n    results_dir=results_dir,\n    use_improved_fine_tuner=True,  # Use improved fine-tuner with NaN prevention\n    detect_environment=True,       # Automatically detect hardware capabilities \n    optimize_memory=True           # Optimize parameters based on model and hardware\n)\n\n# Configure the experiment\nstrategies = [\"random\", \"magnitude\", \"entropy\"]\npruning_level = 0.3  # 30% pruning\nprompt = \"The future of artificial intelligence is\"\nfine_tuning_epochs = 1  # Just one epoch for demonstration"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "Now we'll run experiments for each strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments for each strategy\n",
    "results = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\n\\nRunning experiment with {strategy} strategy...\")\n",
    "    \n",
    "    result = experiment.run_single_experiment(\n",
    "        model=model_to_use,\n",
    "        strategy=strategy,\n",
    "        pruning_level=pruning_level,\n",
    "        prompt=prompt,\n",
    "        fine_tuning_epochs=fine_tuning_epochs,\n",
    "        save_results=True\n",
    "    )\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "    # Update the visualization after each experiment\n",
    "    experiment.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis\n",
    "\n",
    "Now let's analyze the results to compare the different strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for comparison\n",
    "comparison_data = []\n",
    "\n",
    "for result in results:\n",
    "    strategy = result[\"strategy\"]\n",
    "    baseline_perplexity = result[\"stages\"][\"baseline\"][\"perplexity\"]\n",
    "    pruned_perplexity = result[\"stages\"][\"pruned\"][\"perplexity\"]\n",
    "    pruning_effect = pruned_perplexity - baseline_perplexity\n",
    "    \n",
    "    if \"fine_tuned\" in result[\"stages\"]:\n",
    "        fine_tuned_perplexity = result[\"stages\"][\"fine_tuned\"][\"perplexity\"]\n",
    "        fine_tuning_effect = fine_tuned_perplexity - pruned_perplexity\n",
    "        net_effect = fine_tuned_perplexity - baseline_perplexity\n",
    "        \n",
    "        # Get recovery or improvement metrics\n",
    "        recovery_percentage = result[\"stages\"][\"fine_tuned\"].get(\"recovery_percentage\", None)\n",
    "        improvement_percentage = result[\"stages\"][\"fine_tuned\"].get(\"improvement_percentage\", None)\n",
    "        \n",
    "        comparison_data.append({\n",
    "            \"Strategy\": strategy,\n",
    "            \"Baseline Perplexity\": baseline_perplexity,\n",
    "            \"Pruned Perplexity\": pruned_perplexity,\n",
    "            \"Fine-tuned Perplexity\": fine_tuned_perplexity,\n",
    "            \"Pruning Effect\": pruning_effect,\n",
    "            \"Fine-tuning Effect\": fine_tuning_effect,\n",
    "            \"Net Effect\": net_effect,\n",
    "            \"Recovery %\": recovery_percentage,\n",
    "            \"Improvement %\": improvement_percentage\n",
    "        })\n",
    "    else:\n",
    "        comparison_data.append({\n",
    "            \"Strategy\": strategy,\n",
    "            \"Baseline Perplexity\": baseline_perplexity,\n",
    "            \"Pruned Perplexity\": pruned_perplexity,\n",
    "            \"Pruning Effect\": pruning_effect\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Strategy Comparison\n",
    "\n",
    "Let's create a custom visualization to compare the strategies more directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart comparing strategies\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot baseline, pruned, and fine-tuned perplexity for each strategy\n",
    "x = np.arange(len(strategies))\n",
    "width = 0.25\n",
    "\n",
    "if \"Fine-tuned Perplexity\" in comparison_df.columns:\n",
    "    # If we have fine-tuning results\n",
    "    plt.bar(x - width, comparison_df[\"Baseline Perplexity\"], width, label=\"Baseline\")\n",
    "    plt.bar(x, comparison_df[\"Pruned Perplexity\"], width, label=\"Pruned\")\n",
    "    plt.bar(x + width, comparison_df[\"Fine-tuned Perplexity\"], width, label=\"Fine-tuned\")\n",
    "else:\n",
    "    # If we only have pruning results\n",
    "    plt.bar(x - width/2, comparison_df[\"Baseline Perplexity\"], width, label=\"Baseline\")\n",
    "    plt.bar(x + width/2, comparison_df[\"Pruned Perplexity\"], width, label=\"Pruned\")\n",
    "\n",
    "plt.xlabel(\"Pruning Strategy\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(f\"Comparison of Pruning Strategies ({model_to_use}, {pruning_level*100:.0f}% pruning)\")\n",
    "plt.xticks(x, strategies)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, strategy in enumerate(strategies):\n",
    "    row = comparison_df[comparison_df[\"Strategy\"] == strategy].iloc[0]\n",
    "    \n",
    "    # Baseline\n",
    "    plt.text(i - width, row[\"Baseline Perplexity\"] + 5, \n",
    "             f\"{row['Baseline Perplexity']:.1f}\", \n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    \n",
    "    # Pruned\n",
    "    plt.text(i, row[\"Pruned Perplexity\"] + 5, \n",
    "             f\"{row['Pruned Perplexity']:.1f}\", \n",
    "             ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    \n",
    "    # Fine-tuned (if available)\n",
    "    if \"Fine-tuned Perplexity\" in comparison_df.columns:\n",
    "        plt.text(i + width, row[\"Fine-tuned Perplexity\"] + 5, \n",
    "                 f\"{row['Fine-tuned Perplexity']:.1f}\", \n",
    "                 ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery/Improvement Comparison\n",
    "\n",
    "If we have fine-tuning results, let's compare the recovery or improvement percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Recovery %\" in comparison_df.columns or \"Improvement %\" in comparison_df.columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Create a combined metric for display (recovery is positive, improvement is negative)\n",
    "    recovery_values = []\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        row = comparison_df[comparison_df[\"Strategy\"] == strategy].iloc[0]\n",
    "        \n",
    "        if pd.notna(row[\"Recovery %\"]):\n",
    "            # This is a recovery scenario (pruning hurt, fine-tuning helped)\n",
    "            recovery_values.append(row[\"Recovery %\"])\n",
    "        elif pd.notna(row[\"Improvement %\"]):\n",
    "            # This is an improvement scenario (pruning helped, fine-tuning helped more)\n",
    "            recovery_values.append(-row[\"Improvement %\"])\n",
    "        else:\n",
    "            recovery_values.append(0)\n",
    "    \n",
    "    # Create bars with different colors based on whether it's recovery or improvement\n",
    "    colors = [\"red\" if val >= 0 else \"green\" for val in recovery_values]\n",
    "    plt.bar(strategies, recovery_values, color=colors)\n",
    "    \n",
    "    # Add horizontal line at 0\n",
    "    plt.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "    \n",
    "    # Add labels\n",
    "    plt.text(strategies[0], 50, \"Recovery %\", color=\"red\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "    plt.text(strategies[0], -50, \"Improvement %\", color=\"green\", ha=\"center\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    # Add value labels to bars\n",
    "    for i, val in enumerate(recovery_values):\n",
    "        if val >= 0:\n",
    "            plt.text(i, val + 5, f\"{val:.1f}%\", ha=\"center\", va=\"bottom\", color=\"red\", fontsize=9)\n",
    "        else:\n",
    "            plt.text(i, val - 5, f\"{-val:.1f}%\", ha=\"center\", va=\"top\", color=\"green\", fontsize=9)\n",
    "    \n",
    "    plt.xlabel(\"Pruning Strategy\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(f\"Recovery or Improvement by Strategy ({model_to_use}, {pruning_level*100:.0f}% pruning)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Text Comparison\n",
    "\n",
    "Let's compare the quality of generated text from the baseline, pruned, and fine-tuned models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract generated text examples\n",
    "for result in results:\n",
    "    strategy = result[\"strategy\"]\n",
    "    print(f\"\\n\\n{'='*80}\\nStrategy: {strategy}\\n{'='*80}\")\n",
    "    \n",
    "    print(\"\\nPrompt:\")\n",
    "    print(result[\"prompt\"])\n",
    "    \n",
    "    print(\"\\nBaseline generated:\")\n",
    "    print(result[\"stages\"][\"baseline\"][\"generated_text\"])\n",
    "    \n",
    "    print(\"\\nPruned generated:\")\n",
    "    print(result[\"stages\"][\"pruned\"][\"generated_text\"])\n",
    "    \n",
    "    if \"fine_tuned\" in result[\"stages\"]:\n",
    "        print(\"\\nFine-tuned generated:\")\n",
    "        print(result[\"stages\"][\"fine_tuned\"][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on the experiments, we can draw the following conclusions about the different pruning strategies:\n",
    "\n",
    "1. **Entropy-based Pruning**: [Fill in with observations from your experiments]\n",
    "2. **Magnitude-based Pruning**: [Fill in with observations from your experiments]\n",
    "3. **Random Pruning**: [Fill in with observations from your experiments]\n",
    "\n",
    "The most effective strategy appears to be [fill in based on results], which achieved [summarize key results]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Finally, let's save our comparison results to a CSV file for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_csv_path = os.path.join(results_dir, \"strategy_comparison.csv\")\n",
    "comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"Comparison results saved to {comparison_csv_path}\")\n",
    "\n",
    "# If we're in Colab, download the results\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(comparison_csv_path)\n",
    "    print(\"\\nDownload initiated. Check your browser downloads.\")\n",
    "except:\n",
    "    print(\"\\nNot running in Google Colab. Results saved locally.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}